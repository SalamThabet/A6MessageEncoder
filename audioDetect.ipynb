{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "audioDetect.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SalamThabet/A6MessageEncoder/blob/master/audioDetect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOqkOnpZbZ8P",
        "colab_type": "code",
        "outputId": "7d663739-e2f2-4123-e4d7-60da7cd9d9b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUpMYyuobZ8T",
        "colab_type": "code",
        "outputId": "8750cf3c-1750-47bf-fdaf-47bb4a251e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "!pip install librosa\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.8)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.21.2)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.12.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.0)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.13.2)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.40.1)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.16.4)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.29.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5ecJi_hbZ8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa\n",
        "from librosa import display\n",
        "\n",
        "data, sampling_rate = librosa.load('/content/drive/My Drive/AI/ML-project/ravdessDataset/03-01-01-01-01-01-01.wav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGjUE_5FbZ8d",
        "colab_type": "code",
        "outputId": "a2c9a552-4805-49fd-a657-b34b21cbe5d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "% pylab inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob \n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveplot(data, sr=sampling_rate)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['display']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PolyCollection at 0x7f2c0be008d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtUAAAEKCAYAAADdM6kMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Quy_3TyCbZ8j",
        "colab_type": "code",
        "outputId": "179701ff-857d-4328-83d4-27f1bcfc4c3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import time\n",
        "\n",
        "path = '/content/drive/My Drive/AI/ML-project/ravdessDataset/'\n",
        "lst = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for subdir, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "      try:\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=60).T,axis=0) \n",
        "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
        "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
        "        file = int(file[7:8]) - 1 \n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue\n",
        "\n",
        "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Data loaded. Loading time: 81.4667067527771 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av7ViLVmbZ8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "X, y = zip(*lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzJV3r2abZ8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrdxaEkgbZ8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving joblib files to not load them again with the loop above\n",
        "\n",
        "import joblib\n",
        "\n",
        "X_name = 'X.joblib'\n",
        "y_name = 'y.joblib'\n",
        "save_dir = '/content/drive/My Drive/AI/ML-project/ravdessDataset/Ravdess_model'\n",
        "\n",
        "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
        "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kKx_LXJbZ8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "1b1dfe5a-9c3f-49f6-b171-95336cae3265"
      },
      "source": [
        "# Loading saved models\n",
        "\n",
        "X = joblib.load('/content/drive/My Drive/AI/ravdessDataset/Ravdess_model/X.joblib')\n",
        "y = joblib.load('/content/drive/My Drive/AI/ravdessDataset/Ravdess_model/y.joblib')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e4dc4d74ae09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/AI/ravdessDataset/Ravdess_model/X.joblib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/AI/ravdessDataset/Ravdess_model/y.joblib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/AI/ravdessDataset/Ravdess_model/X.joblib'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unW_SufxbZ80",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "5ac00b4b-929f-4b86-9eb4-f7cd1e42573f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b6b7f5ef4702>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH69rhr3x_Kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5OLDkT7yCQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtree = DecisionTreeClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bMgqFJ2yETt",
        "colab_type": "code",
        "outputId": "c0704f59-30f2-413b-c5ac-4f13cd1b3fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "dtree.fit(X_train, y_train)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-16b736c34381>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZSUxY94yGkZ",
        "colab_type": "code",
        "outputId": "d85454d8-29ab-4dbd-888e-16a79b11e1c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "predictions = dtree.predict(X_test)\n",
        "print(predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 2 2 3 2 3 3 2 2 0 2 0 0 0 3 2 3 2 3 3 2 3 3 0 0 2 0 2 3 2 2 3 3 2 3 3 3\n",
            " 0 2 2 2 3 2 3 2 0 0 3 3 0 2 2 0 2 0 2 0 0 0 3 0 3 2 2 0 3 3 0 2 0 3 2 3 3\n",
            " 2 3 3 3 3 2 3 3 0 3 3 3 2 2 0 0 0 2 0 2 3 2 3 2 0 3 3 3 0 2 3 0 3 3 3 2 3\n",
            " 3 2 0 2 3 3 3 0 2 2 2 2 3 0 3 3 3 3 3 0 0 2 3 2 2 2 2 3 0 3 2 0 0 2 0 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nnVqDfUyJ93",
        "colab_type": "code",
        "outputId": "aff210f5-2570-4f41-a25f-f559c6ae9ad8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.54      0.56        41\n",
            "           2       0.58      0.59      0.59        49\n",
            "           3       0.53      0.54      0.53        57\n",
            "\n",
            "    accuracy                           0.56       147\n",
            "   macro avg       0.56      0.56      0.56       147\n",
            "weighted avg       0.56      0.56      0.56       147\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLmsft6nyR-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwaMcet5RUU6",
        "colab_type": "code",
        "outputId": "163aac9d-51cd-49a2-ccd0-c1ae8c6c1598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_traincnn.shape, x_testcnn.shape\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((297, 40, 1), (147, 40, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2_REigCRWy7",
        "colab_type": "code",
        "outputId": "14cb5b43-f09e-48e0-ce74-3d3902a33aad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 5,padding='same',\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(8))\n",
        "model.add(Activation('softmax'))\n",
        "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0620 06:21:06.882517 140502658897792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0620 06:21:06.933963 140502658897792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0620 06:21:06.943560 140502658897792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0620 06:21:07.014201 140502658897792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0620 06:21:07.030093 140502658897792 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0620 06:21:07.053041 140502658897792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVXCQe6fRZ-l",
        "colab_type": "code",
        "outputId": "3dae140d-d93e-4fb0-bb41-91fc3b3b9144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 5128      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 8)                 0         \n",
            "=================================================================\n",
            "Total params: 87,944\n",
            "Trainable params: 87,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOibRDczRdYc",
        "colab_type": "code",
        "outputId": "d52a8ff1-b9e5-49da-9312-428166140138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0620 06:21:17.422198 140502658897792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0620 06:21:17.438595 140502658897792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErwdhyQTRid5",
        "colab_type": "code",
        "outputId": "c7da4dd3-d1c0-4aa9-9923-15a304bac417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36109
        }
      },
      "source": [
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0620 06:21:22.551913 140502658897792 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 297 samples, validate on 147 samples\n",
            "Epoch 1/1000\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 5.0246 - acc: 0.3704 - val_loss: 4.0784 - val_acc: 0.3333\n",
            "Epoch 2/1000\n",
            "297/297 [==============================] - 0s 595us/step - loss: 4.3905 - acc: 0.3906 - val_loss: 3.6982 - val_acc: 0.3946\n",
            "Epoch 3/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 3.9642 - acc: 0.3704 - val_loss: 1.0511 - val_acc: 0.4898\n",
            "Epoch 4/1000\n",
            "297/297 [==============================] - 0s 605us/step - loss: 4.0975 - acc: 0.3771 - val_loss: 2.4832 - val_acc: 0.4014\n",
            "Epoch 5/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 3.7031 - acc: 0.3872 - val_loss: 1.4312 - val_acc: 0.3741\n",
            "Epoch 6/1000\n",
            "297/297 [==============================] - 0s 616us/step - loss: 3.8659 - acc: 0.3838 - val_loss: 2.0023 - val_acc: 0.4286\n",
            "Epoch 7/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 3.6070 - acc: 0.4074 - val_loss: 1.1069 - val_acc: 0.4014\n",
            "Epoch 8/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 3.4958 - acc: 0.3704 - val_loss: 2.2183 - val_acc: 0.4354\n",
            "Epoch 9/1000\n",
            "297/297 [==============================] - 0s 584us/step - loss: 3.3084 - acc: 0.4040 - val_loss: 1.7566 - val_acc: 0.4286\n",
            "Epoch 10/1000\n",
            "297/297 [==============================] - 0s 547us/step - loss: 3.2593 - acc: 0.4343 - val_loss: 4.0031 - val_acc: 0.3946\n",
            "Epoch 11/1000\n",
            "297/297 [==============================] - 0s 540us/step - loss: 3.5312 - acc: 0.4040 - val_loss: 2.4043 - val_acc: 0.4014\n",
            "Epoch 12/1000\n",
            "297/297 [==============================] - 0s 597us/step - loss: 3.4231 - acc: 0.3939 - val_loss: 3.5446 - val_acc: 0.4082\n",
            "Epoch 13/1000\n",
            "297/297 [==============================] - 0s 558us/step - loss: 3.1998 - acc: 0.3939 - val_loss: 1.1481 - val_acc: 0.4354\n",
            "Epoch 14/1000\n",
            "297/297 [==============================] - 0s 547us/step - loss: 2.9873 - acc: 0.4377 - val_loss: 1.6593 - val_acc: 0.4558\n",
            "Epoch 15/1000\n",
            "297/297 [==============================] - 0s 572us/step - loss: 3.1942 - acc: 0.4141 - val_loss: 1.9961 - val_acc: 0.4150\n",
            "Epoch 16/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 3.1062 - acc: 0.4411 - val_loss: 2.5909 - val_acc: 0.4150\n",
            "Epoch 17/1000\n",
            "297/297 [==============================] - 0s 584us/step - loss: 2.7092 - acc: 0.4848 - val_loss: 1.1911 - val_acc: 0.4626\n",
            "Epoch 18/1000\n",
            "297/297 [==============================] - 0s 604us/step - loss: 2.6019 - acc: 0.5017 - val_loss: 1.3766 - val_acc: 0.4218\n",
            "Epoch 19/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 2.8141 - acc: 0.3973 - val_loss: 1.5076 - val_acc: 0.4626\n",
            "Epoch 20/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 2.9141 - acc: 0.4512 - val_loss: 1.2790 - val_acc: 0.4830\n",
            "Epoch 21/1000\n",
            "297/297 [==============================] - 0s 559us/step - loss: 3.0126 - acc: 0.4108 - val_loss: 1.2631 - val_acc: 0.4694\n",
            "Epoch 22/1000\n",
            "297/297 [==============================] - 0s 553us/step - loss: 2.7079 - acc: 0.4478 - val_loss: 1.1411 - val_acc: 0.5102\n",
            "Epoch 23/1000\n",
            "297/297 [==============================] - 0s 562us/step - loss: 2.6107 - acc: 0.4242 - val_loss: 1.0660 - val_acc: 0.5034\n",
            "Epoch 24/1000\n",
            "297/297 [==============================] - 0s 612us/step - loss: 2.6928 - acc: 0.4377 - val_loss: 1.4572 - val_acc: 0.5102\n",
            "Epoch 25/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 2.6598 - acc: 0.3872 - val_loss: 1.3260 - val_acc: 0.5034\n",
            "Epoch 26/1000\n",
            "297/297 [==============================] - 0s 587us/step - loss: 2.2629 - acc: 0.4646 - val_loss: 2.1489 - val_acc: 0.4490\n",
            "Epoch 27/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 2.3759 - acc: 0.4815 - val_loss: 2.2916 - val_acc: 0.4082\n",
            "Epoch 28/1000\n",
            "297/297 [==============================] - 0s 544us/step - loss: 2.5260 - acc: 0.4343 - val_loss: 1.3659 - val_acc: 0.4150\n",
            "Epoch 29/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 2.4950 - acc: 0.4175 - val_loss: 1.1082 - val_acc: 0.4490\n",
            "Epoch 30/1000\n",
            "297/297 [==============================] - 0s 544us/step - loss: 2.1250 - acc: 0.4747 - val_loss: 0.9986 - val_acc: 0.4762\n",
            "Epoch 31/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 2.2365 - acc: 0.4680 - val_loss: 1.9744 - val_acc: 0.5034\n",
            "Epoch 32/1000\n",
            "297/297 [==============================] - 0s 602us/step - loss: 2.0023 - acc: 0.4882 - val_loss: 1.1618 - val_acc: 0.4898\n",
            "Epoch 33/1000\n",
            "297/297 [==============================] - 0s 539us/step - loss: 2.3347 - acc: 0.4175 - val_loss: 1.1983 - val_acc: 0.5102\n",
            "Epoch 34/1000\n",
            "297/297 [==============================] - 0s 611us/step - loss: 2.2550 - acc: 0.4276 - val_loss: 1.1886 - val_acc: 0.4286\n",
            "Epoch 35/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 2.1122 - acc: 0.4848 - val_loss: 1.3561 - val_acc: 0.3741\n",
            "Epoch 36/1000\n",
            "297/297 [==============================] - 0s 587us/step - loss: 1.9261 - acc: 0.4646 - val_loss: 1.1066 - val_acc: 0.5034\n",
            "Epoch 37/1000\n",
            "297/297 [==============================] - 0s 590us/step - loss: 2.0393 - acc: 0.4916 - val_loss: 1.7261 - val_acc: 0.4694\n",
            "Epoch 38/1000\n",
            "297/297 [==============================] - 0s 561us/step - loss: 2.0048 - acc: 0.4579 - val_loss: 1.2920 - val_acc: 0.4626\n",
            "Epoch 39/1000\n",
            "297/297 [==============================] - 0s 603us/step - loss: 1.9235 - acc: 0.4411 - val_loss: 1.4285 - val_acc: 0.4422\n",
            "Epoch 40/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 1.9841 - acc: 0.4545 - val_loss: 0.9335 - val_acc: 0.4558\n",
            "Epoch 41/1000\n",
            "297/297 [==============================] - 0s 577us/step - loss: 1.9200 - acc: 0.4646 - val_loss: 0.9404 - val_acc: 0.4558\n",
            "Epoch 42/1000\n",
            "297/297 [==============================] - 0s 607us/step - loss: 1.7606 - acc: 0.4848 - val_loss: 0.9450 - val_acc: 0.4762\n",
            "Epoch 43/1000\n",
            "297/297 [==============================] - 0s 549us/step - loss: 1.9187 - acc: 0.4646 - val_loss: 1.4916 - val_acc: 0.4626\n",
            "Epoch 44/1000\n",
            "297/297 [==============================] - 0s 542us/step - loss: 1.8371 - acc: 0.4680 - val_loss: 0.9918 - val_acc: 0.5510\n",
            "Epoch 45/1000\n",
            "297/297 [==============================] - 0s 548us/step - loss: 1.9332 - acc: 0.4242 - val_loss: 0.9597 - val_acc: 0.4558\n",
            "Epoch 46/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 1.7173 - acc: 0.4714 - val_loss: 1.2323 - val_acc: 0.4898\n",
            "Epoch 47/1000\n",
            "297/297 [==============================] - 0s 579us/step - loss: 1.5392 - acc: 0.4983 - val_loss: 2.0158 - val_acc: 0.4218\n",
            "Epoch 48/1000\n",
            "297/297 [==============================] - 0s 612us/step - loss: 1.7296 - acc: 0.4747 - val_loss: 1.1149 - val_acc: 0.4898\n",
            "Epoch 49/1000\n",
            "297/297 [==============================] - 0s 582us/step - loss: 1.4426 - acc: 0.5320 - val_loss: 1.4944 - val_acc: 0.4966\n",
            "Epoch 50/1000\n",
            "297/297 [==============================] - 0s 606us/step - loss: 1.6089 - acc: 0.4916 - val_loss: 0.9782 - val_acc: 0.4898\n",
            "Epoch 51/1000\n",
            "297/297 [==============================] - 0s 576us/step - loss: 1.9012 - acc: 0.4411 - val_loss: 1.0365 - val_acc: 0.4966\n",
            "Epoch 52/1000\n",
            "297/297 [==============================] - 0s 579us/step - loss: 1.5921 - acc: 0.4815 - val_loss: 1.2416 - val_acc: 0.4694\n",
            "Epoch 53/1000\n",
            "297/297 [==============================] - 0s 559us/step - loss: 1.6326 - acc: 0.4815 - val_loss: 1.7802 - val_acc: 0.3741\n",
            "Epoch 54/1000\n",
            "297/297 [==============================] - 0s 565us/step - loss: 1.4135 - acc: 0.5657 - val_loss: 1.2186 - val_acc: 0.4694\n",
            "Epoch 55/1000\n",
            "297/297 [==============================] - 0s 555us/step - loss: 1.4940 - acc: 0.4680 - val_loss: 1.4810 - val_acc: 0.4558\n",
            "Epoch 56/1000\n",
            "297/297 [==============================] - 0s 702us/step - loss: 1.3794 - acc: 0.5017 - val_loss: 1.2197 - val_acc: 0.4898\n",
            "Epoch 57/1000\n",
            "297/297 [==============================] - 0s 597us/step - loss: 1.3612 - acc: 0.4815 - val_loss: 1.2200 - val_acc: 0.4354\n",
            "Epoch 58/1000\n",
            "297/297 [==============================] - 0s 580us/step - loss: 1.3201 - acc: 0.5286 - val_loss: 1.0054 - val_acc: 0.4830\n",
            "Epoch 59/1000\n",
            "297/297 [==============================] - 0s 603us/step - loss: 1.2671 - acc: 0.5084 - val_loss: 0.9731 - val_acc: 0.4898\n",
            "Epoch 60/1000\n",
            "297/297 [==============================] - 0s 582us/step - loss: 1.3704 - acc: 0.4882 - val_loss: 1.2766 - val_acc: 0.4966\n",
            "Epoch 61/1000\n",
            "297/297 [==============================] - 0s 564us/step - loss: 1.1441 - acc: 0.5589 - val_loss: 0.9393 - val_acc: 0.4422\n",
            "Epoch 62/1000\n",
            "297/297 [==============================] - 0s 572us/step - loss: 1.3167 - acc: 0.4848 - val_loss: 0.9095 - val_acc: 0.5442\n",
            "Epoch 63/1000\n",
            "297/297 [==============================] - 0s 595us/step - loss: 1.3560 - acc: 0.5084 - val_loss: 1.2606 - val_acc: 0.4490\n",
            "Epoch 64/1000\n",
            "297/297 [==============================] - 0s 599us/step - loss: 1.4158 - acc: 0.4848 - val_loss: 1.1307 - val_acc: 0.4830\n",
            "Epoch 65/1000\n",
            "297/297 [==============================] - 0s 624us/step - loss: 1.2932 - acc: 0.4680 - val_loss: 1.3212 - val_acc: 0.5034\n",
            "Epoch 66/1000\n",
            "297/297 [==============================] - 0s 617us/step - loss: 1.2555 - acc: 0.5084 - val_loss: 0.9118 - val_acc: 0.5782\n",
            "Epoch 67/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 1.3122 - acc: 0.4983 - val_loss: 1.1679 - val_acc: 0.5034\n",
            "Epoch 68/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 1.1167 - acc: 0.5253 - val_loss: 1.0153 - val_acc: 0.4966\n",
            "Epoch 69/1000\n",
            "297/297 [==============================] - 0s 596us/step - loss: 1.2489 - acc: 0.5185 - val_loss: 1.1950 - val_acc: 0.4762\n",
            "Epoch 70/1000\n",
            "297/297 [==============================] - 0s 597us/step - loss: 1.2069 - acc: 0.5286 - val_loss: 0.9533 - val_acc: 0.5442\n",
            "Epoch 71/1000\n",
            "297/297 [==============================] - 0s 599us/step - loss: 1.1741 - acc: 0.5286 - val_loss: 1.1070 - val_acc: 0.4626\n",
            "Epoch 72/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 1.2952 - acc: 0.4747 - val_loss: 1.0708 - val_acc: 0.5170\n",
            "Epoch 73/1000\n",
            "297/297 [==============================] - 0s 556us/step - loss: 1.0962 - acc: 0.5724 - val_loss: 1.0301 - val_acc: 0.4762\n",
            "Epoch 74/1000\n",
            "297/297 [==============================] - 0s 602us/step - loss: 1.1867 - acc: 0.4983 - val_loss: 0.9514 - val_acc: 0.4762\n",
            "Epoch 75/1000\n",
            "297/297 [==============================] - 0s 587us/step - loss: 1.1621 - acc: 0.4680 - val_loss: 1.0816 - val_acc: 0.4898\n",
            "Epoch 76/1000\n",
            "297/297 [==============================] - 0s 552us/step - loss: 1.2262 - acc: 0.4882 - val_loss: 1.0804 - val_acc: 0.4490\n",
            "Epoch 77/1000\n",
            "297/297 [==============================] - 0s 609us/step - loss: 1.0307 - acc: 0.5758 - val_loss: 1.0371 - val_acc: 0.5034\n",
            "Epoch 78/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 1.2365 - acc: 0.4747 - val_loss: 0.9089 - val_acc: 0.4898\n",
            "Epoch 79/1000\n",
            "297/297 [==============================] - 0s 601us/step - loss: 1.1098 - acc: 0.5556 - val_loss: 0.9797 - val_acc: 0.4966\n",
            "Epoch 80/1000\n",
            "297/297 [==============================] - 0s 614us/step - loss: 1.1140 - acc: 0.5421 - val_loss: 0.9291 - val_acc: 0.5306\n",
            "Epoch 81/1000\n",
            "297/297 [==============================] - 0s 602us/step - loss: 1.1460 - acc: 0.5421 - val_loss: 0.9972 - val_acc: 0.4150\n",
            "Epoch 82/1000\n",
            "297/297 [==============================] - 0s 603us/step - loss: 1.0741 - acc: 0.5522 - val_loss: 1.1151 - val_acc: 0.4218\n",
            "Epoch 83/1000\n",
            "297/297 [==============================] - 0s 587us/step - loss: 1.1594 - acc: 0.5354 - val_loss: 1.0021 - val_acc: 0.4966\n",
            "Epoch 84/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 1.1235 - acc: 0.5253 - val_loss: 0.9432 - val_acc: 0.4966\n",
            "Epoch 85/1000\n",
            "297/297 [==============================] - 0s 599us/step - loss: 1.0519 - acc: 0.5556 - val_loss: 1.3673 - val_acc: 0.3741\n",
            "Epoch 86/1000\n",
            "297/297 [==============================] - 0s 577us/step - loss: 1.0382 - acc: 0.5387 - val_loss: 0.9558 - val_acc: 0.4830\n",
            "Epoch 87/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 1.0839 - acc: 0.5118 - val_loss: 1.1720 - val_acc: 0.5102\n",
            "Epoch 88/1000\n",
            "297/297 [==============================] - 0s 618us/step - loss: 1.0607 - acc: 0.5253 - val_loss: 1.1154 - val_acc: 0.4218\n",
            "Epoch 89/1000\n",
            "297/297 [==============================] - 0s 584us/step - loss: 1.0284 - acc: 0.5354 - val_loss: 1.0264 - val_acc: 0.5034\n",
            "Epoch 90/1000\n",
            "297/297 [==============================] - 0s 604us/step - loss: 0.9912 - acc: 0.5354 - val_loss: 1.0379 - val_acc: 0.5034\n",
            "Epoch 91/1000\n",
            "297/297 [==============================] - 0s 561us/step - loss: 0.9786 - acc: 0.5623 - val_loss: 0.9926 - val_acc: 0.4966\n",
            "Epoch 92/1000\n",
            "297/297 [==============================] - 0s 613us/step - loss: 1.0641 - acc: 0.5488 - val_loss: 1.0850 - val_acc: 0.4626\n",
            "Epoch 93/1000\n",
            "297/297 [==============================] - 0s 559us/step - loss: 1.0684 - acc: 0.5320 - val_loss: 0.9890 - val_acc: 0.4558\n",
            "Epoch 94/1000\n",
            "297/297 [==============================] - 0s 551us/step - loss: 1.0758 - acc: 0.5690 - val_loss: 0.9383 - val_acc: 0.5510\n",
            "Epoch 95/1000\n",
            "297/297 [==============================] - 0s 572us/step - loss: 1.0209 - acc: 0.5219 - val_loss: 1.1086 - val_acc: 0.4558\n",
            "Epoch 96/1000\n",
            "297/297 [==============================] - 0s 590us/step - loss: 1.1211 - acc: 0.5219 - val_loss: 1.0608 - val_acc: 0.4966\n",
            "Epoch 97/1000\n",
            "297/297 [==============================] - 0s 553us/step - loss: 1.0434 - acc: 0.4949 - val_loss: 0.9452 - val_acc: 0.5170\n",
            "Epoch 98/1000\n",
            "297/297 [==============================] - 0s 610us/step - loss: 0.9730 - acc: 0.5522 - val_loss: 0.9232 - val_acc: 0.5510\n",
            "Epoch 99/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 0.9685 - acc: 0.5488 - val_loss: 0.9134 - val_acc: 0.5306\n",
            "Epoch 100/1000\n",
            "297/297 [==============================] - 0s 582us/step - loss: 0.9253 - acc: 0.5926 - val_loss: 1.0841 - val_acc: 0.4354\n",
            "Epoch 101/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.9478 - acc: 0.5354 - val_loss: 1.1170 - val_acc: 0.5034\n",
            "Epoch 102/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.9636 - acc: 0.5724 - val_loss: 0.9072 - val_acc: 0.4966\n",
            "Epoch 103/1000\n",
            "297/297 [==============================] - 0s 612us/step - loss: 0.9480 - acc: 0.5960 - val_loss: 0.9226 - val_acc: 0.5034\n",
            "Epoch 104/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.9938 - acc: 0.5623 - val_loss: 0.9290 - val_acc: 0.4898\n",
            "Epoch 105/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 0.8865 - acc: 0.5690 - val_loss: 0.9815 - val_acc: 0.5102\n",
            "Epoch 106/1000\n",
            "297/297 [==============================] - 0s 580us/step - loss: 0.9600 - acc: 0.5488 - val_loss: 1.0917 - val_acc: 0.4626\n",
            "Epoch 107/1000\n",
            "297/297 [==============================] - 0s 567us/step - loss: 1.0197 - acc: 0.5084 - val_loss: 0.9244 - val_acc: 0.5102\n",
            "Epoch 108/1000\n",
            "297/297 [==============================] - 0s 596us/step - loss: 1.0112 - acc: 0.5455 - val_loss: 0.9247 - val_acc: 0.5170\n",
            "Epoch 109/1000\n",
            "297/297 [==============================] - 0s 534us/step - loss: 0.9320 - acc: 0.5690 - val_loss: 0.9154 - val_acc: 0.5238\n",
            "Epoch 110/1000\n",
            "297/297 [==============================] - 0s 554us/step - loss: 0.9600 - acc: 0.5758 - val_loss: 1.0527 - val_acc: 0.4830\n",
            "Epoch 111/1000\n",
            "297/297 [==============================] - 0s 610us/step - loss: 0.9593 - acc: 0.5522 - val_loss: 0.9152 - val_acc: 0.5238\n",
            "Epoch 112/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.9481 - acc: 0.5589 - val_loss: 1.0023 - val_acc: 0.5374\n",
            "Epoch 113/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.9642 - acc: 0.5791 - val_loss: 0.9381 - val_acc: 0.5034\n",
            "Epoch 114/1000\n",
            "297/297 [==============================] - 0s 623us/step - loss: 0.9196 - acc: 0.5758 - val_loss: 0.9974 - val_acc: 0.4490\n",
            "Epoch 115/1000\n",
            "297/297 [==============================] - 0s 640us/step - loss: 0.9735 - acc: 0.5926 - val_loss: 0.9127 - val_acc: 0.5102\n",
            "Epoch 116/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.9259 - acc: 0.5488 - val_loss: 0.9171 - val_acc: 0.4966\n",
            "Epoch 117/1000\n",
            "297/297 [==============================] - 0s 561us/step - loss: 0.9054 - acc: 0.5253 - val_loss: 1.1354 - val_acc: 0.4490\n",
            "Epoch 118/1000\n",
            "297/297 [==============================] - 0s 548us/step - loss: 0.9439 - acc: 0.5825 - val_loss: 0.9047 - val_acc: 0.5510\n",
            "Epoch 119/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.8987 - acc: 0.5657 - val_loss: 0.8980 - val_acc: 0.5442\n",
            "Epoch 120/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.9334 - acc: 0.5589 - val_loss: 0.9355 - val_acc: 0.5102\n",
            "Epoch 121/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 0.9189 - acc: 0.5791 - val_loss: 0.9379 - val_acc: 0.5238\n",
            "Epoch 122/1000\n",
            "297/297 [==============================] - 0s 620us/step - loss: 0.8862 - acc: 0.5724 - val_loss: 0.9521 - val_acc: 0.5034\n",
            "Epoch 123/1000\n",
            "297/297 [==============================] - 0s 595us/step - loss: 0.9559 - acc: 0.5522 - val_loss: 0.9335 - val_acc: 0.5034\n",
            "Epoch 124/1000\n",
            "297/297 [==============================] - 0s 577us/step - loss: 0.8921 - acc: 0.5960 - val_loss: 0.9253 - val_acc: 0.5034\n",
            "Epoch 125/1000\n",
            "297/297 [==============================] - 0s 587us/step - loss: 0.9535 - acc: 0.5690 - val_loss: 0.9470 - val_acc: 0.5170\n",
            "Epoch 126/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.8804 - acc: 0.5960 - val_loss: 0.8948 - val_acc: 0.5714\n",
            "Epoch 127/1000\n",
            "297/297 [==============================] - 0s 621us/step - loss: 0.8883 - acc: 0.6027 - val_loss: 0.9590 - val_acc: 0.4966\n",
            "Epoch 128/1000\n",
            "297/297 [==============================] - 0s 590us/step - loss: 0.8547 - acc: 0.6162 - val_loss: 0.8998 - val_acc: 0.5442\n",
            "Epoch 129/1000\n",
            "297/297 [==============================] - 0s 604us/step - loss: 0.8966 - acc: 0.5859 - val_loss: 0.9367 - val_acc: 0.5102\n",
            "Epoch 130/1000\n",
            "297/297 [==============================] - 0s 577us/step - loss: 1.0027 - acc: 0.5084 - val_loss: 0.9247 - val_acc: 0.5238\n",
            "Epoch 131/1000\n",
            "297/297 [==============================] - 0s 591us/step - loss: 0.8623 - acc: 0.5960 - val_loss: 0.8942 - val_acc: 0.5510\n",
            "Epoch 132/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.9206 - acc: 0.5825 - val_loss: 0.9337 - val_acc: 0.5306\n",
            "Epoch 133/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.9279 - acc: 0.5623 - val_loss: 0.8963 - val_acc: 0.5374\n",
            "Epoch 134/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.8632 - acc: 0.5859 - val_loss: 0.9075 - val_acc: 0.5374\n",
            "Epoch 135/1000\n",
            "297/297 [==============================] - 0s 555us/step - loss: 0.9255 - acc: 0.5690 - val_loss: 0.9411 - val_acc: 0.4966\n",
            "Epoch 136/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.9192 - acc: 0.5791 - val_loss: 0.9128 - val_acc: 0.5238\n",
            "Epoch 137/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.8376 - acc: 0.6094 - val_loss: 0.9458 - val_acc: 0.5374\n",
            "Epoch 138/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.8888 - acc: 0.5690 - val_loss: 0.9409 - val_acc: 0.4966\n",
            "Epoch 139/1000\n",
            "297/297 [==============================] - 0s 579us/step - loss: 0.8735 - acc: 0.5589 - val_loss: 0.9656 - val_acc: 0.4558\n",
            "Epoch 140/1000\n",
            "297/297 [==============================] - 0s 544us/step - loss: 0.8409 - acc: 0.6128 - val_loss: 0.9381 - val_acc: 0.5034\n",
            "Epoch 141/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 0.8930 - acc: 0.5892 - val_loss: 0.9273 - val_acc: 0.5034\n",
            "Epoch 142/1000\n",
            "297/297 [==============================] - 0s 612us/step - loss: 0.8592 - acc: 0.5657 - val_loss: 0.8912 - val_acc: 0.5306\n",
            "Epoch 143/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 0.8085 - acc: 0.6061 - val_loss: 0.9151 - val_acc: 0.4966\n",
            "Epoch 144/1000\n",
            "297/297 [==============================] - 0s 621us/step - loss: 0.9203 - acc: 0.5657 - val_loss: 0.8952 - val_acc: 0.5306\n",
            "Epoch 145/1000\n",
            "297/297 [==============================] - 0s 624us/step - loss: 0.8825 - acc: 0.5758 - val_loss: 0.9120 - val_acc: 0.5238\n",
            "Epoch 146/1000\n",
            "297/297 [==============================] - 0s 547us/step - loss: 0.8653 - acc: 0.5960 - val_loss: 0.8973 - val_acc: 0.5306\n",
            "Epoch 147/1000\n",
            "297/297 [==============================] - 0s 601us/step - loss: 0.8538 - acc: 0.5993 - val_loss: 0.8953 - val_acc: 0.5714\n",
            "Epoch 148/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.8496 - acc: 0.5960 - val_loss: 0.9216 - val_acc: 0.5102\n",
            "Epoch 149/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 0.8351 - acc: 0.5690 - val_loss: 0.9003 - val_acc: 0.5238\n",
            "Epoch 150/1000\n",
            "297/297 [==============================] - 0s 613us/step - loss: 0.9115 - acc: 0.5926 - val_loss: 0.8906 - val_acc: 0.5578\n",
            "Epoch 151/1000\n",
            "297/297 [==============================] - 0s 634us/step - loss: 0.8672 - acc: 0.5993 - val_loss: 0.9770 - val_acc: 0.5170\n",
            "Epoch 152/1000\n",
            "297/297 [==============================] - 0s 612us/step - loss: 0.8385 - acc: 0.6027 - val_loss: 0.9291 - val_acc: 0.4694\n",
            "Epoch 153/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.8306 - acc: 0.5960 - val_loss: 0.9399 - val_acc: 0.5238\n",
            "Epoch 154/1000\n",
            "297/297 [==============================] - 0s 579us/step - loss: 0.8437 - acc: 0.6128 - val_loss: 0.9406 - val_acc: 0.5306\n",
            "Epoch 155/1000\n",
            "297/297 [==============================] - 0s 565us/step - loss: 0.8114 - acc: 0.6195 - val_loss: 0.8961 - val_acc: 0.5170\n",
            "Epoch 156/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.8345 - acc: 0.5960 - val_loss: 0.9197 - val_acc: 0.5102\n",
            "Epoch 157/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.8536 - acc: 0.5825 - val_loss: 0.8962 - val_acc: 0.5238\n",
            "Epoch 158/1000\n",
            "297/297 [==============================] - 0s 554us/step - loss: 0.8515 - acc: 0.5791 - val_loss: 0.8973 - val_acc: 0.5306\n",
            "Epoch 159/1000\n",
            "297/297 [==============================] - 0s 606us/step - loss: 0.7936 - acc: 0.6162 - val_loss: 0.8907 - val_acc: 0.5374\n",
            "Epoch 160/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 0.8168 - acc: 0.6162 - val_loss: 0.8951 - val_acc: 0.5442\n",
            "Epoch 161/1000\n",
            "297/297 [==============================] - 0s 587us/step - loss: 0.8147 - acc: 0.5960 - val_loss: 0.9194 - val_acc: 0.5238\n",
            "Epoch 162/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.8555 - acc: 0.5892 - val_loss: 0.8878 - val_acc: 0.5510\n",
            "Epoch 163/1000\n",
            "297/297 [==============================] - 0s 565us/step - loss: 0.8350 - acc: 0.5859 - val_loss: 0.8845 - val_acc: 0.5646\n",
            "Epoch 164/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 0.8243 - acc: 0.6061 - val_loss: 0.8831 - val_acc: 0.5374\n",
            "Epoch 165/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 0.8206 - acc: 0.6094 - val_loss: 0.8950 - val_acc: 0.5306\n",
            "Epoch 166/1000\n",
            "297/297 [==============================] - 0s 584us/step - loss: 0.8199 - acc: 0.6263 - val_loss: 0.9021 - val_acc: 0.5238\n",
            "Epoch 167/1000\n",
            "297/297 [==============================] - 0s 621us/step - loss: 0.8230 - acc: 0.5993 - val_loss: 0.8954 - val_acc: 0.5442\n",
            "Epoch 168/1000\n",
            "297/297 [==============================] - 0s 601us/step - loss: 0.8322 - acc: 0.6061 - val_loss: 0.9161 - val_acc: 0.5306\n",
            "Epoch 169/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 0.7294 - acc: 0.6835 - val_loss: 0.9494 - val_acc: 0.4694\n",
            "Epoch 170/1000\n",
            "297/297 [==============================] - 0s 601us/step - loss: 0.8409 - acc: 0.6296 - val_loss: 0.8879 - val_acc: 0.5306\n",
            "Epoch 171/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.8259 - acc: 0.5892 - val_loss: 0.8620 - val_acc: 0.5510\n",
            "Epoch 172/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.8046 - acc: 0.6229 - val_loss: 0.8630 - val_acc: 0.5646\n",
            "Epoch 173/1000\n",
            "297/297 [==============================] - 0s 719us/step - loss: 0.8204 - acc: 0.6229 - val_loss: 0.8755 - val_acc: 0.5578\n",
            "Epoch 174/1000\n",
            "297/297 [==============================] - 0s 569us/step - loss: 0.8090 - acc: 0.6397 - val_loss: 0.8870 - val_acc: 0.5306\n",
            "Epoch 175/1000\n",
            "297/297 [==============================] - 0s 571us/step - loss: 0.8058 - acc: 0.6061 - val_loss: 0.8966 - val_acc: 0.5510\n",
            "Epoch 176/1000\n",
            "297/297 [==============================] - 0s 582us/step - loss: 0.7836 - acc: 0.6296 - val_loss: 0.8991 - val_acc: 0.5374\n",
            "Epoch 177/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.7953 - acc: 0.6195 - val_loss: 0.8906 - val_acc: 0.5442\n",
            "Epoch 178/1000\n",
            "297/297 [==============================] - 0s 604us/step - loss: 0.7543 - acc: 0.6498 - val_loss: 0.8649 - val_acc: 0.5306\n",
            "Epoch 179/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 0.8056 - acc: 0.6229 - val_loss: 0.9159 - val_acc: 0.4966\n",
            "Epoch 180/1000\n",
            "297/297 [==============================] - 0s 591us/step - loss: 0.8195 - acc: 0.6431 - val_loss: 0.8921 - val_acc: 0.5646\n",
            "Epoch 181/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 0.7601 - acc: 0.6296 - val_loss: 0.8941 - val_acc: 0.5442\n",
            "Epoch 182/1000\n",
            "297/297 [==============================] - 0s 596us/step - loss: 0.8016 - acc: 0.6195 - val_loss: 0.8885 - val_acc: 0.5578\n",
            "Epoch 183/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.7708 - acc: 0.6566 - val_loss: 0.8624 - val_acc: 0.5850\n",
            "Epoch 184/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.7883 - acc: 0.6667 - val_loss: 0.8493 - val_acc: 0.5782\n",
            "Epoch 185/1000\n",
            "297/297 [==============================] - 0s 602us/step - loss: 0.8030 - acc: 0.6229 - val_loss: 0.8624 - val_acc: 0.5510\n",
            "Epoch 186/1000\n",
            "297/297 [==============================] - 0s 587us/step - loss: 0.7473 - acc: 0.6734 - val_loss: 0.8723 - val_acc: 0.5510\n",
            "Epoch 187/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 0.8084 - acc: 0.6195 - val_loss: 0.8814 - val_acc: 0.5442\n",
            "Epoch 188/1000\n",
            "297/297 [==============================] - 0s 595us/step - loss: 0.7999 - acc: 0.5960 - val_loss: 0.9067 - val_acc: 0.5646\n",
            "Epoch 189/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.8043 - acc: 0.6128 - val_loss: 0.8596 - val_acc: 0.5782\n",
            "Epoch 190/1000\n",
            "297/297 [==============================] - 0s 618us/step - loss: 0.7411 - acc: 0.6633 - val_loss: 0.8608 - val_acc: 0.5714\n",
            "Epoch 191/1000\n",
            "297/297 [==============================] - 0s 591us/step - loss: 0.7339 - acc: 0.6633 - val_loss: 0.8866 - val_acc: 0.5238\n",
            "Epoch 192/1000\n",
            "297/297 [==============================] - 0s 567us/step - loss: 0.7492 - acc: 0.6195 - val_loss: 0.8888 - val_acc: 0.5306\n",
            "Epoch 193/1000\n",
            "297/297 [==============================] - 0s 580us/step - loss: 0.7916 - acc: 0.6599 - val_loss: 0.8699 - val_acc: 0.5442\n",
            "Epoch 194/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.7931 - acc: 0.6229 - val_loss: 0.8930 - val_acc: 0.5170\n",
            "Epoch 195/1000\n",
            "297/297 [==============================] - 0s 564us/step - loss: 0.7716 - acc: 0.6599 - val_loss: 0.9083 - val_acc: 0.5306\n",
            "Epoch 196/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 0.7626 - acc: 0.6700 - val_loss: 0.9338 - val_acc: 0.5306\n",
            "Epoch 197/1000\n",
            "297/297 [==============================] - 0s 609us/step - loss: 0.7880 - acc: 0.6330 - val_loss: 0.8830 - val_acc: 0.5510\n",
            "Epoch 198/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.7930 - acc: 0.6465 - val_loss: 0.8626 - val_acc: 0.5986\n",
            "Epoch 199/1000\n",
            "297/297 [==============================] - 0s 619us/step - loss: 0.7288 - acc: 0.6397 - val_loss: 0.9328 - val_acc: 0.4694\n",
            "Epoch 200/1000\n",
            "297/297 [==============================] - 0s 577us/step - loss: 0.7645 - acc: 0.6330 - val_loss: 0.8898 - val_acc: 0.5442\n",
            "Epoch 201/1000\n",
            "297/297 [==============================] - 0s 605us/step - loss: 0.7640 - acc: 0.6667 - val_loss: 0.8675 - val_acc: 0.5442\n",
            "Epoch 202/1000\n",
            "297/297 [==============================] - 0s 563us/step - loss: 0.7758 - acc: 0.6195 - val_loss: 0.8739 - val_acc: 0.5714\n",
            "Epoch 203/1000\n",
            "297/297 [==============================] - 0s 556us/step - loss: 0.7334 - acc: 0.6734 - val_loss: 0.9039 - val_acc: 0.5374\n",
            "Epoch 204/1000\n",
            "297/297 [==============================] - 0s 591us/step - loss: 0.7191 - acc: 0.6902 - val_loss: 0.9051 - val_acc: 0.5374\n",
            "Epoch 205/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.7532 - acc: 0.6465 - val_loss: 0.8549 - val_acc: 0.5374\n",
            "Epoch 206/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.7578 - acc: 0.6633 - val_loss: 0.8478 - val_acc: 0.5782\n",
            "Epoch 207/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.7809 - acc: 0.6599 - val_loss: 0.8528 - val_acc: 0.5782\n",
            "Epoch 208/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.7672 - acc: 0.6330 - val_loss: 0.8554 - val_acc: 0.5782\n",
            "Epoch 209/1000\n",
            "297/297 [==============================] - 0s 557us/step - loss: 0.7334 - acc: 0.6364 - val_loss: 0.8343 - val_acc: 0.5442\n",
            "Epoch 210/1000\n",
            "297/297 [==============================] - 0s 643us/step - loss: 0.7509 - acc: 0.6431 - val_loss: 0.8372 - val_acc: 0.5306\n",
            "Epoch 211/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.7485 - acc: 0.6869 - val_loss: 0.8684 - val_acc: 0.5374\n",
            "Epoch 212/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.7420 - acc: 0.6768 - val_loss: 0.8701 - val_acc: 0.5442\n",
            "Epoch 213/1000\n",
            "297/297 [==============================] - 0s 611us/step - loss: 0.7373 - acc: 0.6465 - val_loss: 0.8420 - val_acc: 0.5510\n",
            "Epoch 214/1000\n",
            "297/297 [==============================] - 0s 580us/step - loss: 0.7490 - acc: 0.6431 - val_loss: 0.8389 - val_acc: 0.5646\n",
            "Epoch 215/1000\n",
            "297/297 [==============================] - 0s 612us/step - loss: 0.7804 - acc: 0.6431 - val_loss: 0.8597 - val_acc: 0.5442\n",
            "Epoch 216/1000\n",
            "297/297 [==============================] - 0s 619us/step - loss: 0.7360 - acc: 0.6633 - val_loss: 0.8431 - val_acc: 0.5782\n",
            "Epoch 217/1000\n",
            "297/297 [==============================] - 0s 554us/step - loss: 0.7349 - acc: 0.6667 - val_loss: 0.8756 - val_acc: 0.5442\n",
            "Epoch 218/1000\n",
            "297/297 [==============================] - 0s 597us/step - loss: 0.7506 - acc: 0.6599 - val_loss: 0.8600 - val_acc: 0.5442\n",
            "Epoch 219/1000\n",
            "297/297 [==============================] - 0s 545us/step - loss: 0.6929 - acc: 0.7003 - val_loss: 0.8521 - val_acc: 0.5510\n",
            "Epoch 220/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.7309 - acc: 0.6532 - val_loss: 0.8531 - val_acc: 0.5646\n",
            "Epoch 221/1000\n",
            "297/297 [==============================] - 0s 582us/step - loss: 0.7452 - acc: 0.6633 - val_loss: 0.8348 - val_acc: 0.5850\n",
            "Epoch 222/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 0.7436 - acc: 0.6397 - val_loss: 0.8347 - val_acc: 0.5850\n",
            "Epoch 223/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.7556 - acc: 0.6364 - val_loss: 0.8463 - val_acc: 0.5918\n",
            "Epoch 224/1000\n",
            "297/297 [==============================] - 0s 590us/step - loss: 0.6991 - acc: 0.6835 - val_loss: 0.8271 - val_acc: 0.5646\n",
            "Epoch 225/1000\n",
            "297/297 [==============================] - 0s 587us/step - loss: 0.7250 - acc: 0.6465 - val_loss: 0.8314 - val_acc: 0.5918\n",
            "Epoch 226/1000\n",
            "297/297 [==============================] - 0s 565us/step - loss: 0.6837 - acc: 0.6869 - val_loss: 0.8371 - val_acc: 0.5918\n",
            "Epoch 227/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.7168 - acc: 0.6532 - val_loss: 0.8498 - val_acc: 0.5714\n",
            "Epoch 228/1000\n",
            "297/297 [==============================] - 0s 629us/step - loss: 0.7576 - acc: 0.6431 - val_loss: 0.8436 - val_acc: 0.5918\n",
            "Epoch 229/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 0.7279 - acc: 0.6801 - val_loss: 0.8463 - val_acc: 0.5714\n",
            "Epoch 230/1000\n",
            "297/297 [==============================] - 0s 618us/step - loss: 0.6925 - acc: 0.6801 - val_loss: 0.8384 - val_acc: 0.5374\n",
            "Epoch 231/1000\n",
            "297/297 [==============================] - 0s 671us/step - loss: 0.7313 - acc: 0.6599 - val_loss: 0.8627 - val_acc: 0.5510\n",
            "Epoch 232/1000\n",
            "297/297 [==============================] - 0s 614us/step - loss: 0.7043 - acc: 0.6970 - val_loss: 0.8503 - val_acc: 0.5578\n",
            "Epoch 233/1000\n",
            "297/297 [==============================] - 0s 572us/step - loss: 0.7218 - acc: 0.6768 - val_loss: 0.8430 - val_acc: 0.5714\n",
            "Epoch 234/1000\n",
            "297/297 [==============================] - 0s 612us/step - loss: 0.6834 - acc: 0.7138 - val_loss: 0.8950 - val_acc: 0.5510\n",
            "Epoch 235/1000\n",
            "297/297 [==============================] - 0s 629us/step - loss: 0.7035 - acc: 0.6869 - val_loss: 0.8842 - val_acc: 0.5442\n",
            "Epoch 236/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.7243 - acc: 0.6498 - val_loss: 0.8981 - val_acc: 0.5510\n",
            "Epoch 237/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.7139 - acc: 0.7104 - val_loss: 0.8633 - val_acc: 0.5374\n",
            "Epoch 238/1000\n",
            "297/297 [==============================] - 0s 566us/step - loss: 0.6702 - acc: 0.7071 - val_loss: 0.8859 - val_acc: 0.5374\n",
            "Epoch 239/1000\n",
            "297/297 [==============================] - 0s 597us/step - loss: 0.7082 - acc: 0.6902 - val_loss: 0.8463 - val_acc: 0.5714\n",
            "Epoch 240/1000\n",
            "297/297 [==============================] - 0s 603us/step - loss: 0.6503 - acc: 0.7037 - val_loss: 0.8479 - val_acc: 0.5850\n",
            "Epoch 241/1000\n",
            "297/297 [==============================] - 0s 546us/step - loss: 0.6651 - acc: 0.6936 - val_loss: 0.9408 - val_acc: 0.5238\n",
            "Epoch 242/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.6977 - acc: 0.6801 - val_loss: 0.9832 - val_acc: 0.5034\n",
            "Epoch 243/1000\n",
            "297/297 [==============================] - 0s 577us/step - loss: 0.6615 - acc: 0.6835 - val_loss: 0.8688 - val_acc: 0.5578\n",
            "Epoch 244/1000\n",
            "297/297 [==============================] - 0s 604us/step - loss: 0.7290 - acc: 0.6397 - val_loss: 0.8364 - val_acc: 0.5918\n",
            "Epoch 245/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.7037 - acc: 0.6700 - val_loss: 0.8279 - val_acc: 0.5986\n",
            "Epoch 246/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.6861 - acc: 0.6734 - val_loss: 0.8514 - val_acc: 0.5578\n",
            "Epoch 247/1000\n",
            "297/297 [==============================] - 0s 566us/step - loss: 0.6840 - acc: 0.7037 - val_loss: 0.8340 - val_acc: 0.5850\n",
            "Epoch 248/1000\n",
            "297/297 [==============================] - 0s 599us/step - loss: 0.6832 - acc: 0.7037 - val_loss: 0.8422 - val_acc: 0.5986\n",
            "Epoch 249/1000\n",
            "297/297 [==============================] - 0s 553us/step - loss: 0.6826 - acc: 0.7340 - val_loss: 0.8464 - val_acc: 0.5714\n",
            "Epoch 250/1000\n",
            "297/297 [==============================] - 0s 571us/step - loss: 0.6540 - acc: 0.7071 - val_loss: 0.8184 - val_acc: 0.6122\n",
            "Epoch 251/1000\n",
            "297/297 [==============================] - 0s 609us/step - loss: 0.6956 - acc: 0.7003 - val_loss: 0.8304 - val_acc: 0.5850\n",
            "Epoch 252/1000\n",
            "297/297 [==============================] - 0s 619us/step - loss: 0.6975 - acc: 0.7138 - val_loss: 0.8272 - val_acc: 0.5646\n",
            "Epoch 253/1000\n",
            "297/297 [==============================] - 0s 553us/step - loss: 0.6403 - acc: 0.7138 - val_loss: 0.8580 - val_acc: 0.5306\n",
            "Epoch 254/1000\n",
            "297/297 [==============================] - 0s 596us/step - loss: 0.6961 - acc: 0.6902 - val_loss: 0.8904 - val_acc: 0.5102\n",
            "Epoch 255/1000\n",
            "297/297 [==============================] - 0s 613us/step - loss: 0.6798 - acc: 0.7037 - val_loss: 0.8737 - val_acc: 0.5510\n",
            "Epoch 256/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.6986 - acc: 0.6936 - val_loss: 0.8195 - val_acc: 0.5918\n",
            "Epoch 257/1000\n",
            "297/297 [==============================] - 0s 605us/step - loss: 0.6651 - acc: 0.7071 - val_loss: 0.8121 - val_acc: 0.6190\n",
            "Epoch 258/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.6686 - acc: 0.7205 - val_loss: 0.8074 - val_acc: 0.6122\n",
            "Epoch 259/1000\n",
            "297/297 [==============================] - 0s 615us/step - loss: 0.6848 - acc: 0.6700 - val_loss: 0.9005 - val_acc: 0.5238\n",
            "Epoch 260/1000\n",
            "297/297 [==============================] - 0s 580us/step - loss: 0.6509 - acc: 0.7003 - val_loss: 0.8733 - val_acc: 0.5306\n",
            "Epoch 261/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.6879 - acc: 0.6902 - val_loss: 0.8354 - val_acc: 0.5578\n",
            "Epoch 262/1000\n",
            "297/297 [==============================] - 0s 601us/step - loss: 0.6517 - acc: 0.7071 - val_loss: 0.8560 - val_acc: 0.5442\n",
            "Epoch 263/1000\n",
            "297/297 [==============================] - 0s 541us/step - loss: 0.6279 - acc: 0.7475 - val_loss: 0.8747 - val_acc: 0.5578\n",
            "Epoch 264/1000\n",
            "297/297 [==============================] - 0s 575us/step - loss: 0.6815 - acc: 0.7003 - val_loss: 0.8192 - val_acc: 0.6122\n",
            "Epoch 265/1000\n",
            "297/297 [==============================] - 0s 596us/step - loss: 0.6738 - acc: 0.6936 - val_loss: 0.8197 - val_acc: 0.6190\n",
            "Epoch 266/1000\n",
            "297/297 [==============================] - 0s 562us/step - loss: 0.6843 - acc: 0.6936 - val_loss: 0.8156 - val_acc: 0.6122\n",
            "Epoch 267/1000\n",
            "297/297 [==============================] - 0s 601us/step - loss: 0.6690 - acc: 0.6801 - val_loss: 0.8396 - val_acc: 0.5850\n",
            "Epoch 268/1000\n",
            "297/297 [==============================] - 0s 580us/step - loss: 0.6569 - acc: 0.7003 - val_loss: 0.8533 - val_acc: 0.5510\n",
            "Epoch 269/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.6391 - acc: 0.7172 - val_loss: 0.8411 - val_acc: 0.5782\n",
            "Epoch 270/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 0.6588 - acc: 0.6835 - val_loss: 0.8584 - val_acc: 0.5714\n",
            "Epoch 271/1000\n",
            "297/297 [==============================] - 0s 567us/step - loss: 0.6653 - acc: 0.6970 - val_loss: 0.8237 - val_acc: 0.5578\n",
            "Epoch 272/1000\n",
            "297/297 [==============================] - 0s 590us/step - loss: 0.7078 - acc: 0.6532 - val_loss: 0.8100 - val_acc: 0.5986\n",
            "Epoch 273/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.6749 - acc: 0.7003 - val_loss: 0.8194 - val_acc: 0.5850\n",
            "Epoch 274/1000\n",
            "297/297 [==============================] - 0s 610us/step - loss: 0.6652 - acc: 0.6869 - val_loss: 0.8238 - val_acc: 0.5782\n",
            "Epoch 275/1000\n",
            "297/297 [==============================] - 0s 577us/step - loss: 0.6469 - acc: 0.7172 - val_loss: 0.8122 - val_acc: 0.6190\n",
            "Epoch 276/1000\n",
            "297/297 [==============================] - 0s 571us/step - loss: 0.6482 - acc: 0.7003 - val_loss: 0.8593 - val_acc: 0.5442\n",
            "Epoch 277/1000\n",
            "297/297 [==============================] - 0s 530us/step - loss: 0.6312 - acc: 0.7306 - val_loss: 0.8774 - val_acc: 0.5578\n",
            "Epoch 278/1000\n",
            "297/297 [==============================] - 0s 596us/step - loss: 0.6330 - acc: 0.7306 - val_loss: 0.8774 - val_acc: 0.5578\n",
            "Epoch 279/1000\n",
            "297/297 [==============================] - 0s 546us/step - loss: 0.6793 - acc: 0.6801 - val_loss: 0.8145 - val_acc: 0.6122\n",
            "Epoch 280/1000\n",
            "297/297 [==============================] - 0s 567us/step - loss: 0.6208 - acc: 0.7475 - val_loss: 0.8356 - val_acc: 0.5782\n",
            "Epoch 281/1000\n",
            "297/297 [==============================] - 0s 554us/step - loss: 0.6566 - acc: 0.7205 - val_loss: 0.8800 - val_acc: 0.5306\n",
            "Epoch 282/1000\n",
            "297/297 [==============================] - 0s 575us/step - loss: 0.6347 - acc: 0.7205 - val_loss: 0.8484 - val_acc: 0.5374\n",
            "Epoch 283/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.6543 - acc: 0.7104 - val_loss: 0.8223 - val_acc: 0.5850\n",
            "Epoch 284/1000\n",
            "297/297 [==============================] - 0s 575us/step - loss: 0.6110 - acc: 0.7475 - val_loss: 0.8497 - val_acc: 0.5578\n",
            "Epoch 285/1000\n",
            "297/297 [==============================] - 0s 542us/step - loss: 0.6620 - acc: 0.6970 - val_loss: 0.8112 - val_acc: 0.5850\n",
            "Epoch 286/1000\n",
            "297/297 [==============================] - 0s 548us/step - loss: 0.6375 - acc: 0.7239 - val_loss: 0.9492 - val_acc: 0.4354\n",
            "Epoch 287/1000\n",
            "297/297 [==============================] - 0s 551us/step - loss: 0.6339 - acc: 0.7306 - val_loss: 0.8049 - val_acc: 0.6122\n",
            "Epoch 288/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.6466 - acc: 0.7104 - val_loss: 0.8055 - val_acc: 0.6122\n",
            "Epoch 289/1000\n",
            "297/297 [==============================] - 0s 543us/step - loss: 0.5805 - acc: 0.7576 - val_loss: 0.8342 - val_acc: 0.5986\n",
            "Epoch 290/1000\n",
            "297/297 [==============================] - 0s 662us/step - loss: 0.6450 - acc: 0.7306 - val_loss: 0.8079 - val_acc: 0.5918\n",
            "Epoch 291/1000\n",
            "297/297 [==============================] - 0s 611us/step - loss: 0.6498 - acc: 0.7037 - val_loss: 0.8237 - val_acc: 0.5714\n",
            "Epoch 292/1000\n",
            "297/297 [==============================] - 0s 566us/step - loss: 0.5965 - acc: 0.7441 - val_loss: 0.7985 - val_acc: 0.6122\n",
            "Epoch 293/1000\n",
            "297/297 [==============================] - 0s 548us/step - loss: 0.6046 - acc: 0.7508 - val_loss: 0.8398 - val_acc: 0.5918\n",
            "Epoch 294/1000\n",
            "297/297 [==============================] - 0s 546us/step - loss: 0.6028 - acc: 0.7542 - val_loss: 0.7948 - val_acc: 0.6190\n",
            "Epoch 295/1000\n",
            "297/297 [==============================] - 0s 565us/step - loss: 0.5956 - acc: 0.7407 - val_loss: 0.7984 - val_acc: 0.5510\n",
            "Epoch 296/1000\n",
            "297/297 [==============================] - 0s 565us/step - loss: 0.6258 - acc: 0.7273 - val_loss: 0.8438 - val_acc: 0.6054\n",
            "Epoch 297/1000\n",
            "297/297 [==============================] - 0s 551us/step - loss: 0.6423 - acc: 0.7172 - val_loss: 0.8003 - val_acc: 0.5850\n",
            "Epoch 298/1000\n",
            "297/297 [==============================] - 0s 556us/step - loss: 0.6658 - acc: 0.6936 - val_loss: 0.8167 - val_acc: 0.5646\n",
            "Epoch 299/1000\n",
            "297/297 [==============================] - 0s 539us/step - loss: 0.6054 - acc: 0.7273 - val_loss: 0.7993 - val_acc: 0.6122\n",
            "Epoch 300/1000\n",
            "297/297 [==============================] - 0s 561us/step - loss: 0.6147 - acc: 0.7508 - val_loss: 0.8072 - val_acc: 0.6122\n",
            "Epoch 301/1000\n",
            "297/297 [==============================] - 0s 549us/step - loss: 0.6198 - acc: 0.7239 - val_loss: 0.8122 - val_acc: 0.6122\n",
            "Epoch 302/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.6640 - acc: 0.7407 - val_loss: 0.7890 - val_acc: 0.5714\n",
            "Epoch 303/1000\n",
            "297/297 [==============================] - 0s 562us/step - loss: 0.6046 - acc: 0.7542 - val_loss: 0.8072 - val_acc: 0.5850\n",
            "Epoch 304/1000\n",
            "297/297 [==============================] - 0s 590us/step - loss: 0.6116 - acc: 0.7374 - val_loss: 0.8049 - val_acc: 0.6054\n",
            "Epoch 305/1000\n",
            "297/297 [==============================] - 0s 575us/step - loss: 0.6494 - acc: 0.7441 - val_loss: 0.8092 - val_acc: 0.5850\n",
            "Epoch 306/1000\n",
            "297/297 [==============================] - 0s 562us/step - loss: 0.6414 - acc: 0.7407 - val_loss: 0.8372 - val_acc: 0.5782\n",
            "Epoch 307/1000\n",
            "297/297 [==============================] - 0s 538us/step - loss: 0.5975 - acc: 0.7475 - val_loss: 0.7913 - val_acc: 0.6054\n",
            "Epoch 308/1000\n",
            "297/297 [==============================] - 0s 563us/step - loss: 0.6083 - acc: 0.7205 - val_loss: 0.8348 - val_acc: 0.5782\n",
            "Epoch 309/1000\n",
            "297/297 [==============================] - 0s 557us/step - loss: 0.6410 - acc: 0.7407 - val_loss: 0.8192 - val_acc: 0.5782\n",
            "Epoch 310/1000\n",
            "297/297 [==============================] - 0s 534us/step - loss: 0.5777 - acc: 0.7407 - val_loss: 0.7778 - val_acc: 0.5782\n",
            "Epoch 311/1000\n",
            "297/297 [==============================] - 0s 545us/step - loss: 0.5975 - acc: 0.7306 - val_loss: 0.8248 - val_acc: 0.5714\n",
            "Epoch 312/1000\n",
            "297/297 [==============================] - 0s 538us/step - loss: 0.6229 - acc: 0.7138 - val_loss: 0.8305 - val_acc: 0.5646\n",
            "Epoch 313/1000\n",
            "297/297 [==============================] - 0s 541us/step - loss: 0.5980 - acc: 0.7475 - val_loss: 0.7928 - val_acc: 0.5782\n",
            "Epoch 314/1000\n",
            "297/297 [==============================] - 0s 556us/step - loss: 0.6151 - acc: 0.7508 - val_loss: 0.8445 - val_acc: 0.5782\n",
            "Epoch 315/1000\n",
            "297/297 [==============================] - 0s 536us/step - loss: 0.5848 - acc: 0.7542 - val_loss: 0.8156 - val_acc: 0.6190\n",
            "Epoch 316/1000\n",
            "297/297 [==============================] - 0s 580us/step - loss: 0.6087 - acc: 0.7508 - val_loss: 0.8153 - val_acc: 0.6054\n",
            "Epoch 317/1000\n",
            "297/297 [==============================] - 0s 546us/step - loss: 0.5763 - acc: 0.7441 - val_loss: 0.9041 - val_acc: 0.5306\n",
            "Epoch 318/1000\n",
            "297/297 [==============================] - 0s 562us/step - loss: 0.6299 - acc: 0.7340 - val_loss: 0.8342 - val_acc: 0.5918\n",
            "Epoch 319/1000\n",
            "297/297 [==============================] - 0s 580us/step - loss: 0.5920 - acc: 0.7306 - val_loss: 0.8293 - val_acc: 0.5986\n",
            "Epoch 320/1000\n",
            "297/297 [==============================] - 0s 552us/step - loss: 0.5999 - acc: 0.7778 - val_loss: 0.7768 - val_acc: 0.5918\n",
            "Epoch 321/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.6026 - acc: 0.7475 - val_loss: 0.8195 - val_acc: 0.5578\n",
            "Epoch 322/1000\n",
            "297/297 [==============================] - 0s 540us/step - loss: 0.5811 - acc: 0.7643 - val_loss: 0.8037 - val_acc: 0.5714\n",
            "Epoch 323/1000\n",
            "297/297 [==============================] - 0s 543us/step - loss: 0.6269 - acc: 0.7340 - val_loss: 0.7847 - val_acc: 0.6327\n",
            "Epoch 324/1000\n",
            "297/297 [==============================] - 0s 535us/step - loss: 0.5794 - acc: 0.7508 - val_loss: 0.7765 - val_acc: 0.6122\n",
            "Epoch 325/1000\n",
            "297/297 [==============================] - 0s 554us/step - loss: 0.6133 - acc: 0.7407 - val_loss: 0.7763 - val_acc: 0.6190\n",
            "Epoch 326/1000\n",
            "297/297 [==============================] - 0s 557us/step - loss: 0.5820 - acc: 0.7609 - val_loss: 0.8459 - val_acc: 0.5918\n",
            "Epoch 327/1000\n",
            "297/297 [==============================] - 0s 567us/step - loss: 0.5857 - acc: 0.7475 - val_loss: 0.7843 - val_acc: 0.6122\n",
            "Epoch 328/1000\n",
            "297/297 [==============================] - 0s 619us/step - loss: 0.6034 - acc: 0.7609 - val_loss: 0.7749 - val_acc: 0.5850\n",
            "Epoch 329/1000\n",
            "297/297 [==============================] - 0s 546us/step - loss: 0.5448 - acc: 0.7980 - val_loss: 0.8193 - val_acc: 0.6054\n",
            "Epoch 330/1000\n",
            "297/297 [==============================] - 0s 561us/step - loss: 0.5609 - acc: 0.7744 - val_loss: 0.7842 - val_acc: 0.6259\n",
            "Epoch 331/1000\n",
            "297/297 [==============================] - 0s 559us/step - loss: 0.5361 - acc: 0.7778 - val_loss: 0.7964 - val_acc: 0.6259\n",
            "Epoch 332/1000\n",
            "297/297 [==============================] - 0s 554us/step - loss: 0.5710 - acc: 0.7542 - val_loss: 0.8149 - val_acc: 0.5782\n",
            "Epoch 333/1000\n",
            "297/297 [==============================] - 0s 559us/step - loss: 0.5727 - acc: 0.7508 - val_loss: 0.7957 - val_acc: 0.6259\n",
            "Epoch 334/1000\n",
            "297/297 [==============================] - 0s 540us/step - loss: 0.5999 - acc: 0.7407 - val_loss: 0.8160 - val_acc: 0.5714\n",
            "Epoch 335/1000\n",
            "297/297 [==============================] - 0s 545us/step - loss: 0.5760 - acc: 0.7609 - val_loss: 0.7990 - val_acc: 0.6190\n",
            "Epoch 336/1000\n",
            "297/297 [==============================] - 0s 537us/step - loss: 0.5656 - acc: 0.7576 - val_loss: 0.8009 - val_acc: 0.5646\n",
            "Epoch 337/1000\n",
            "297/297 [==============================] - 0s 552us/step - loss: 0.5911 - acc: 0.7407 - val_loss: 0.8259 - val_acc: 0.5782\n",
            "Epoch 338/1000\n",
            "297/297 [==============================] - 0s 568us/step - loss: 0.5634 - acc: 0.7710 - val_loss: 0.8272 - val_acc: 0.5850\n",
            "Epoch 339/1000\n",
            "297/297 [==============================] - 0s 603us/step - loss: 0.5630 - acc: 0.7744 - val_loss: 0.7866 - val_acc: 0.6190\n",
            "Epoch 340/1000\n",
            "297/297 [==============================] - 0s 602us/step - loss: 0.5551 - acc: 0.7845 - val_loss: 0.9067 - val_acc: 0.5646\n",
            "Epoch 341/1000\n",
            "297/297 [==============================] - 0s 590us/step - loss: 0.5706 - acc: 0.7677 - val_loss: 0.8192 - val_acc: 0.5782\n",
            "Epoch 342/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.5378 - acc: 0.7609 - val_loss: 0.7910 - val_acc: 0.5986\n",
            "Epoch 343/1000\n",
            "297/297 [==============================] - 0s 567us/step - loss: 0.5642 - acc: 0.7643 - val_loss: 0.8880 - val_acc: 0.5578\n",
            "Epoch 344/1000\n",
            "297/297 [==============================] - 0s 673us/step - loss: 0.5629 - acc: 0.7609 - val_loss: 0.8249 - val_acc: 0.5918\n",
            "Epoch 345/1000\n",
            "297/297 [==============================] - 0s 575us/step - loss: 0.5725 - acc: 0.7273 - val_loss: 0.7857 - val_acc: 0.5986\n",
            "Epoch 346/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.5304 - acc: 0.7946 - val_loss: 0.8228 - val_acc: 0.5850\n",
            "Epoch 347/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.5852 - acc: 0.7677 - val_loss: 0.8275 - val_acc: 0.5986\n",
            "Epoch 348/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.5289 - acc: 0.7879 - val_loss: 0.7896 - val_acc: 0.6122\n",
            "Epoch 349/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.5542 - acc: 0.7677 - val_loss: 0.8019 - val_acc: 0.6054\n",
            "Epoch 350/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.5286 - acc: 0.7845 - val_loss: 0.8109 - val_acc: 0.6054\n",
            "Epoch 351/1000\n",
            "297/297 [==============================] - 0s 650us/step - loss: 0.5670 - acc: 0.7508 - val_loss: 0.7839 - val_acc: 0.6054\n",
            "Epoch 352/1000\n",
            "297/297 [==============================] - 0s 605us/step - loss: 0.5425 - acc: 0.7778 - val_loss: 0.7760 - val_acc: 0.5646\n",
            "Epoch 353/1000\n",
            "297/297 [==============================] - 0s 561us/step - loss: 0.5200 - acc: 0.7912 - val_loss: 0.8076 - val_acc: 0.5782\n",
            "Epoch 354/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.5461 - acc: 0.7710 - val_loss: 0.8055 - val_acc: 0.5986\n",
            "Epoch 355/1000\n",
            "297/297 [==============================] - 0s 605us/step - loss: 0.5396 - acc: 0.7778 - val_loss: 0.8204 - val_acc: 0.5918\n",
            "Epoch 356/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.5071 - acc: 0.7912 - val_loss: 0.8348 - val_acc: 0.5918\n",
            "Epoch 357/1000\n",
            "297/297 [==============================] - 0s 596us/step - loss: 0.5174 - acc: 0.7980 - val_loss: 0.8401 - val_acc: 0.5918\n",
            "Epoch 358/1000\n",
            "297/297 [==============================] - 0s 568us/step - loss: 0.5114 - acc: 0.7811 - val_loss: 0.8313 - val_acc: 0.5782\n",
            "Epoch 359/1000\n",
            "297/297 [==============================] - 0s 541us/step - loss: 0.5044 - acc: 0.7912 - val_loss: 0.7815 - val_acc: 0.6122\n",
            "Epoch 360/1000\n",
            "297/297 [==============================] - 0s 575us/step - loss: 0.5520 - acc: 0.7811 - val_loss: 0.8180 - val_acc: 0.6054\n",
            "Epoch 361/1000\n",
            "297/297 [==============================] - 0s 576us/step - loss: 0.5377 - acc: 0.7643 - val_loss: 0.7854 - val_acc: 0.6122\n",
            "Epoch 362/1000\n",
            "297/297 [==============================] - 0s 582us/step - loss: 0.5292 - acc: 0.7744 - val_loss: 0.7657 - val_acc: 0.5850\n",
            "Epoch 363/1000\n",
            "297/297 [==============================] - 0s 596us/step - loss: 0.5298 - acc: 0.7643 - val_loss: 0.7971 - val_acc: 0.6190\n",
            "Epoch 364/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.5173 - acc: 0.7946 - val_loss: 0.7783 - val_acc: 0.5714\n",
            "Epoch 365/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.5260 - acc: 0.8013 - val_loss: 0.8166 - val_acc: 0.5986\n",
            "Epoch 366/1000\n",
            "297/297 [==============================] - 0s 577us/step - loss: 0.5621 - acc: 0.7677 - val_loss: 0.8298 - val_acc: 0.6054\n",
            "Epoch 367/1000\n",
            "297/297 [==============================] - 0s 564us/step - loss: 0.5101 - acc: 0.7946 - val_loss: 0.7888 - val_acc: 0.6122\n",
            "Epoch 368/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.5356 - acc: 0.7576 - val_loss: 0.7760 - val_acc: 0.6259\n",
            "Epoch 369/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 0.5124 - acc: 0.7879 - val_loss: 0.7799 - val_acc: 0.6122\n",
            "Epoch 370/1000\n",
            "297/297 [==============================] - 0s 555us/step - loss: 0.5603 - acc: 0.7811 - val_loss: 0.7892 - val_acc: 0.6327\n",
            "Epoch 371/1000\n",
            "297/297 [==============================] - 0s 608us/step - loss: 0.5337 - acc: 0.7811 - val_loss: 0.8038 - val_acc: 0.6463\n",
            "Epoch 372/1000\n",
            "297/297 [==============================] - 0s 561us/step - loss: 0.5476 - acc: 0.7710 - val_loss: 0.8204 - val_acc: 0.5782\n",
            "Epoch 373/1000\n",
            "297/297 [==============================] - 0s 621us/step - loss: 0.5460 - acc: 0.7778 - val_loss: 0.8342 - val_acc: 0.5442\n",
            "Epoch 374/1000\n",
            "297/297 [==============================] - 0s 606us/step - loss: 0.5537 - acc: 0.7643 - val_loss: 0.7901 - val_acc: 0.5986\n",
            "Epoch 375/1000\n",
            "297/297 [==============================] - 0s 560us/step - loss: 0.5040 - acc: 0.7845 - val_loss: 0.7679 - val_acc: 0.5918\n",
            "Epoch 376/1000\n",
            "297/297 [==============================] - 0s 566us/step - loss: 0.5032 - acc: 0.8047 - val_loss: 0.8660 - val_acc: 0.5986\n",
            "Epoch 377/1000\n",
            "297/297 [==============================] - 0s 549us/step - loss: 0.5165 - acc: 0.7946 - val_loss: 0.7915 - val_acc: 0.6190\n",
            "Epoch 378/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.5253 - acc: 0.7710 - val_loss: 0.8103 - val_acc: 0.6190\n",
            "Epoch 379/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.5330 - acc: 0.7778 - val_loss: 0.7771 - val_acc: 0.6259\n",
            "Epoch 380/1000\n",
            "297/297 [==============================] - 0s 609us/step - loss: 0.5194 - acc: 0.7946 - val_loss: 0.7673 - val_acc: 0.6259\n",
            "Epoch 381/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.5015 - acc: 0.7946 - val_loss: 0.7681 - val_acc: 0.6190\n",
            "Epoch 382/1000\n",
            "297/297 [==============================] - 0s 580us/step - loss: 0.5326 - acc: 0.7677 - val_loss: 0.7981 - val_acc: 0.6190\n",
            "Epoch 383/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 0.5535 - acc: 0.7677 - val_loss: 0.8051 - val_acc: 0.6327\n",
            "Epoch 384/1000\n",
            "297/297 [==============================] - 0s 564us/step - loss: 0.5283 - acc: 0.7744 - val_loss: 0.7648 - val_acc: 0.5986\n",
            "Epoch 385/1000\n",
            "297/297 [==============================] - 0s 613us/step - loss: 0.5053 - acc: 0.8114 - val_loss: 0.7832 - val_acc: 0.6190\n",
            "Epoch 386/1000\n",
            "297/297 [==============================] - 0s 576us/step - loss: 0.5112 - acc: 0.7778 - val_loss: 0.8028 - val_acc: 0.5850\n",
            "Epoch 387/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.4975 - acc: 0.8148 - val_loss: 0.8027 - val_acc: 0.6122\n",
            "Epoch 388/1000\n",
            "297/297 [==============================] - 0s 596us/step - loss: 0.5074 - acc: 0.8283 - val_loss: 0.7870 - val_acc: 0.6054\n",
            "Epoch 389/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.4917 - acc: 0.7980 - val_loss: 0.8412 - val_acc: 0.6054\n",
            "Epoch 390/1000\n",
            "297/297 [==============================] - 0s 620us/step - loss: 0.5085 - acc: 0.7980 - val_loss: 0.7680 - val_acc: 0.6395\n",
            "Epoch 391/1000\n",
            "297/297 [==============================] - 0s 551us/step - loss: 0.5294 - acc: 0.7778 - val_loss: 0.7635 - val_acc: 0.6327\n",
            "Epoch 392/1000\n",
            "297/297 [==============================] - 0s 616us/step - loss: 0.5180 - acc: 0.8047 - val_loss: 0.7623 - val_acc: 0.6054\n",
            "Epoch 393/1000\n",
            "297/297 [==============================] - 0s 601us/step - loss: 0.5070 - acc: 0.7980 - val_loss: 0.8062 - val_acc: 0.6054\n",
            "Epoch 394/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.5011 - acc: 0.7980 - val_loss: 0.8035 - val_acc: 0.6259\n",
            "Epoch 395/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.4971 - acc: 0.7912 - val_loss: 0.8167 - val_acc: 0.5782\n",
            "Epoch 396/1000\n",
            "297/297 [==============================] - 0s 563us/step - loss: 0.4940 - acc: 0.7912 - val_loss: 0.8122 - val_acc: 0.6054\n",
            "Epoch 397/1000\n",
            "297/297 [==============================] - 0s 612us/step - loss: 0.5181 - acc: 0.8047 - val_loss: 0.8337 - val_acc: 0.5986\n",
            "Epoch 398/1000\n",
            "297/297 [==============================] - 0s 548us/step - loss: 0.4557 - acc: 0.8047 - val_loss: 0.7734 - val_acc: 0.5986\n",
            "Epoch 399/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 0.5337 - acc: 0.7879 - val_loss: 0.7873 - val_acc: 0.6054\n",
            "Epoch 400/1000\n",
            "297/297 [==============================] - 0s 562us/step - loss: 0.4815 - acc: 0.8182 - val_loss: 0.7952 - val_acc: 0.5714\n",
            "Epoch 401/1000\n",
            "297/297 [==============================] - 0s 617us/step - loss: 0.4774 - acc: 0.8182 - val_loss: 0.8349 - val_acc: 0.5442\n",
            "Epoch 402/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.5156 - acc: 0.7879 - val_loss: 0.7801 - val_acc: 0.5986\n",
            "Epoch 403/1000\n",
            "297/297 [==============================] - 0s 595us/step - loss: 0.4863 - acc: 0.8350 - val_loss: 0.7830 - val_acc: 0.5986\n",
            "Epoch 404/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.4565 - acc: 0.8418 - val_loss: 0.7740 - val_acc: 0.5986\n",
            "Epoch 405/1000\n",
            "297/297 [==============================] - 0s 571us/step - loss: 0.5088 - acc: 0.7912 - val_loss: 0.8175 - val_acc: 0.6122\n",
            "Epoch 406/1000\n",
            "297/297 [==============================] - 0s 560us/step - loss: 0.4787 - acc: 0.8081 - val_loss: 0.7703 - val_acc: 0.5986\n",
            "Epoch 407/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 0.4843 - acc: 0.8215 - val_loss: 0.7699 - val_acc: 0.5850\n",
            "Epoch 408/1000\n",
            "297/297 [==============================] - 0s 671us/step - loss: 0.4619 - acc: 0.8316 - val_loss: 0.7942 - val_acc: 0.6327\n",
            "Epoch 409/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 0.5044 - acc: 0.7845 - val_loss: 0.8302 - val_acc: 0.5986\n",
            "Epoch 410/1000\n",
            "297/297 [==============================] - 0s 604us/step - loss: 0.4594 - acc: 0.8451 - val_loss: 0.8387 - val_acc: 0.5986\n",
            "Epoch 411/1000\n",
            "297/297 [==============================] - 0s 678us/step - loss: 0.4896 - acc: 0.8114 - val_loss: 0.8007 - val_acc: 0.5986\n",
            "Epoch 412/1000\n",
            "297/297 [==============================] - 0s 599us/step - loss: 0.4854 - acc: 0.8047 - val_loss: 0.8011 - val_acc: 0.5986\n",
            "Epoch 413/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.4837 - acc: 0.7946 - val_loss: 0.8179 - val_acc: 0.6122\n",
            "Epoch 414/1000\n",
            "297/297 [==============================] - 0s 575us/step - loss: 0.4820 - acc: 0.8081 - val_loss: 0.8518 - val_acc: 0.5986\n",
            "Epoch 415/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 0.4549 - acc: 0.8350 - val_loss: 0.8689 - val_acc: 0.6122\n",
            "Epoch 416/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.4879 - acc: 0.8215 - val_loss: 0.8155 - val_acc: 0.6190\n",
            "Epoch 417/1000\n",
            "297/297 [==============================] - 0s 615us/step - loss: 0.4767 - acc: 0.8283 - val_loss: 0.7655 - val_acc: 0.6259\n",
            "Epoch 418/1000\n",
            "297/297 [==============================] - 0s 604us/step - loss: 0.4639 - acc: 0.8013 - val_loss: 0.7898 - val_acc: 0.6054\n",
            "Epoch 419/1000\n",
            "297/297 [==============================] - 0s 610us/step - loss: 0.4736 - acc: 0.8215 - val_loss: 0.7768 - val_acc: 0.6054\n",
            "Epoch 420/1000\n",
            "297/297 [==============================] - 0s 612us/step - loss: 0.4526 - acc: 0.8384 - val_loss: 0.7602 - val_acc: 0.6395\n",
            "Epoch 421/1000\n",
            "297/297 [==============================] - 0s 545us/step - loss: 0.4596 - acc: 0.8283 - val_loss: 0.8517 - val_acc: 0.5510\n",
            "Epoch 422/1000\n",
            "297/297 [==============================] - 0s 563us/step - loss: 0.4789 - acc: 0.8148 - val_loss: 0.7751 - val_acc: 0.6122\n",
            "Epoch 423/1000\n",
            "297/297 [==============================] - 0s 590us/step - loss: 0.4422 - acc: 0.8485 - val_loss: 0.8137 - val_acc: 0.6122\n",
            "Epoch 424/1000\n",
            "297/297 [==============================] - 0s 609us/step - loss: 0.4808 - acc: 0.8283 - val_loss: 0.8089 - val_acc: 0.5714\n",
            "Epoch 425/1000\n",
            "297/297 [==============================] - 0s 590us/step - loss: 0.4812 - acc: 0.7879 - val_loss: 0.8172 - val_acc: 0.5986\n",
            "Epoch 426/1000\n",
            "297/297 [==============================] - 0s 590us/step - loss: 0.4623 - acc: 0.8148 - val_loss: 0.7467 - val_acc: 0.6054\n",
            "Epoch 427/1000\n",
            "297/297 [==============================] - 0s 544us/step - loss: 0.4534 - acc: 0.8215 - val_loss: 0.7726 - val_acc: 0.6395\n",
            "Epoch 428/1000\n",
            "297/297 [==============================] - 0s 623us/step - loss: 0.4494 - acc: 0.8215 - val_loss: 0.7515 - val_acc: 0.6259\n",
            "Epoch 429/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 0.4533 - acc: 0.8620 - val_loss: 0.7764 - val_acc: 0.6395\n",
            "Epoch 430/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 0.4546 - acc: 0.8350 - val_loss: 0.7541 - val_acc: 0.6054\n",
            "Epoch 431/1000\n",
            "297/297 [==============================] - 0s 597us/step - loss: 0.4632 - acc: 0.8182 - val_loss: 0.8012 - val_acc: 0.6667\n",
            "Epoch 432/1000\n",
            "297/297 [==============================] - 0s 606us/step - loss: 0.4524 - acc: 0.8249 - val_loss: 0.7947 - val_acc: 0.5850\n",
            "Epoch 433/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.4712 - acc: 0.7946 - val_loss: 0.7467 - val_acc: 0.6190\n",
            "Epoch 434/1000\n",
            "297/297 [==============================] - 0s 612us/step - loss: 0.4398 - acc: 0.8350 - val_loss: 0.8028 - val_acc: 0.5918\n",
            "Epoch 435/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 0.4715 - acc: 0.8148 - val_loss: 0.8197 - val_acc: 0.5986\n",
            "Epoch 436/1000\n",
            "297/297 [==============================] - 0s 580us/step - loss: 0.4666 - acc: 0.8148 - val_loss: 0.8206 - val_acc: 0.5986\n",
            "Epoch 437/1000\n",
            "297/297 [==============================] - 0s 555us/step - loss: 0.4452 - acc: 0.8283 - val_loss: 0.7928 - val_acc: 0.6259\n",
            "Epoch 438/1000\n",
            "297/297 [==============================] - 0s 582us/step - loss: 0.4863 - acc: 0.8047 - val_loss: 0.8491 - val_acc: 0.5782\n",
            "Epoch 439/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 0.4558 - acc: 0.8316 - val_loss: 0.7964 - val_acc: 0.6395\n",
            "Epoch 440/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.4656 - acc: 0.8249 - val_loss: 0.8683 - val_acc: 0.5986\n",
            "Epoch 441/1000\n",
            "297/297 [==============================] - 0s 539us/step - loss: 0.4276 - acc: 0.8687 - val_loss: 0.7893 - val_acc: 0.6463\n",
            "Epoch 442/1000\n",
            "297/297 [==============================] - 0s 614us/step - loss: 0.4642 - acc: 0.8047 - val_loss: 0.7658 - val_acc: 0.6259\n",
            "Epoch 443/1000\n",
            "297/297 [==============================] - 0s 575us/step - loss: 0.4435 - acc: 0.8485 - val_loss: 0.7685 - val_acc: 0.6259\n",
            "Epoch 444/1000\n",
            "297/297 [==============================] - 0s 570us/step - loss: 0.4657 - acc: 0.8182 - val_loss: 0.7635 - val_acc: 0.6122\n",
            "Epoch 445/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.4423 - acc: 0.8384 - val_loss: 0.7561 - val_acc: 0.6190\n",
            "Epoch 446/1000\n",
            "297/297 [==============================] - 0s 579us/step - loss: 0.4642 - acc: 0.8047 - val_loss: 0.7533 - val_acc: 0.6122\n",
            "Epoch 447/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.4480 - acc: 0.8384 - val_loss: 0.8339 - val_acc: 0.6054\n",
            "Epoch 448/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 0.4109 - acc: 0.8485 - val_loss: 0.8197 - val_acc: 0.6327\n",
            "Epoch 449/1000\n",
            "297/297 [==============================] - 0s 584us/step - loss: 0.4441 - acc: 0.8485 - val_loss: 0.8330 - val_acc: 0.6190\n",
            "Epoch 450/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 0.4437 - acc: 0.8384 - val_loss: 0.8229 - val_acc: 0.5782\n",
            "Epoch 451/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 0.4462 - acc: 0.8215 - val_loss: 0.7639 - val_acc: 0.6395\n",
            "Epoch 452/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.4372 - acc: 0.8249 - val_loss: 0.8016 - val_acc: 0.6327\n",
            "Epoch 453/1000\n",
            "297/297 [==============================] - 0s 558us/step - loss: 0.4416 - acc: 0.8418 - val_loss: 0.8099 - val_acc: 0.6190\n",
            "Epoch 454/1000\n",
            "297/297 [==============================] - 0s 584us/step - loss: 0.4360 - acc: 0.8350 - val_loss: 0.7742 - val_acc: 0.6531\n",
            "Epoch 455/1000\n",
            "297/297 [==============================] - 0s 590us/step - loss: 0.4214 - acc: 0.8451 - val_loss: 0.7585 - val_acc: 0.6190\n",
            "Epoch 456/1000\n",
            "297/297 [==============================] - 0s 576us/step - loss: 0.4341 - acc: 0.8418 - val_loss: 0.8259 - val_acc: 0.6259\n",
            "Epoch 457/1000\n",
            "297/297 [==============================] - 0s 603us/step - loss: 0.4396 - acc: 0.8418 - val_loss: 0.7745 - val_acc: 0.6259\n",
            "Epoch 458/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.4066 - acc: 0.8586 - val_loss: 0.8341 - val_acc: 0.6395\n",
            "Epoch 459/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.4288 - acc: 0.8485 - val_loss: 0.7683 - val_acc: 0.6395\n",
            "Epoch 460/1000\n",
            "297/297 [==============================] - 0s 554us/step - loss: 0.4203 - acc: 0.8552 - val_loss: 0.7916 - val_acc: 0.6190\n",
            "Epoch 461/1000\n",
            "297/297 [==============================] - 0s 570us/step - loss: 0.4352 - acc: 0.8350 - val_loss: 0.7887 - val_acc: 0.6395\n",
            "Epoch 462/1000\n",
            "297/297 [==============================] - 0s 614us/step - loss: 0.4219 - acc: 0.8552 - val_loss: 0.7840 - val_acc: 0.6190\n",
            "Epoch 463/1000\n",
            "297/297 [==============================] - 0s 570us/step - loss: 0.4444 - acc: 0.8249 - val_loss: 0.7797 - val_acc: 0.6122\n",
            "Epoch 464/1000\n",
            "297/297 [==============================] - 0s 621us/step - loss: 0.4096 - acc: 0.8519 - val_loss: 0.7819 - val_acc: 0.6259\n",
            "Epoch 465/1000\n",
            "297/297 [==============================] - 0s 599us/step - loss: 0.4492 - acc: 0.8316 - val_loss: 0.8273 - val_acc: 0.6531\n",
            "Epoch 466/1000\n",
            "297/297 [==============================] - 0s 549us/step - loss: 0.4158 - acc: 0.8754 - val_loss: 0.7868 - val_acc: 0.6395\n",
            "Epoch 467/1000\n",
            "297/297 [==============================] - 0s 571us/step - loss: 0.4127 - acc: 0.8519 - val_loss: 0.7858 - val_acc: 0.6327\n",
            "Epoch 468/1000\n",
            "297/297 [==============================] - 0s 570us/step - loss: 0.4220 - acc: 0.8384 - val_loss: 0.7663 - val_acc: 0.6190\n",
            "Epoch 469/1000\n",
            "297/297 [==============================] - 0s 566us/step - loss: 0.4447 - acc: 0.8384 - val_loss: 0.7840 - val_acc: 0.6327\n",
            "Epoch 470/1000\n",
            "297/297 [==============================] - 0s 687us/step - loss: 0.4412 - acc: 0.8384 - val_loss: 0.8072 - val_acc: 0.6463\n",
            "Epoch 471/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.4372 - acc: 0.8653 - val_loss: 0.8154 - val_acc: 0.5918\n",
            "Epoch 472/1000\n",
            "297/297 [==============================] - 0s 597us/step - loss: 0.4062 - acc: 0.8451 - val_loss: 0.8060 - val_acc: 0.6259\n",
            "Epoch 473/1000\n",
            "297/297 [==============================] - 0s 595us/step - loss: 0.3841 - acc: 0.8822 - val_loss: 0.7887 - val_acc: 0.6395\n",
            "Epoch 474/1000\n",
            "297/297 [==============================] - 0s 584us/step - loss: 0.4373 - acc: 0.8215 - val_loss: 0.7669 - val_acc: 0.6259\n",
            "Epoch 475/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.3939 - acc: 0.8889 - val_loss: 0.8247 - val_acc: 0.6463\n",
            "Epoch 476/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.4264 - acc: 0.8350 - val_loss: 0.7724 - val_acc: 0.6327\n",
            "Epoch 477/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 0.3899 - acc: 0.8620 - val_loss: 0.7521 - val_acc: 0.6122\n",
            "Epoch 478/1000\n",
            "297/297 [==============================] - 0s 590us/step - loss: 0.4328 - acc: 0.8283 - val_loss: 0.7708 - val_acc: 0.6190\n",
            "Epoch 479/1000\n",
            "297/297 [==============================] - 0s 569us/step - loss: 0.3872 - acc: 0.8519 - val_loss: 0.7802 - val_acc: 0.6122\n",
            "Epoch 480/1000\n",
            "297/297 [==============================] - 0s 668us/step - loss: 0.4437 - acc: 0.8249 - val_loss: 0.7561 - val_acc: 0.6259\n",
            "Epoch 481/1000\n",
            "297/297 [==============================] - 0s 575us/step - loss: 0.4094 - acc: 0.8182 - val_loss: 0.7597 - val_acc: 0.6327\n",
            "Epoch 482/1000\n",
            "297/297 [==============================] - 0s 596us/step - loss: 0.4113 - acc: 0.8519 - val_loss: 0.8625 - val_acc: 0.6190\n",
            "Epoch 483/1000\n",
            "297/297 [==============================] - 0s 563us/step - loss: 0.4243 - acc: 0.8620 - val_loss: 0.7747 - val_acc: 0.6395\n",
            "Epoch 484/1000\n",
            "297/297 [==============================] - 0s 544us/step - loss: 0.4117 - acc: 0.8552 - val_loss: 0.8029 - val_acc: 0.6395\n",
            "Epoch 485/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.3941 - acc: 0.8519 - val_loss: 0.7692 - val_acc: 0.6463\n",
            "Epoch 486/1000\n",
            "297/297 [==============================] - 0s 559us/step - loss: 0.3951 - acc: 0.8653 - val_loss: 0.7951 - val_acc: 0.6531\n",
            "Epoch 487/1000\n",
            "297/297 [==============================] - 0s 635us/step - loss: 0.3948 - acc: 0.8485 - val_loss: 0.7899 - val_acc: 0.6395\n",
            "Epoch 488/1000\n",
            "297/297 [==============================] - 0s 608us/step - loss: 0.4238 - acc: 0.8384 - val_loss: 0.7639 - val_acc: 0.6259\n",
            "Epoch 489/1000\n",
            "297/297 [==============================] - 0s 608us/step - loss: 0.4097 - acc: 0.8451 - val_loss: 0.7964 - val_acc: 0.6463\n",
            "Epoch 490/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.4133 - acc: 0.8384 - val_loss: 0.7715 - val_acc: 0.6395\n",
            "Epoch 491/1000\n",
            "297/297 [==============================] - 0s 596us/step - loss: 0.3870 - acc: 0.8620 - val_loss: 0.7917 - val_acc: 0.5986\n",
            "Epoch 492/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.4023 - acc: 0.8384 - val_loss: 0.7516 - val_acc: 0.6259\n",
            "Epoch 493/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.4009 - acc: 0.8384 - val_loss: 0.7630 - val_acc: 0.6122\n",
            "Epoch 494/1000\n",
            "297/297 [==============================] - 0s 615us/step - loss: 0.4004 - acc: 0.8519 - val_loss: 0.7857 - val_acc: 0.6054\n",
            "Epoch 495/1000\n",
            "297/297 [==============================] - 0s 572us/step - loss: 0.3985 - acc: 0.8519 - val_loss: 0.7636 - val_acc: 0.5986\n",
            "Epoch 496/1000\n",
            "297/297 [==============================] - 0s 599us/step - loss: 0.3744 - acc: 0.8822 - val_loss: 0.8241 - val_acc: 0.6463\n",
            "Epoch 497/1000\n",
            "297/297 [==============================] - 0s 562us/step - loss: 0.4253 - acc: 0.8316 - val_loss: 0.7940 - val_acc: 0.6190\n",
            "Epoch 498/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.3948 - acc: 0.8687 - val_loss: 0.8265 - val_acc: 0.6395\n",
            "Epoch 499/1000\n",
            "297/297 [==============================] - 0s 608us/step - loss: 0.4069 - acc: 0.8653 - val_loss: 0.7779 - val_acc: 0.6395\n",
            "Epoch 500/1000\n",
            "297/297 [==============================] - 0s 555us/step - loss: 0.3924 - acc: 0.8653 - val_loss: 0.8148 - val_acc: 0.6259\n",
            "Epoch 501/1000\n",
            "297/297 [==============================] - 0s 606us/step - loss: 0.3932 - acc: 0.8687 - val_loss: 0.7650 - val_acc: 0.6327\n",
            "Epoch 502/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 0.4170 - acc: 0.8316 - val_loss: 0.8625 - val_acc: 0.6190\n",
            "Epoch 503/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.3803 - acc: 0.8485 - val_loss: 0.7589 - val_acc: 0.6122\n",
            "Epoch 504/1000\n",
            "297/297 [==============================] - 0s 584us/step - loss: 0.3921 - acc: 0.8552 - val_loss: 0.7695 - val_acc: 0.6463\n",
            "Epoch 505/1000\n",
            "297/297 [==============================] - 0s 599us/step - loss: 0.3778 - acc: 0.8788 - val_loss: 0.7637 - val_acc: 0.6327\n",
            "Epoch 506/1000\n",
            "297/297 [==============================] - 0s 558us/step - loss: 0.3763 - acc: 0.8653 - val_loss: 0.7550 - val_acc: 0.6395\n",
            "Epoch 507/1000\n",
            "297/297 [==============================] - 0s 587us/step - loss: 0.4120 - acc: 0.8552 - val_loss: 0.7670 - val_acc: 0.6531\n",
            "Epoch 508/1000\n",
            "297/297 [==============================] - 0s 558us/step - loss: 0.3990 - acc: 0.8687 - val_loss: 0.8028 - val_acc: 0.6122\n",
            "Epoch 509/1000\n",
            "297/297 [==============================] - 0s 575us/step - loss: 0.3619 - acc: 0.8687 - val_loss: 0.7672 - val_acc: 0.6190\n",
            "Epoch 510/1000\n",
            "297/297 [==============================] - 0s 606us/step - loss: 0.3715 - acc: 0.8754 - val_loss: 0.7613 - val_acc: 0.6259\n",
            "Epoch 511/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 0.3853 - acc: 0.8687 - val_loss: 0.7871 - val_acc: 0.6327\n",
            "Epoch 512/1000\n",
            "297/297 [==============================] - 0s 624us/step - loss: 0.3841 - acc: 0.8653 - val_loss: 0.8111 - val_acc: 0.6259\n",
            "Epoch 513/1000\n",
            "297/297 [==============================] - 0s 580us/step - loss: 0.3771 - acc: 0.8451 - val_loss: 0.7863 - val_acc: 0.6122\n",
            "Epoch 514/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 0.3407 - acc: 0.9024 - val_loss: 0.8262 - val_acc: 0.6395\n",
            "Epoch 515/1000\n",
            "297/297 [==============================] - 0s 579us/step - loss: 0.3668 - acc: 0.8788 - val_loss: 0.8843 - val_acc: 0.6190\n",
            "Epoch 516/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.3994 - acc: 0.8081 - val_loss: 0.8522 - val_acc: 0.6190\n",
            "Epoch 517/1000\n",
            "297/297 [==============================] - 0s 562us/step - loss: 0.3712 - acc: 0.8721 - val_loss: 0.7917 - val_acc: 0.6667\n",
            "Epoch 518/1000\n",
            "297/297 [==============================] - 0s 618us/step - loss: 0.3726 - acc: 0.8653 - val_loss: 0.7827 - val_acc: 0.6327\n",
            "Epoch 519/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.3614 - acc: 0.8552 - val_loss: 0.7793 - val_acc: 0.6190\n",
            "Epoch 520/1000\n",
            "297/297 [==============================] - 0s 596us/step - loss: 0.3563 - acc: 0.8620 - val_loss: 0.8218 - val_acc: 0.6463\n",
            "Epoch 521/1000\n",
            "297/297 [==============================] - 0s 611us/step - loss: 0.3747 - acc: 0.8754 - val_loss: 0.8133 - val_acc: 0.6531\n",
            "Epoch 522/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.3703 - acc: 0.8620 - val_loss: 0.7763 - val_acc: 0.6531\n",
            "Epoch 523/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.3930 - acc: 0.8586 - val_loss: 0.8207 - val_acc: 0.6531\n",
            "Epoch 524/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.3559 - acc: 0.8721 - val_loss: 0.8227 - val_acc: 0.6327\n",
            "Epoch 525/1000\n",
            "297/297 [==============================] - 0s 605us/step - loss: 0.3580 - acc: 0.8754 - val_loss: 0.7808 - val_acc: 0.6122\n",
            "Epoch 526/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 0.3707 - acc: 0.8586 - val_loss: 0.8656 - val_acc: 0.6054\n",
            "Epoch 527/1000\n",
            "297/297 [==============================] - 0s 616us/step - loss: 0.3495 - acc: 0.8620 - val_loss: 0.7710 - val_acc: 0.5850\n",
            "Epoch 528/1000\n",
            "297/297 [==============================] - 0s 663us/step - loss: 0.3623 - acc: 0.8754 - val_loss: 0.7520 - val_acc: 0.6327\n",
            "Epoch 529/1000\n",
            "297/297 [==============================] - 0s 601us/step - loss: 0.3387 - acc: 0.8721 - val_loss: 0.7764 - val_acc: 0.6395\n",
            "Epoch 530/1000\n",
            "297/297 [==============================] - 0s 575us/step - loss: 0.3548 - acc: 0.8620 - val_loss: 0.7643 - val_acc: 0.6259\n",
            "Epoch 531/1000\n",
            "297/297 [==============================] - 0s 570us/step - loss: 0.3632 - acc: 0.8788 - val_loss: 0.7652 - val_acc: 0.6463\n",
            "Epoch 532/1000\n",
            "297/297 [==============================] - 0s 630us/step - loss: 0.3755 - acc: 0.8552 - val_loss: 0.7989 - val_acc: 0.5918\n",
            "Epoch 533/1000\n",
            "297/297 [==============================] - 0s 617us/step - loss: 0.3430 - acc: 0.8586 - val_loss: 0.7525 - val_acc: 0.6122\n",
            "Epoch 534/1000\n",
            "297/297 [==============================] - 0s 614us/step - loss: 0.3667 - acc: 0.8754 - val_loss: 0.7595 - val_acc: 0.6395\n",
            "Epoch 535/1000\n",
            "297/297 [==============================] - 0s 599us/step - loss: 0.3689 - acc: 0.8687 - val_loss: 0.8296 - val_acc: 0.6395\n",
            "Epoch 536/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 0.3690 - acc: 0.8788 - val_loss: 0.7617 - val_acc: 0.6122\n",
            "Epoch 537/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.3609 - acc: 0.8855 - val_loss: 0.8067 - val_acc: 0.6395\n",
            "Epoch 538/1000\n",
            "297/297 [==============================] - 0s 555us/step - loss: 0.3623 - acc: 0.8822 - val_loss: 0.8016 - val_acc: 0.6599\n",
            "Epoch 539/1000\n",
            "297/297 [==============================] - 0s 595us/step - loss: 0.3690 - acc: 0.8754 - val_loss: 0.7985 - val_acc: 0.6463\n",
            "Epoch 540/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.3287 - acc: 0.8923 - val_loss: 0.7674 - val_acc: 0.6259\n",
            "Epoch 541/1000\n",
            "297/297 [==============================] - 0s 610us/step - loss: 0.3666 - acc: 0.8956 - val_loss: 0.7611 - val_acc: 0.6463\n",
            "Epoch 542/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.3476 - acc: 0.8855 - val_loss: 0.7959 - val_acc: 0.6395\n",
            "Epoch 543/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.3504 - acc: 0.8956 - val_loss: 0.7917 - val_acc: 0.6395\n",
            "Epoch 544/1000\n",
            "297/297 [==============================] - 0s 560us/step - loss: 0.3342 - acc: 0.8788 - val_loss: 0.7563 - val_acc: 0.6463\n",
            "Epoch 545/1000\n",
            "297/297 [==============================] - 0s 596us/step - loss: 0.3226 - acc: 0.8923 - val_loss: 0.7868 - val_acc: 0.6395\n",
            "Epoch 546/1000\n",
            "297/297 [==============================] - 0s 559us/step - loss: 0.3390 - acc: 0.8889 - val_loss: 0.7758 - val_acc: 0.6531\n",
            "Epoch 547/1000\n",
            "297/297 [==============================] - 0s 636us/step - loss: 0.3633 - acc: 0.8653 - val_loss: 0.7769 - val_acc: 0.6190\n",
            "Epoch 548/1000\n",
            "297/297 [==============================] - 0s 614us/step - loss: 0.3335 - acc: 0.8990 - val_loss: 0.7658 - val_acc: 0.6395\n",
            "Epoch 549/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.3627 - acc: 0.8788 - val_loss: 0.8069 - val_acc: 0.6531\n",
            "Epoch 550/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.3577 - acc: 0.8586 - val_loss: 0.7947 - val_acc: 0.6395\n",
            "Epoch 551/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.3393 - acc: 0.8956 - val_loss: 0.7488 - val_acc: 0.6463\n",
            "Epoch 552/1000\n",
            "297/297 [==============================] - 0s 613us/step - loss: 0.3251 - acc: 0.9024 - val_loss: 0.8602 - val_acc: 0.5850\n",
            "Epoch 553/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.3293 - acc: 0.8855 - val_loss: 0.8306 - val_acc: 0.6327\n",
            "Epoch 554/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.3637 - acc: 0.8653 - val_loss: 0.7734 - val_acc: 0.6463\n",
            "Epoch 555/1000\n",
            "297/297 [==============================] - 0s 676us/step - loss: 0.3185 - acc: 0.9091 - val_loss: 0.7926 - val_acc: 0.6259\n",
            "Epoch 556/1000\n",
            "297/297 [==============================] - 0s 595us/step - loss: 0.3585 - acc: 0.8822 - val_loss: 0.7720 - val_acc: 0.6259\n",
            "Epoch 557/1000\n",
            "297/297 [==============================] - 0s 582us/step - loss: 0.3055 - acc: 0.9024 - val_loss: 0.8756 - val_acc: 0.6395\n",
            "Epoch 558/1000\n",
            "297/297 [==============================] - 0s 672us/step - loss: 0.3537 - acc: 0.8687 - val_loss: 0.7728 - val_acc: 0.6122\n",
            "Epoch 559/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.3159 - acc: 0.8990 - val_loss: 0.8027 - val_acc: 0.6395\n",
            "Epoch 560/1000\n",
            "297/297 [==============================] - 0s 603us/step - loss: 0.3333 - acc: 0.8754 - val_loss: 0.7606 - val_acc: 0.6327\n",
            "Epoch 561/1000\n",
            "297/297 [==============================] - 0s 567us/step - loss: 0.3453 - acc: 0.8990 - val_loss: 0.7776 - val_acc: 0.6327\n",
            "Epoch 562/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.3444 - acc: 0.8788 - val_loss: 0.7788 - val_acc: 0.6599\n",
            "Epoch 563/1000\n",
            "297/297 [==============================] - 0s 603us/step - loss: 0.3107 - acc: 0.8956 - val_loss: 0.8075 - val_acc: 0.6531\n",
            "Epoch 564/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 0.3564 - acc: 0.8889 - val_loss: 0.8005 - val_acc: 0.6327\n",
            "Epoch 565/1000\n",
            "297/297 [==============================] - 0s 618us/step - loss: 0.3226 - acc: 0.8889 - val_loss: 0.7695 - val_acc: 0.6327\n",
            "Epoch 566/1000\n",
            "297/297 [==============================] - 0s 605us/step - loss: 0.2982 - acc: 0.8990 - val_loss: 0.7817 - val_acc: 0.6122\n",
            "Epoch 567/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.3254 - acc: 0.8923 - val_loss: 0.8116 - val_acc: 0.6395\n",
            "Epoch 568/1000\n",
            "297/297 [==============================] - 0s 587us/step - loss: 0.3330 - acc: 0.8956 - val_loss: 0.7943 - val_acc: 0.6463\n",
            "Epoch 569/1000\n",
            "297/297 [==============================] - 0s 567us/step - loss: 0.3214 - acc: 0.8788 - val_loss: 0.7632 - val_acc: 0.6259\n",
            "Epoch 570/1000\n",
            "297/297 [==============================] - 0s 558us/step - loss: 0.3262 - acc: 0.8923 - val_loss: 0.7936 - val_acc: 0.6395\n",
            "Epoch 571/1000\n",
            "297/297 [==============================] - 0s 567us/step - loss: 0.3279 - acc: 0.8889 - val_loss: 0.7759 - val_acc: 0.6599\n",
            "Epoch 572/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.3245 - acc: 0.8923 - val_loss: 0.7591 - val_acc: 0.6122\n",
            "Epoch 573/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.3116 - acc: 0.8990 - val_loss: 0.7903 - val_acc: 0.6531\n",
            "Epoch 574/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.3128 - acc: 0.8990 - val_loss: 0.7523 - val_acc: 0.6395\n",
            "Epoch 575/1000\n",
            "297/297 [==============================] - 0s 595us/step - loss: 0.3109 - acc: 0.9057 - val_loss: 0.8194 - val_acc: 0.6599\n",
            "Epoch 576/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.3328 - acc: 0.8754 - val_loss: 0.7772 - val_acc: 0.6735\n",
            "Epoch 577/1000\n",
            "297/297 [==============================] - 0s 637us/step - loss: 0.3153 - acc: 0.8822 - val_loss: 0.8135 - val_acc: 0.6054\n",
            "Epoch 578/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 0.3107 - acc: 0.9158 - val_loss: 0.8054 - val_acc: 0.6531\n",
            "Epoch 579/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.2797 - acc: 0.9327 - val_loss: 0.8139 - val_acc: 0.6599\n",
            "Epoch 580/1000\n",
            "297/297 [==============================] - 0s 564us/step - loss: 0.3077 - acc: 0.8956 - val_loss: 0.8172 - val_acc: 0.6667\n",
            "Epoch 581/1000\n",
            "297/297 [==============================] - 0s 609us/step - loss: 0.3182 - acc: 0.9057 - val_loss: 0.7479 - val_acc: 0.6599\n",
            "Epoch 582/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 0.3282 - acc: 0.8990 - val_loss: 0.7734 - val_acc: 0.6327\n",
            "Epoch 583/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 0.2907 - acc: 0.9226 - val_loss: 0.8039 - val_acc: 0.6395\n",
            "Epoch 584/1000\n",
            "297/297 [==============================] - 0s 606us/step - loss: 0.3151 - acc: 0.8889 - val_loss: 0.8237 - val_acc: 0.6531\n",
            "Epoch 585/1000\n",
            "297/297 [==============================] - 0s 587us/step - loss: 0.3288 - acc: 0.8956 - val_loss: 0.7813 - val_acc: 0.6122\n",
            "Epoch 586/1000\n",
            "297/297 [==============================] - 0s 697us/step - loss: 0.3210 - acc: 0.8923 - val_loss: 0.7659 - val_acc: 0.6463\n",
            "Epoch 587/1000\n",
            "297/297 [==============================] - 0s 562us/step - loss: 0.3093 - acc: 0.8990 - val_loss: 0.8285 - val_acc: 0.6463\n",
            "Epoch 588/1000\n",
            "297/297 [==============================] - 0s 640us/step - loss: 0.3177 - acc: 0.8956 - val_loss: 0.7554 - val_acc: 0.6395\n",
            "Epoch 589/1000\n",
            "297/297 [==============================] - 0s 591us/step - loss: 0.3075 - acc: 0.8822 - val_loss: 0.7687 - val_acc: 0.6531\n",
            "Epoch 590/1000\n",
            "297/297 [==============================] - 0s 638us/step - loss: 0.2881 - acc: 0.9158 - val_loss: 0.7623 - val_acc: 0.6463\n",
            "Epoch 591/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 0.2940 - acc: 0.9024 - val_loss: 0.8459 - val_acc: 0.6395\n",
            "Epoch 592/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 0.3032 - acc: 0.9158 - val_loss: 0.8149 - val_acc: 0.6531\n",
            "Epoch 593/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.3205 - acc: 0.8889 - val_loss: 0.7555 - val_acc: 0.6395\n",
            "Epoch 594/1000\n",
            "297/297 [==============================] - 0s 570us/step - loss: 0.3039 - acc: 0.9057 - val_loss: 0.7772 - val_acc: 0.6531\n",
            "Epoch 595/1000\n",
            "297/297 [==============================] - 0s 603us/step - loss: 0.2916 - acc: 0.9024 - val_loss: 0.7772 - val_acc: 0.6531\n",
            "Epoch 596/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.3173 - acc: 0.8923 - val_loss: 0.7552 - val_acc: 0.6735\n",
            "Epoch 597/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.2903 - acc: 0.9158 - val_loss: 0.7578 - val_acc: 0.6327\n",
            "Epoch 598/1000\n",
            "297/297 [==============================] - 0s 577us/step - loss: 0.3295 - acc: 0.8855 - val_loss: 0.7794 - val_acc: 0.6395\n",
            "Epoch 599/1000\n",
            "297/297 [==============================] - 0s 622us/step - loss: 0.2902 - acc: 0.8956 - val_loss: 0.8592 - val_acc: 0.6463\n",
            "Epoch 600/1000\n",
            "297/297 [==============================] - 0s 576us/step - loss: 0.2889 - acc: 0.9226 - val_loss: 0.8016 - val_acc: 0.6667\n",
            "Epoch 601/1000\n",
            "297/297 [==============================] - 0s 584us/step - loss: 0.3247 - acc: 0.8956 - val_loss: 0.7539 - val_acc: 0.6327\n",
            "Epoch 602/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 0.2955 - acc: 0.9057 - val_loss: 0.7401 - val_acc: 0.6599\n",
            "Epoch 603/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.3089 - acc: 0.8956 - val_loss: 0.7739 - val_acc: 0.6531\n",
            "Epoch 604/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 0.2930 - acc: 0.9091 - val_loss: 0.7524 - val_acc: 0.6939\n",
            "Epoch 605/1000\n",
            "297/297 [==============================] - 0s 595us/step - loss: 0.3167 - acc: 0.9024 - val_loss: 0.7987 - val_acc: 0.6599\n",
            "Epoch 606/1000\n",
            "297/297 [==============================] - 0s 601us/step - loss: 0.2853 - acc: 0.9293 - val_loss: 0.7688 - val_acc: 0.6871\n",
            "Epoch 607/1000\n",
            "297/297 [==============================] - 0s 571us/step - loss: 0.2997 - acc: 0.9091 - val_loss: 0.7623 - val_acc: 0.6463\n",
            "Epoch 608/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.2649 - acc: 0.9057 - val_loss: 0.7628 - val_acc: 0.6327\n",
            "Epoch 609/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.3010 - acc: 0.9091 - val_loss: 0.7713 - val_acc: 0.6395\n",
            "Epoch 610/1000\n",
            "297/297 [==============================] - 0s 602us/step - loss: 0.2939 - acc: 0.9192 - val_loss: 0.8349 - val_acc: 0.6531\n",
            "Epoch 611/1000\n",
            "297/297 [==============================] - 0s 621us/step - loss: 0.3011 - acc: 0.9057 - val_loss: 0.8138 - val_acc: 0.6667\n",
            "Epoch 612/1000\n",
            "297/297 [==============================] - 0s 613us/step - loss: 0.2947 - acc: 0.9024 - val_loss: 0.7428 - val_acc: 0.6395\n",
            "Epoch 613/1000\n",
            "297/297 [==============================] - 0s 619us/step - loss: 0.2832 - acc: 0.8956 - val_loss: 0.7906 - val_acc: 0.6327\n",
            "Epoch 614/1000\n",
            "297/297 [==============================] - 0s 569us/step - loss: 0.2964 - acc: 0.9024 - val_loss: 0.7945 - val_acc: 0.6531\n",
            "Epoch 615/1000\n",
            "297/297 [==============================] - 0s 590us/step - loss: 0.2559 - acc: 0.9360 - val_loss: 0.7804 - val_acc: 0.6395\n",
            "Epoch 616/1000\n",
            "297/297 [==============================] - 0s 563us/step - loss: 0.2722 - acc: 0.9091 - val_loss: 0.7670 - val_acc: 0.6735\n",
            "Epoch 617/1000\n",
            "297/297 [==============================] - 0s 601us/step - loss: 0.2801 - acc: 0.9259 - val_loss: 0.7883 - val_acc: 0.6327\n",
            "Epoch 618/1000\n",
            "297/297 [==============================] - 0s 608us/step - loss: 0.2843 - acc: 0.8889 - val_loss: 0.7881 - val_acc: 0.6395\n",
            "Epoch 619/1000\n",
            "297/297 [==============================] - 0s 570us/step - loss: 0.2783 - acc: 0.9327 - val_loss: 0.8046 - val_acc: 0.6395\n",
            "Epoch 620/1000\n",
            "297/297 [==============================] - 0s 654us/step - loss: 0.2656 - acc: 0.9125 - val_loss: 0.8096 - val_acc: 0.6735\n",
            "Epoch 621/1000\n",
            "297/297 [==============================] - 0s 604us/step - loss: 0.2868 - acc: 0.9259 - val_loss: 0.7738 - val_acc: 0.6667\n",
            "Epoch 622/1000\n",
            "297/297 [==============================] - 0s 587us/step - loss: 0.2705 - acc: 0.9360 - val_loss: 0.7990 - val_acc: 0.6735\n",
            "Epoch 623/1000\n",
            "297/297 [==============================] - 0s 575us/step - loss: 0.2926 - acc: 0.8990 - val_loss: 0.7707 - val_acc: 0.6735\n",
            "Epoch 624/1000\n",
            "297/297 [==============================] - 0s 553us/step - loss: 0.2805 - acc: 0.9259 - val_loss: 0.7597 - val_acc: 0.6599\n",
            "Epoch 625/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.2925 - acc: 0.9057 - val_loss: 0.7376 - val_acc: 0.6395\n",
            "Epoch 626/1000\n",
            "297/297 [==============================] - 0s 623us/step - loss: 0.2829 - acc: 0.9192 - val_loss: 0.7599 - val_acc: 0.6327\n",
            "Epoch 627/1000\n",
            "297/297 [==============================] - 0s 595us/step - loss: 0.2825 - acc: 0.9091 - val_loss: 0.7647 - val_acc: 0.6667\n",
            "Epoch 628/1000\n",
            "297/297 [==============================] - 0s 644us/step - loss: 0.2782 - acc: 0.9091 - val_loss: 0.7745 - val_acc: 0.6667\n",
            "Epoch 629/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.2678 - acc: 0.9057 - val_loss: 0.8013 - val_acc: 0.6395\n",
            "Epoch 630/1000\n",
            "297/297 [==============================] - 0s 608us/step - loss: 0.2631 - acc: 0.9360 - val_loss: 0.8118 - val_acc: 0.6735\n",
            "Epoch 631/1000\n",
            "297/297 [==============================] - 0s 619us/step - loss: 0.2807 - acc: 0.8956 - val_loss: 0.8178 - val_acc: 0.6599\n",
            "Epoch 632/1000\n",
            "297/297 [==============================] - 0s 587us/step - loss: 0.2387 - acc: 0.9327 - val_loss: 0.8168 - val_acc: 0.6599\n",
            "Epoch 633/1000\n",
            "297/297 [==============================] - 0s 618us/step - loss: 0.2726 - acc: 0.9293 - val_loss: 0.7562 - val_acc: 0.6735\n",
            "Epoch 634/1000\n",
            "297/297 [==============================] - 0s 575us/step - loss: 0.2618 - acc: 0.9259 - val_loss: 0.7594 - val_acc: 0.6531\n",
            "Epoch 635/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 0.2490 - acc: 0.9428 - val_loss: 0.8241 - val_acc: 0.6531\n",
            "Epoch 636/1000\n",
            "297/297 [==============================] - 0s 595us/step - loss: 0.2627 - acc: 0.9226 - val_loss: 0.8115 - val_acc: 0.6599\n",
            "Epoch 637/1000\n",
            "297/297 [==============================] - 0s 607us/step - loss: 0.2507 - acc: 0.9360 - val_loss: 0.8689 - val_acc: 0.6463\n",
            "Epoch 638/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.2648 - acc: 0.9226 - val_loss: 0.8675 - val_acc: 0.6054\n",
            "Epoch 639/1000\n",
            "297/297 [==============================] - 0s 576us/step - loss: 0.2589 - acc: 0.9293 - val_loss: 0.8235 - val_acc: 0.6463\n",
            "Epoch 640/1000\n",
            "297/297 [==============================] - 0s 626us/step - loss: 0.2725 - acc: 0.9226 - val_loss: 0.8076 - val_acc: 0.6259\n",
            "Epoch 641/1000\n",
            "297/297 [==============================] - 0s 603us/step - loss: 0.3004 - acc: 0.8956 - val_loss: 0.7980 - val_acc: 0.6395\n",
            "Epoch 642/1000\n",
            "297/297 [==============================] - 0s 617us/step - loss: 0.2540 - acc: 0.9192 - val_loss: 0.8003 - val_acc: 0.6531\n",
            "Epoch 643/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.2589 - acc: 0.9091 - val_loss: 0.8083 - val_acc: 0.6395\n",
            "Epoch 644/1000\n",
            "297/297 [==============================] - 0s 699us/step - loss: 0.2520 - acc: 0.9259 - val_loss: 0.8462 - val_acc: 0.6599\n",
            "Epoch 645/1000\n",
            "297/297 [==============================] - 0s 612us/step - loss: 0.2518 - acc: 0.9394 - val_loss: 0.8251 - val_acc: 0.6667\n",
            "Epoch 646/1000\n",
            "297/297 [==============================] - 0s 620us/step - loss: 0.3013 - acc: 0.8990 - val_loss: 0.7774 - val_acc: 0.6667\n",
            "Epoch 647/1000\n",
            "297/297 [==============================] - 0s 666us/step - loss: 0.2403 - acc: 0.9394 - val_loss: 0.8194 - val_acc: 0.6667\n",
            "Epoch 648/1000\n",
            "297/297 [==============================] - 0s 618us/step - loss: 0.2596 - acc: 0.9192 - val_loss: 0.7639 - val_acc: 0.6735\n",
            "Epoch 649/1000\n",
            "297/297 [==============================] - 0s 612us/step - loss: 0.2608 - acc: 0.9125 - val_loss: 0.7632 - val_acc: 0.6531\n",
            "Epoch 650/1000\n",
            "297/297 [==============================] - 0s 569us/step - loss: 0.2552 - acc: 0.9293 - val_loss: 0.7734 - val_acc: 0.6259\n",
            "Epoch 651/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.2475 - acc: 0.9360 - val_loss: 0.7836 - val_acc: 0.6531\n",
            "Epoch 652/1000\n",
            "297/297 [==============================] - 0s 563us/step - loss: 0.2689 - acc: 0.9158 - val_loss: 0.7556 - val_acc: 0.6463\n",
            "Epoch 653/1000\n",
            "297/297 [==============================] - 0s 604us/step - loss: 0.2807 - acc: 0.9091 - val_loss: 0.7647 - val_acc: 0.6531\n",
            "Epoch 654/1000\n",
            "297/297 [==============================] - 0s 563us/step - loss: 0.2564 - acc: 0.9192 - val_loss: 0.8076 - val_acc: 0.6667\n",
            "Epoch 655/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 0.2530 - acc: 0.9360 - val_loss: 0.7468 - val_acc: 0.6531\n",
            "Epoch 656/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.2837 - acc: 0.9226 - val_loss: 0.7958 - val_acc: 0.6667\n",
            "Epoch 657/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 0.2397 - acc: 0.9360 - val_loss: 0.8088 - val_acc: 0.6531\n",
            "Epoch 658/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.2631 - acc: 0.9091 - val_loss: 0.7675 - val_acc: 0.6327\n",
            "Epoch 659/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.2554 - acc: 0.9226 - val_loss: 0.8355 - val_acc: 0.6599\n",
            "Epoch 660/1000\n",
            "297/297 [==============================] - 0s 601us/step - loss: 0.2636 - acc: 0.9293 - val_loss: 0.7967 - val_acc: 0.6327\n",
            "Epoch 661/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.2763 - acc: 0.9091 - val_loss: 0.8827 - val_acc: 0.6599\n",
            "Epoch 662/1000\n",
            "297/297 [==============================] - 0s 572us/step - loss: 0.2405 - acc: 0.9394 - val_loss: 0.8086 - val_acc: 0.6395\n",
            "Epoch 663/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.2331 - acc: 0.9360 - val_loss: 0.8267 - val_acc: 0.6735\n",
            "Epoch 664/1000\n",
            "297/297 [==============================] - 0s 550us/step - loss: 0.2251 - acc: 0.9360 - val_loss: 0.7648 - val_acc: 0.6599\n",
            "Epoch 665/1000\n",
            "297/297 [==============================] - 0s 605us/step - loss: 0.2623 - acc: 0.9226 - val_loss: 0.7911 - val_acc: 0.6599\n",
            "Epoch 666/1000\n",
            "297/297 [==============================] - 0s 645us/step - loss: 0.2712 - acc: 0.9125 - val_loss: 0.8066 - val_acc: 0.6463\n",
            "Epoch 667/1000\n",
            "297/297 [==============================] - 0s 612us/step - loss: 0.2520 - acc: 0.9226 - val_loss: 0.8231 - val_acc: 0.6667\n",
            "Epoch 668/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.2649 - acc: 0.9091 - val_loss: 0.8189 - val_acc: 0.6463\n",
            "Epoch 669/1000\n",
            "297/297 [==============================] - 0s 572us/step - loss: 0.2624 - acc: 0.9158 - val_loss: 0.8134 - val_acc: 0.6599\n",
            "Epoch 670/1000\n",
            "297/297 [==============================] - 0s 571us/step - loss: 0.2577 - acc: 0.9293 - val_loss: 0.8302 - val_acc: 0.6327\n",
            "Epoch 671/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.2451 - acc: 0.9394 - val_loss: 0.7970 - val_acc: 0.6667\n",
            "Epoch 672/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 0.2442 - acc: 0.9192 - val_loss: 0.7587 - val_acc: 0.6463\n",
            "Epoch 673/1000\n",
            "297/297 [==============================] - 0s 602us/step - loss: 0.2395 - acc: 0.9293 - val_loss: 0.7761 - val_acc: 0.6735\n",
            "Epoch 674/1000\n",
            "297/297 [==============================] - 0s 584us/step - loss: 0.2335 - acc: 0.9360 - val_loss: 0.7659 - val_acc: 0.6667\n",
            "Epoch 675/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.2463 - acc: 0.9226 - val_loss: 0.8133 - val_acc: 0.6463\n",
            "Epoch 676/1000\n",
            "297/297 [==============================] - 0s 584us/step - loss: 0.2302 - acc: 0.9428 - val_loss: 0.8484 - val_acc: 0.6259\n",
            "Epoch 677/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.2438 - acc: 0.9259 - val_loss: 0.7791 - val_acc: 0.6327\n",
            "Epoch 678/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.2389 - acc: 0.9394 - val_loss: 0.7921 - val_acc: 0.6599\n",
            "Epoch 679/1000\n",
            "297/297 [==============================] - 0s 540us/step - loss: 0.2407 - acc: 0.9158 - val_loss: 0.7809 - val_acc: 0.6599\n",
            "Epoch 680/1000\n",
            "297/297 [==============================] - 0s 587us/step - loss: 0.2477 - acc: 0.9259 - val_loss: 0.8062 - val_acc: 0.6463\n",
            "Epoch 681/1000\n",
            "297/297 [==============================] - 0s 601us/step - loss: 0.2607 - acc: 0.8923 - val_loss: 0.8184 - val_acc: 0.6667\n",
            "Epoch 682/1000\n",
            "297/297 [==============================] - 0s 601us/step - loss: 0.2455 - acc: 0.9125 - val_loss: 0.7601 - val_acc: 0.6667\n",
            "Epoch 683/1000\n",
            "297/297 [==============================] - 0s 626us/step - loss: 0.2474 - acc: 0.9293 - val_loss: 0.8521 - val_acc: 0.6667\n",
            "Epoch 684/1000\n",
            "297/297 [==============================] - 0s 668us/step - loss: 0.2314 - acc: 0.9327 - val_loss: 0.7776 - val_acc: 0.6667\n",
            "Epoch 685/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.2212 - acc: 0.9394 - val_loss: 0.7867 - val_acc: 0.6803\n",
            "Epoch 686/1000\n",
            "297/297 [==============================] - 0s 555us/step - loss: 0.2340 - acc: 0.9461 - val_loss: 0.8383 - val_acc: 0.6599\n",
            "Epoch 687/1000\n",
            "297/297 [==============================] - 0s 617us/step - loss: 0.2349 - acc: 0.9192 - val_loss: 0.8450 - val_acc: 0.6463\n",
            "Epoch 688/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.2350 - acc: 0.9327 - val_loss: 0.8498 - val_acc: 0.6667\n",
            "Epoch 689/1000\n",
            "297/297 [==============================] - 0s 615us/step - loss: 0.2431 - acc: 0.9259 - val_loss: 0.8097 - val_acc: 0.6463\n",
            "Epoch 690/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 0.2217 - acc: 0.9360 - val_loss: 0.7883 - val_acc: 0.6735\n",
            "Epoch 691/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.2153 - acc: 0.9495 - val_loss: 0.8198 - val_acc: 0.6599\n",
            "Epoch 692/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.2184 - acc: 0.9360 - val_loss: 0.8484 - val_acc: 0.6327\n",
            "Epoch 693/1000\n",
            "297/297 [==============================] - 0s 611us/step - loss: 0.2298 - acc: 0.9259 - val_loss: 0.8243 - val_acc: 0.6599\n",
            "Epoch 694/1000\n",
            "297/297 [==============================] - 0s 632us/step - loss: 0.2463 - acc: 0.9192 - val_loss: 0.7726 - val_acc: 0.6395\n",
            "Epoch 695/1000\n",
            "297/297 [==============================] - 0s 608us/step - loss: 0.2359 - acc: 0.9360 - val_loss: 0.8361 - val_acc: 0.6395\n",
            "Epoch 696/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.2194 - acc: 0.9293 - val_loss: 0.7949 - val_acc: 0.6190\n",
            "Epoch 697/1000\n",
            "297/297 [==============================] - 0s 603us/step - loss: 0.2051 - acc: 0.9394 - val_loss: 0.8124 - val_acc: 0.6531\n",
            "Epoch 698/1000\n",
            "297/297 [==============================] - 0s 572us/step - loss: 0.2215 - acc: 0.9259 - val_loss: 0.8135 - val_acc: 0.6667\n",
            "Epoch 699/1000\n",
            "297/297 [==============================] - 0s 566us/step - loss: 0.2258 - acc: 0.9360 - val_loss: 0.7792 - val_acc: 0.6667\n",
            "Epoch 700/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.2376 - acc: 0.9360 - val_loss: 0.8025 - val_acc: 0.6803\n",
            "Epoch 701/1000\n",
            "297/297 [==============================] - 0s 582us/step - loss: 0.2300 - acc: 0.9091 - val_loss: 0.8353 - val_acc: 0.6599\n",
            "Epoch 702/1000\n",
            "297/297 [==============================] - 0s 692us/step - loss: 0.2097 - acc: 0.9495 - val_loss: 0.7997 - val_acc: 0.6599\n",
            "Epoch 703/1000\n",
            "297/297 [==============================] - 0s 608us/step - loss: 0.2191 - acc: 0.9461 - val_loss: 0.7767 - val_acc: 0.6190\n",
            "Epoch 704/1000\n",
            "297/297 [==============================] - 0s 606us/step - loss: 0.2273 - acc: 0.9394 - val_loss: 0.8945 - val_acc: 0.6599\n",
            "Epoch 705/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.2235 - acc: 0.9293 - val_loss: 0.8078 - val_acc: 0.6327\n",
            "Epoch 706/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.2091 - acc: 0.9461 - val_loss: 0.7785 - val_acc: 0.6599\n",
            "Epoch 707/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 0.2206 - acc: 0.9428 - val_loss: 0.8148 - val_acc: 0.6667\n",
            "Epoch 708/1000\n",
            "297/297 [==============================] - 0s 563us/step - loss: 0.2103 - acc: 0.9360 - val_loss: 0.8105 - val_acc: 0.6531\n",
            "Epoch 709/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.2326 - acc: 0.9562 - val_loss: 0.8165 - val_acc: 0.6531\n",
            "Epoch 710/1000\n",
            "297/297 [==============================] - 0s 636us/step - loss: 0.2379 - acc: 0.9259 - val_loss: 0.8239 - val_acc: 0.6531\n",
            "Epoch 711/1000\n",
            "297/297 [==============================] - 0s 606us/step - loss: 0.2178 - acc: 0.9495 - val_loss: 0.8103 - val_acc: 0.6667\n",
            "Epoch 712/1000\n",
            "297/297 [==============================] - 0s 616us/step - loss: 0.2197 - acc: 0.9293 - val_loss: 0.7852 - val_acc: 0.6939\n",
            "Epoch 713/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.2143 - acc: 0.9293 - val_loss: 0.7697 - val_acc: 0.6599\n",
            "Epoch 714/1000\n",
            "297/297 [==============================] - 0s 603us/step - loss: 0.2324 - acc: 0.9226 - val_loss: 0.8082 - val_acc: 0.6667\n",
            "Epoch 715/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.2080 - acc: 0.9394 - val_loss: 0.7879 - val_acc: 0.6667\n",
            "Epoch 716/1000\n",
            "297/297 [==============================] - 0s 599us/step - loss: 0.2324 - acc: 0.9293 - val_loss: 0.7847 - val_acc: 0.6735\n",
            "Epoch 717/1000\n",
            "297/297 [==============================] - 0s 614us/step - loss: 0.2111 - acc: 0.9529 - val_loss: 0.7818 - val_acc: 0.6463\n",
            "Epoch 718/1000\n",
            "297/297 [==============================] - 0s 568us/step - loss: 0.2055 - acc: 0.9394 - val_loss: 0.7814 - val_acc: 0.6735\n",
            "Epoch 719/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.1863 - acc: 0.9529 - val_loss: 0.8738 - val_acc: 0.6463\n",
            "Epoch 720/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 0.2323 - acc: 0.9327 - val_loss: 0.8103 - val_acc: 0.6531\n",
            "Epoch 721/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.2297 - acc: 0.9293 - val_loss: 0.8059 - val_acc: 0.6599\n",
            "Epoch 722/1000\n",
            "297/297 [==============================] - 0s 611us/step - loss: 0.1990 - acc: 0.9461 - val_loss: 0.8242 - val_acc: 0.6395\n",
            "Epoch 723/1000\n",
            "297/297 [==============================] - 0s 562us/step - loss: 0.1986 - acc: 0.9495 - val_loss: 0.8269 - val_acc: 0.6531\n",
            "Epoch 724/1000\n",
            "297/297 [==============================] - 0s 559us/step - loss: 0.2296 - acc: 0.9226 - val_loss: 0.7994 - val_acc: 0.6939\n",
            "Epoch 725/1000\n",
            "297/297 [==============================] - 0s 614us/step - loss: 0.2154 - acc: 0.9394 - val_loss: 0.7941 - val_acc: 0.6667\n",
            "Epoch 726/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.2140 - acc: 0.9394 - val_loss: 0.7780 - val_acc: 0.6463\n",
            "Epoch 727/1000\n",
            "297/297 [==============================] - 0s 638us/step - loss: 0.2230 - acc: 0.9091 - val_loss: 0.7814 - val_acc: 0.6667\n",
            "Epoch 728/1000\n",
            "297/297 [==============================] - 0s 636us/step - loss: 0.2166 - acc: 0.9461 - val_loss: 0.7768 - val_acc: 0.6599\n",
            "Epoch 729/1000\n",
            "297/297 [==============================] - 0s 556us/step - loss: 0.2076 - acc: 0.9562 - val_loss: 0.7931 - val_acc: 0.6871\n",
            "Epoch 730/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.2055 - acc: 0.9394 - val_loss: 0.8876 - val_acc: 0.6599\n",
            "Epoch 731/1000\n",
            "297/297 [==============================] - 0s 572us/step - loss: 0.2019 - acc: 0.9360 - val_loss: 0.8124 - val_acc: 0.6122\n",
            "Epoch 732/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 0.2265 - acc: 0.9360 - val_loss: 0.7922 - val_acc: 0.6735\n",
            "Epoch 733/1000\n",
            "297/297 [==============================] - 0s 599us/step - loss: 0.1918 - acc: 0.9562 - val_loss: 0.8028 - val_acc: 0.6871\n",
            "Epoch 734/1000\n",
            "297/297 [==============================] - 0s 584us/step - loss: 0.1915 - acc: 0.9428 - val_loss: 0.7845 - val_acc: 0.6395\n",
            "Epoch 735/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.2095 - acc: 0.9461 - val_loss: 0.7857 - val_acc: 0.6939\n",
            "Epoch 736/1000\n",
            "297/297 [==============================] - 0s 580us/step - loss: 0.2132 - acc: 0.9327 - val_loss: 0.8116 - val_acc: 0.6735\n",
            "Epoch 737/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.2069 - acc: 0.9360 - val_loss: 0.8280 - val_acc: 0.6735\n",
            "Epoch 738/1000\n",
            "297/297 [==============================] - 0s 605us/step - loss: 0.1949 - acc: 0.9428 - val_loss: 0.7927 - val_acc: 0.6599\n",
            "Epoch 739/1000\n",
            "297/297 [==============================] - 0s 550us/step - loss: 0.2151 - acc: 0.9394 - val_loss: 0.8106 - val_acc: 0.6871\n",
            "Epoch 740/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.2243 - acc: 0.9226 - val_loss: 0.7994 - val_acc: 0.6871\n",
            "Epoch 741/1000\n",
            "297/297 [==============================] - 0s 575us/step - loss: 0.1799 - acc: 0.9663 - val_loss: 0.8827 - val_acc: 0.6871\n",
            "Epoch 742/1000\n",
            "297/297 [==============================] - 0s 590us/step - loss: 0.1909 - acc: 0.9562 - val_loss: 0.7797 - val_acc: 0.6803\n",
            "Epoch 743/1000\n",
            "297/297 [==============================] - 0s 559us/step - loss: 0.1988 - acc: 0.9461 - val_loss: 0.8076 - val_acc: 0.6667\n",
            "Epoch 744/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.1905 - acc: 0.9596 - val_loss: 0.7901 - val_acc: 0.6667\n",
            "Epoch 745/1000\n",
            "297/297 [==============================] - 0s 616us/step - loss: 0.1733 - acc: 0.9596 - val_loss: 0.9486 - val_acc: 0.6531\n",
            "Epoch 746/1000\n",
            "297/297 [==============================] - 0s 603us/step - loss: 0.1988 - acc: 0.9529 - val_loss: 0.8164 - val_acc: 0.6667\n",
            "Epoch 747/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 0.1872 - acc: 0.9663 - val_loss: 0.8300 - val_acc: 0.6735\n",
            "Epoch 748/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 0.2031 - acc: 0.9360 - val_loss: 0.7722 - val_acc: 0.6735\n",
            "Epoch 749/1000\n",
            "297/297 [==============================] - 0s 553us/step - loss: 0.1974 - acc: 0.9495 - val_loss: 0.7690 - val_acc: 0.6735\n",
            "Epoch 750/1000\n",
            "297/297 [==============================] - 0s 599us/step - loss: 0.2030 - acc: 0.9461 - val_loss: 0.7928 - val_acc: 0.6531\n",
            "Epoch 751/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.1895 - acc: 0.9495 - val_loss: 0.8391 - val_acc: 0.6599\n",
            "Epoch 752/1000\n",
            "297/297 [==============================] - 0s 569us/step - loss: 0.1911 - acc: 0.9495 - val_loss: 0.8201 - val_acc: 0.6599\n",
            "Epoch 753/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.1896 - acc: 0.9529 - val_loss: 0.7770 - val_acc: 0.7007\n",
            "Epoch 754/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 0.1844 - acc: 0.9562 - val_loss: 0.8182 - val_acc: 0.6531\n",
            "Epoch 755/1000\n",
            "297/297 [==============================] - 0s 584us/step - loss: 0.1943 - acc: 0.9428 - val_loss: 0.7984 - val_acc: 0.6803\n",
            "Epoch 756/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 0.1735 - acc: 0.9562 - val_loss: 0.8400 - val_acc: 0.6531\n",
            "Epoch 757/1000\n",
            "297/297 [==============================] - 0s 548us/step - loss: 0.1961 - acc: 0.9529 - val_loss: 0.7873 - val_acc: 0.6803\n",
            "Epoch 758/1000\n",
            "297/297 [==============================] - 0s 566us/step - loss: 0.1690 - acc: 0.9562 - val_loss: 0.7689 - val_acc: 0.6531\n",
            "Epoch 759/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.1949 - acc: 0.9529 - val_loss: 0.7730 - val_acc: 0.6735\n",
            "Epoch 760/1000\n",
            "297/297 [==============================] - 0s 621us/step - loss: 0.1786 - acc: 0.9428 - val_loss: 0.7977 - val_acc: 0.6871\n",
            "Epoch 761/1000\n",
            "297/297 [==============================] - 0s 705us/step - loss: 0.1902 - acc: 0.9529 - val_loss: 0.7814 - val_acc: 0.6803\n",
            "Epoch 762/1000\n",
            "297/297 [==============================] - 0s 591us/step - loss: 0.1950 - acc: 0.9495 - val_loss: 0.7935 - val_acc: 0.6803\n",
            "Epoch 763/1000\n",
            "297/297 [==============================] - 0s 561us/step - loss: 0.2151 - acc: 0.9259 - val_loss: 0.7853 - val_acc: 0.6599\n",
            "Epoch 764/1000\n",
            "297/297 [==============================] - 0s 568us/step - loss: 0.1849 - acc: 0.9495 - val_loss: 0.8124 - val_acc: 0.6803\n",
            "Epoch 765/1000\n",
            "297/297 [==============================] - 0s 584us/step - loss: 0.1691 - acc: 0.9596 - val_loss: 0.8183 - val_acc: 0.6735\n",
            "Epoch 766/1000\n",
            "297/297 [==============================] - 0s 576us/step - loss: 0.1886 - acc: 0.9394 - val_loss: 0.7718 - val_acc: 0.6871\n",
            "Epoch 767/1000\n",
            "297/297 [==============================] - 0s 575us/step - loss: 0.1819 - acc: 0.9596 - val_loss: 0.7984 - val_acc: 0.6803\n",
            "Epoch 768/1000\n",
            "297/297 [==============================] - 0s 613us/step - loss: 0.1921 - acc: 0.9428 - val_loss: 0.7821 - val_acc: 0.6735\n",
            "Epoch 769/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.2041 - acc: 0.9360 - val_loss: 0.7852 - val_acc: 0.6803\n",
            "Epoch 770/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 0.1740 - acc: 0.9630 - val_loss: 0.8146 - val_acc: 0.6599\n",
            "Epoch 771/1000\n",
            "297/297 [==============================] - 0s 597us/step - loss: 0.1857 - acc: 0.9596 - val_loss: 0.7984 - val_acc: 0.6735\n",
            "Epoch 772/1000\n",
            "297/297 [==============================] - 0s 630us/step - loss: 0.1940 - acc: 0.9495 - val_loss: 0.8244 - val_acc: 0.6667\n",
            "Epoch 773/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.1967 - acc: 0.9461 - val_loss: 0.8403 - val_acc: 0.6735\n",
            "Epoch 774/1000\n",
            "297/297 [==============================] - 0s 567us/step - loss: 0.2213 - acc: 0.9428 - val_loss: 0.8821 - val_acc: 0.6803\n",
            "Epoch 775/1000\n",
            "297/297 [==============================] - 0s 560us/step - loss: 0.1791 - acc: 0.9562 - val_loss: 0.7800 - val_acc: 0.6939\n",
            "Epoch 776/1000\n",
            "297/297 [==============================] - 0s 642us/step - loss: 0.1762 - acc: 0.9630 - val_loss: 0.7727 - val_acc: 0.6735\n",
            "Epoch 777/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.1651 - acc: 0.9663 - val_loss: 0.8290 - val_acc: 0.6463\n",
            "Epoch 778/1000\n",
            "297/297 [==============================] - 0s 611us/step - loss: 0.1800 - acc: 0.9495 - val_loss: 0.8535 - val_acc: 0.6735\n",
            "Epoch 779/1000\n",
            "297/297 [==============================] - 0s 608us/step - loss: 0.1956 - acc: 0.9461 - val_loss: 0.7927 - val_acc: 0.6667\n",
            "Epoch 780/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.1706 - acc: 0.9731 - val_loss: 0.7780 - val_acc: 0.7075\n",
            "Epoch 781/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.1661 - acc: 0.9630 - val_loss: 0.8089 - val_acc: 0.6871\n",
            "Epoch 782/1000\n",
            "297/297 [==============================] - 0s 577us/step - loss: 0.1843 - acc: 0.9529 - val_loss: 0.8831 - val_acc: 0.6599\n",
            "Epoch 783/1000\n",
            "297/297 [==============================] - 0s 591us/step - loss: 0.1723 - acc: 0.9562 - val_loss: 0.8137 - val_acc: 0.6667\n",
            "Epoch 784/1000\n",
            "297/297 [==============================] - 0s 608us/step - loss: 0.1719 - acc: 0.9596 - val_loss: 0.8015 - val_acc: 0.7075\n",
            "Epoch 785/1000\n",
            "297/297 [==============================] - 0s 615us/step - loss: 0.1633 - acc: 0.9630 - val_loss: 0.7874 - val_acc: 0.6939\n",
            "Epoch 786/1000\n",
            "297/297 [==============================] - 0s 567us/step - loss: 0.1789 - acc: 0.9529 - val_loss: 0.8085 - val_acc: 0.6871\n",
            "Epoch 787/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 0.1589 - acc: 0.9764 - val_loss: 0.8018 - val_acc: 0.7007\n",
            "Epoch 788/1000\n",
            "297/297 [==============================] - 0s 611us/step - loss: 0.1837 - acc: 0.9529 - val_loss: 0.8121 - val_acc: 0.6395\n",
            "Epoch 789/1000\n",
            "297/297 [==============================] - 0s 562us/step - loss: 0.1621 - acc: 0.9596 - val_loss: 0.8194 - val_acc: 0.6871\n",
            "Epoch 790/1000\n",
            "297/297 [==============================] - 0s 649us/step - loss: 0.1902 - acc: 0.9428 - val_loss: 0.8352 - val_acc: 0.6735\n",
            "Epoch 791/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.1634 - acc: 0.9596 - val_loss: 0.7972 - val_acc: 0.6735\n",
            "Epoch 792/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.1757 - acc: 0.9697 - val_loss: 0.8342 - val_acc: 0.6667\n",
            "Epoch 793/1000\n",
            "297/297 [==============================] - 0s 575us/step - loss: 0.1629 - acc: 0.9562 - val_loss: 0.8370 - val_acc: 0.6599\n",
            "Epoch 794/1000\n",
            "297/297 [==============================] - 0s 582us/step - loss: 0.1703 - acc: 0.9596 - val_loss: 0.8230 - val_acc: 0.6599\n",
            "Epoch 795/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.1683 - acc: 0.9562 - val_loss: 0.8226 - val_acc: 0.6599\n",
            "Epoch 796/1000\n",
            "297/297 [==============================] - 0s 571us/step - loss: 0.2016 - acc: 0.9495 - val_loss: 0.7699 - val_acc: 0.6735\n",
            "Epoch 797/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.1908 - acc: 0.9293 - val_loss: 0.8027 - val_acc: 0.6667\n",
            "Epoch 798/1000\n",
            "297/297 [==============================] - 0s 584us/step - loss: 0.1715 - acc: 0.9529 - val_loss: 0.8045 - val_acc: 0.6599\n",
            "Epoch 799/1000\n",
            "297/297 [==============================] - 0s 610us/step - loss: 0.1769 - acc: 0.9562 - val_loss: 0.7828 - val_acc: 0.6939\n",
            "Epoch 800/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.1808 - acc: 0.9630 - val_loss: 0.8317 - val_acc: 0.6667\n",
            "Epoch 801/1000\n",
            "297/297 [==============================] - 0s 577us/step - loss: 0.1871 - acc: 0.9394 - val_loss: 0.8500 - val_acc: 0.6803\n",
            "Epoch 802/1000\n",
            "297/297 [==============================] - 0s 571us/step - loss: 0.1700 - acc: 0.9562 - val_loss: 0.8162 - val_acc: 0.6803\n",
            "Epoch 803/1000\n",
            "297/297 [==============================] - 0s 568us/step - loss: 0.1558 - acc: 0.9731 - val_loss: 0.8470 - val_acc: 0.6667\n",
            "Epoch 804/1000\n",
            "297/297 [==============================] - 0s 570us/step - loss: 0.1624 - acc: 0.9663 - val_loss: 0.8032 - val_acc: 0.6803\n",
            "Epoch 805/1000\n",
            "297/297 [==============================] - 0s 570us/step - loss: 0.1718 - acc: 0.9596 - val_loss: 0.8146 - val_acc: 0.6667\n",
            "Epoch 806/1000\n",
            "297/297 [==============================] - 0s 591us/step - loss: 0.1670 - acc: 0.9596 - val_loss: 0.7989 - val_acc: 0.6667\n",
            "Epoch 807/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.1650 - acc: 0.9428 - val_loss: 0.7982 - val_acc: 0.6667\n",
            "Epoch 808/1000\n",
            "297/297 [==============================] - 0s 591us/step - loss: 0.1583 - acc: 0.9663 - val_loss: 0.8161 - val_acc: 0.6735\n",
            "Epoch 809/1000\n",
            "297/297 [==============================] - 0s 576us/step - loss: 0.1818 - acc: 0.9495 - val_loss: 0.7934 - val_acc: 0.7007\n",
            "Epoch 810/1000\n",
            "297/297 [==============================] - 0s 603us/step - loss: 0.1706 - acc: 0.9461 - val_loss: 0.8359 - val_acc: 0.6667\n",
            "Epoch 811/1000\n",
            "297/297 [==============================] - 0s 557us/step - loss: 0.1595 - acc: 0.9630 - val_loss: 0.8125 - val_acc: 0.6599\n",
            "Epoch 812/1000\n",
            "297/297 [==============================] - 0s 572us/step - loss: 0.1680 - acc: 0.9495 - val_loss: 0.8160 - val_acc: 0.6803\n",
            "Epoch 813/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.1844 - acc: 0.9596 - val_loss: 0.8404 - val_acc: 0.6939\n",
            "Epoch 814/1000\n",
            "297/297 [==============================] - 0s 597us/step - loss: 0.1758 - acc: 0.9529 - val_loss: 0.9168 - val_acc: 0.6735\n",
            "Epoch 815/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.1723 - acc: 0.9529 - val_loss: 0.8333 - val_acc: 0.7075\n",
            "Epoch 816/1000\n",
            "297/297 [==============================] - 0s 590us/step - loss: 0.1598 - acc: 0.9764 - val_loss: 0.8069 - val_acc: 0.6939\n",
            "Epoch 817/1000\n",
            "297/297 [==============================] - 0s 597us/step - loss: 0.1581 - acc: 0.9663 - val_loss: 0.8129 - val_acc: 0.6803\n",
            "Epoch 818/1000\n",
            "297/297 [==============================] - 0s 567us/step - loss: 0.1680 - acc: 0.9562 - val_loss: 0.8270 - val_acc: 0.6803\n",
            "Epoch 819/1000\n",
            "297/297 [==============================] - 0s 696us/step - loss: 0.1347 - acc: 0.9832 - val_loss: 0.8175 - val_acc: 0.6939\n",
            "Epoch 820/1000\n",
            "297/297 [==============================] - 0s 543us/step - loss: 0.1552 - acc: 0.9663 - val_loss: 0.8147 - val_acc: 0.7075\n",
            "Epoch 821/1000\n",
            "297/297 [==============================] - 0s 609us/step - loss: 0.1598 - acc: 0.9495 - val_loss: 0.7814 - val_acc: 0.6735\n",
            "Epoch 822/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.1468 - acc: 0.9798 - val_loss: 0.7948 - val_acc: 0.7143\n",
            "Epoch 823/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.1636 - acc: 0.9596 - val_loss: 0.8080 - val_acc: 0.6803\n",
            "Epoch 824/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.1567 - acc: 0.9562 - val_loss: 0.8302 - val_acc: 0.6871\n",
            "Epoch 825/1000\n",
            "297/297 [==============================] - 0s 571us/step - loss: 0.1738 - acc: 0.9461 - val_loss: 0.8240 - val_acc: 0.6735\n",
            "Epoch 826/1000\n",
            "297/297 [==============================] - 0s 571us/step - loss: 0.1439 - acc: 0.9630 - val_loss: 0.8695 - val_acc: 0.6667\n",
            "Epoch 827/1000\n",
            "297/297 [==============================] - 0s 587us/step - loss: 0.1625 - acc: 0.9562 - val_loss: 0.8286 - val_acc: 0.7007\n",
            "Epoch 828/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.1423 - acc: 0.9731 - val_loss: 0.7693 - val_acc: 0.6803\n",
            "Epoch 829/1000\n",
            "297/297 [==============================] - 0s 613us/step - loss: 0.1579 - acc: 0.9529 - val_loss: 0.8496 - val_acc: 0.6531\n",
            "Epoch 830/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.1481 - acc: 0.9630 - val_loss: 0.7859 - val_acc: 0.6667\n",
            "Epoch 831/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.1624 - acc: 0.9529 - val_loss: 0.7874 - val_acc: 0.6667\n",
            "Epoch 832/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.1526 - acc: 0.9630 - val_loss: 0.8987 - val_acc: 0.6735\n",
            "Epoch 833/1000\n",
            "297/297 [==============================] - 0s 591us/step - loss: 0.1740 - acc: 0.9428 - val_loss: 0.7959 - val_acc: 0.6939\n",
            "Epoch 834/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.1509 - acc: 0.9731 - val_loss: 0.8084 - val_acc: 0.6871\n",
            "Epoch 835/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.1524 - acc: 0.9562 - val_loss: 0.8000 - val_acc: 0.6803\n",
            "Epoch 836/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.1816 - acc: 0.9461 - val_loss: 0.7875 - val_acc: 0.7007\n",
            "Epoch 837/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.1577 - acc: 0.9663 - val_loss: 0.8036 - val_acc: 0.6735\n",
            "Epoch 838/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.1342 - acc: 0.9731 - val_loss: 0.8437 - val_acc: 0.7143\n",
            "Epoch 839/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.1577 - acc: 0.9596 - val_loss: 0.8083 - val_acc: 0.6803\n",
            "Epoch 840/1000\n",
            "297/297 [==============================] - 0s 570us/step - loss: 0.1544 - acc: 0.9562 - val_loss: 0.8210 - val_acc: 0.6871\n",
            "Epoch 841/1000\n",
            "297/297 [==============================] - 0s 599us/step - loss: 0.1542 - acc: 0.9596 - val_loss: 0.8625 - val_acc: 0.6463\n",
            "Epoch 842/1000\n",
            "297/297 [==============================] - 0s 608us/step - loss: 0.1520 - acc: 0.9630 - val_loss: 0.7964 - val_acc: 0.7007\n",
            "Epoch 843/1000\n",
            "297/297 [==============================] - 0s 563us/step - loss: 0.1614 - acc: 0.9596 - val_loss: 0.8029 - val_acc: 0.6599\n",
            "Epoch 844/1000\n",
            "297/297 [==============================] - 0s 556us/step - loss: 0.1556 - acc: 0.9697 - val_loss: 0.8465 - val_acc: 0.7007\n",
            "Epoch 845/1000\n",
            "297/297 [==============================] - 0s 546us/step - loss: 0.1507 - acc: 0.9596 - val_loss: 0.8386 - val_acc: 0.6803\n",
            "Epoch 846/1000\n",
            "297/297 [==============================] - 0s 608us/step - loss: 0.1395 - acc: 0.9731 - val_loss: 0.8857 - val_acc: 0.6803\n",
            "Epoch 847/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.1543 - acc: 0.9596 - val_loss: 0.8204 - val_acc: 0.6735\n",
            "Epoch 848/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.1427 - acc: 0.9663 - val_loss: 0.8217 - val_acc: 0.6667\n",
            "Epoch 849/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.1415 - acc: 0.9798 - val_loss: 0.8485 - val_acc: 0.6735\n",
            "Epoch 850/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.1785 - acc: 0.9562 - val_loss: 0.8824 - val_acc: 0.6871\n",
            "Epoch 851/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.1475 - acc: 0.9663 - val_loss: 0.8027 - val_acc: 0.6803\n",
            "Epoch 852/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.1502 - acc: 0.9663 - val_loss: 0.8565 - val_acc: 0.6939\n",
            "Epoch 853/1000\n",
            "297/297 [==============================] - 0s 612us/step - loss: 0.1378 - acc: 0.9630 - val_loss: 0.7947 - val_acc: 0.7007\n",
            "Epoch 854/1000\n",
            "297/297 [==============================] - 0s 604us/step - loss: 0.1397 - acc: 0.9663 - val_loss: 0.8046 - val_acc: 0.6803\n",
            "Epoch 855/1000\n",
            "297/297 [==============================] - 0s 579us/step - loss: 0.1421 - acc: 0.9630 - val_loss: 0.8081 - val_acc: 0.7075\n",
            "Epoch 856/1000\n",
            "297/297 [==============================] - 0s 607us/step - loss: 0.1361 - acc: 0.9764 - val_loss: 0.8584 - val_acc: 0.6871\n",
            "Epoch 857/1000\n",
            "297/297 [==============================] - 0s 563us/step - loss: 0.1573 - acc: 0.9596 - val_loss: 0.8655 - val_acc: 0.7007\n",
            "Epoch 858/1000\n",
            "297/297 [==============================] - 0s 612us/step - loss: 0.1588 - acc: 0.9529 - val_loss: 0.8700 - val_acc: 0.6667\n",
            "Epoch 859/1000\n",
            "297/297 [==============================] - 0s 565us/step - loss: 0.1228 - acc: 0.9764 - val_loss: 0.8607 - val_acc: 0.6667\n",
            "Epoch 860/1000\n",
            "297/297 [==============================] - 0s 580us/step - loss: 0.1617 - acc: 0.9562 - val_loss: 0.8871 - val_acc: 0.6871\n",
            "Epoch 861/1000\n",
            "297/297 [==============================] - 0s 612us/step - loss: 0.1447 - acc: 0.9663 - val_loss: 0.8496 - val_acc: 0.6803\n",
            "Epoch 862/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.1410 - acc: 0.9731 - val_loss: 0.8308 - val_acc: 0.6939\n",
            "Epoch 863/1000\n",
            "297/297 [==============================] - 0s 617us/step - loss: 0.1182 - acc: 0.9731 - val_loss: 0.8793 - val_acc: 0.6735\n",
            "Epoch 864/1000\n",
            "297/297 [==============================] - 0s 570us/step - loss: 0.1223 - acc: 0.9899 - val_loss: 0.8155 - val_acc: 0.6871\n",
            "Epoch 865/1000\n",
            "297/297 [==============================] - 0s 515us/step - loss: 0.1366 - acc: 0.9697 - val_loss: 0.8116 - val_acc: 0.6939\n",
            "Epoch 866/1000\n",
            "297/297 [==============================] - 0s 557us/step - loss: 0.1399 - acc: 0.9596 - val_loss: 0.8346 - val_acc: 0.6599\n",
            "Epoch 867/1000\n",
            "297/297 [==============================] - 0s 572us/step - loss: 0.1531 - acc: 0.9562 - val_loss: 0.8407 - val_acc: 0.6599\n",
            "Epoch 868/1000\n",
            "297/297 [==============================] - 0s 551us/step - loss: 0.1314 - acc: 0.9731 - val_loss: 0.8100 - val_acc: 0.6667\n",
            "Epoch 869/1000\n",
            "297/297 [==============================] - 0s 560us/step - loss: 0.1510 - acc: 0.9630 - val_loss: 0.8659 - val_acc: 0.6599\n",
            "Epoch 870/1000\n",
            "297/297 [==============================] - 0s 580us/step - loss: 0.1468 - acc: 0.9562 - val_loss: 0.8051 - val_acc: 0.6735\n",
            "Epoch 871/1000\n",
            "297/297 [==============================] - 0s 599us/step - loss: 0.1445 - acc: 0.9731 - val_loss: 0.8037 - val_acc: 0.6531\n",
            "Epoch 872/1000\n",
            "297/297 [==============================] - 0s 596us/step - loss: 0.1434 - acc: 0.9596 - val_loss: 0.8167 - val_acc: 0.6667\n",
            "Epoch 873/1000\n",
            "297/297 [==============================] - 0s 599us/step - loss: 0.1547 - acc: 0.9596 - val_loss: 0.8332 - val_acc: 0.7007\n",
            "Epoch 874/1000\n",
            "297/297 [==============================] - 0s 597us/step - loss: 0.1584 - acc: 0.9495 - val_loss: 0.8512 - val_acc: 0.6803\n",
            "Epoch 875/1000\n",
            "297/297 [==============================] - 0s 564us/step - loss: 0.1531 - acc: 0.9630 - val_loss: 0.8229 - val_acc: 0.6803\n",
            "Epoch 876/1000\n",
            "297/297 [==============================] - 0s 565us/step - loss: 0.1529 - acc: 0.9663 - val_loss: 0.8369 - val_acc: 0.6803\n",
            "Epoch 877/1000\n",
            "297/297 [==============================] - 0s 561us/step - loss: 0.1146 - acc: 0.9865 - val_loss: 0.8557 - val_acc: 0.6803\n",
            "Epoch 878/1000\n",
            "297/297 [==============================] - 0s 709us/step - loss: 0.1367 - acc: 0.9663 - val_loss: 0.8588 - val_acc: 0.7143\n",
            "Epoch 879/1000\n",
            "297/297 [==============================] - 0s 571us/step - loss: 0.1238 - acc: 0.9731 - val_loss: 0.8326 - val_acc: 0.6803\n",
            "Epoch 880/1000\n",
            "297/297 [==============================] - 0s 582us/step - loss: 0.1373 - acc: 0.9731 - val_loss: 0.8142 - val_acc: 0.7075\n",
            "Epoch 881/1000\n",
            "297/297 [==============================] - 0s 594us/step - loss: 0.1369 - acc: 0.9663 - val_loss: 0.7920 - val_acc: 0.6939\n",
            "Epoch 882/1000\n",
            "297/297 [==============================] - 0s 565us/step - loss: 0.1336 - acc: 0.9798 - val_loss: 0.8113 - val_acc: 0.6939\n",
            "Epoch 883/1000\n",
            "297/297 [==============================] - 0s 577us/step - loss: 0.1418 - acc: 0.9697 - val_loss: 0.8923 - val_acc: 0.6735\n",
            "Epoch 884/1000\n",
            "297/297 [==============================] - 0s 567us/step - loss: 0.1278 - acc: 0.9764 - val_loss: 0.8600 - val_acc: 0.6939\n",
            "Epoch 885/1000\n",
            "297/297 [==============================] - 0s 551us/step - loss: 0.1159 - acc: 0.9899 - val_loss: 0.9154 - val_acc: 0.6735\n",
            "Epoch 886/1000\n",
            "297/297 [==============================] - 0s 663us/step - loss: 0.1085 - acc: 0.9865 - val_loss: 0.8456 - val_acc: 0.6803\n",
            "Epoch 887/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 0.1388 - acc: 0.9630 - val_loss: 0.8264 - val_acc: 0.6735\n",
            "Epoch 888/1000\n",
            "297/297 [==============================] - 0s 577us/step - loss: 0.1407 - acc: 0.9697 - val_loss: 0.8826 - val_acc: 0.6735\n",
            "Epoch 889/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.1237 - acc: 0.9798 - val_loss: 0.8522 - val_acc: 0.6667\n",
            "Epoch 890/1000\n",
            "297/297 [==============================] - 0s 559us/step - loss: 0.1105 - acc: 0.9865 - val_loss: 0.8275 - val_acc: 0.6803\n",
            "Epoch 891/1000\n",
            "297/297 [==============================] - 0s 560us/step - loss: 0.1304 - acc: 0.9764 - val_loss: 0.8932 - val_acc: 0.6735\n",
            "Epoch 892/1000\n",
            "297/297 [==============================] - 0s 555us/step - loss: 0.1274 - acc: 0.9697 - val_loss: 0.7948 - val_acc: 0.7007\n",
            "Epoch 893/1000\n",
            "297/297 [==============================] - 0s 616us/step - loss: 0.1141 - acc: 0.9865 - val_loss: 0.8169 - val_acc: 0.6599\n",
            "Epoch 894/1000\n",
            "297/297 [==============================] - 0s 593us/step - loss: 0.1254 - acc: 0.9832 - val_loss: 0.8088 - val_acc: 0.7075\n",
            "Epoch 895/1000\n",
            "297/297 [==============================] - 0s 580us/step - loss: 0.1448 - acc: 0.9764 - val_loss: 0.8219 - val_acc: 0.7211\n",
            "Epoch 896/1000\n",
            "297/297 [==============================] - 0s 590us/step - loss: 0.1002 - acc: 0.9832 - val_loss: 0.7880 - val_acc: 0.7007\n",
            "Epoch 897/1000\n",
            "297/297 [==============================] - 0s 609us/step - loss: 0.1283 - acc: 0.9764 - val_loss: 0.9183 - val_acc: 0.7007\n",
            "Epoch 898/1000\n",
            "297/297 [==============================] - 0s 570us/step - loss: 0.1269 - acc: 0.9731 - val_loss: 0.8010 - val_acc: 0.7075\n",
            "Epoch 899/1000\n",
            "297/297 [==============================] - 0s 568us/step - loss: 0.1182 - acc: 0.9798 - val_loss: 0.8564 - val_acc: 0.6939\n",
            "Epoch 900/1000\n",
            "297/297 [==============================] - 0s 583us/step - loss: 0.1126 - acc: 0.9731 - val_loss: 0.8202 - val_acc: 0.6939\n",
            "Epoch 901/1000\n",
            "297/297 [==============================] - 0s 602us/step - loss: 0.1296 - acc: 0.9630 - val_loss: 0.8669 - val_acc: 0.6871\n",
            "Epoch 902/1000\n",
            "297/297 [==============================] - 0s 601us/step - loss: 0.1318 - acc: 0.9764 - val_loss: 0.8615 - val_acc: 0.6939\n",
            "Epoch 903/1000\n",
            "297/297 [==============================] - 0s 559us/step - loss: 0.1113 - acc: 0.9832 - val_loss: 0.8384 - val_acc: 0.6871\n",
            "Epoch 904/1000\n",
            "297/297 [==============================] - 0s 617us/step - loss: 0.1260 - acc: 0.9764 - val_loss: 0.8240 - val_acc: 0.7143\n",
            "Epoch 905/1000\n",
            "297/297 [==============================] - 0s 558us/step - loss: 0.1198 - acc: 0.9764 - val_loss: 0.8070 - val_acc: 0.7211\n",
            "Epoch 906/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.1214 - acc: 0.9697 - val_loss: 0.8130 - val_acc: 0.6735\n",
            "Epoch 907/1000\n",
            "297/297 [==============================] - 0s 565us/step - loss: 0.1256 - acc: 0.9697 - val_loss: 0.8268 - val_acc: 0.7075\n",
            "Epoch 908/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.1491 - acc: 0.9697 - val_loss: 0.8295 - val_acc: 0.6871\n",
            "Epoch 909/1000\n",
            "297/297 [==============================] - 0s 588us/step - loss: 0.1180 - acc: 0.9764 - val_loss: 0.8278 - val_acc: 0.6871\n",
            "Epoch 910/1000\n",
            "297/297 [==============================] - 0s 581us/step - loss: 0.1074 - acc: 0.9798 - val_loss: 0.8478 - val_acc: 0.6871\n",
            "Epoch 911/1000\n",
            "297/297 [==============================] - 0s 595us/step - loss: 0.1161 - acc: 0.9764 - val_loss: 0.8250 - val_acc: 0.7143\n",
            "Epoch 912/1000\n",
            "297/297 [==============================] - 0s 551us/step - loss: 0.1188 - acc: 0.9832 - val_loss: 0.8858 - val_acc: 0.6327\n",
            "Epoch 913/1000\n",
            "297/297 [==============================] - 0s 591us/step - loss: 0.1266 - acc: 0.9731 - val_loss: 0.8788 - val_acc: 0.6667\n",
            "Epoch 914/1000\n",
            "297/297 [==============================] - 0s 584us/step - loss: 0.1340 - acc: 0.9562 - val_loss: 0.8689 - val_acc: 0.7007\n",
            "Epoch 915/1000\n",
            "297/297 [==============================] - 0s 547us/step - loss: 0.1319 - acc: 0.9731 - val_loss: 0.8571 - val_acc: 0.6939\n",
            "Epoch 916/1000\n",
            "297/297 [==============================] - 0s 592us/step - loss: 0.1245 - acc: 0.9562 - val_loss: 0.8026 - val_acc: 0.6939\n",
            "Epoch 917/1000\n",
            "297/297 [==============================] - 0s 625us/step - loss: 0.1107 - acc: 0.9798 - val_loss: 0.8512 - val_acc: 0.7007\n",
            "Epoch 918/1000\n",
            "297/297 [==============================] - 0s 560us/step - loss: 0.1279 - acc: 0.9697 - val_loss: 0.8303 - val_acc: 0.6871\n",
            "Epoch 919/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.1387 - acc: 0.9663 - val_loss: 0.8445 - val_acc: 0.7075\n",
            "Epoch 920/1000\n",
            "297/297 [==============================] - 0s 571us/step - loss: 0.1213 - acc: 0.9832 - val_loss: 0.8210 - val_acc: 0.7007\n",
            "Epoch 921/1000\n",
            "297/297 [==============================] - 0s 562us/step - loss: 0.1154 - acc: 0.9798 - val_loss: 0.8344 - val_acc: 0.7007\n",
            "Epoch 922/1000\n",
            "297/297 [==============================] - 0s 569us/step - loss: 0.1266 - acc: 0.9731 - val_loss: 0.9001 - val_acc: 0.7007\n",
            "Epoch 923/1000\n",
            "297/297 [==============================] - 0s 536us/step - loss: 0.0945 - acc: 0.9899 - val_loss: 0.8168 - val_acc: 0.6939\n",
            "Epoch 924/1000\n",
            "297/297 [==============================] - 0s 564us/step - loss: 0.1052 - acc: 0.9832 - val_loss: 0.8769 - val_acc: 0.6939\n",
            "Epoch 925/1000\n",
            "297/297 [==============================] - 0s 540us/step - loss: 0.1190 - acc: 0.9798 - val_loss: 0.8244 - val_acc: 0.7075\n",
            "Epoch 926/1000\n",
            "297/297 [==============================] - 0s 548us/step - loss: 0.1174 - acc: 0.9731 - val_loss: 0.8210 - val_acc: 0.7007\n",
            "Epoch 927/1000\n",
            "297/297 [==============================] - 0s 552us/step - loss: 0.1128 - acc: 0.9731 - val_loss: 0.8142 - val_acc: 0.6735\n",
            "Epoch 928/1000\n",
            "297/297 [==============================] - 0s 544us/step - loss: 0.1330 - acc: 0.9663 - val_loss: 0.8191 - val_acc: 0.7279\n",
            "Epoch 929/1000\n",
            "297/297 [==============================] - 0s 540us/step - loss: 0.0974 - acc: 0.9865 - val_loss: 0.8654 - val_acc: 0.6939\n",
            "Epoch 930/1000\n",
            "297/297 [==============================] - 0s 536us/step - loss: 0.1110 - acc: 0.9865 - val_loss: 0.8637 - val_acc: 0.6531\n",
            "Epoch 931/1000\n",
            "297/297 [==============================] - 0s 547us/step - loss: 0.1260 - acc: 0.9596 - val_loss: 0.8323 - val_acc: 0.6871\n",
            "Epoch 932/1000\n",
            "297/297 [==============================] - 0s 557us/step - loss: 0.1016 - acc: 0.9832 - val_loss: 0.8737 - val_acc: 0.6803\n",
            "Epoch 933/1000\n",
            "297/297 [==============================] - 0s 556us/step - loss: 0.1120 - acc: 0.9798 - val_loss: 0.9125 - val_acc: 0.6871\n",
            "Epoch 934/1000\n",
            "297/297 [==============================] - 0s 543us/step - loss: 0.0963 - acc: 0.9865 - val_loss: 0.8188 - val_acc: 0.6735\n",
            "Epoch 935/1000\n",
            "297/297 [==============================] - 0s 527us/step - loss: 0.0939 - acc: 0.9731 - val_loss: 0.8490 - val_acc: 0.7143\n",
            "Epoch 936/1000\n",
            "297/297 [==============================] - 0s 538us/step - loss: 0.1258 - acc: 0.9630 - val_loss: 0.8152 - val_acc: 0.7007\n",
            "Epoch 937/1000\n",
            "297/297 [==============================] - 0s 541us/step - loss: 0.1137 - acc: 0.9764 - val_loss: 1.0037 - val_acc: 0.6735\n",
            "Epoch 938/1000\n",
            "297/297 [==============================] - 0s 576us/step - loss: 0.1210 - acc: 0.9697 - val_loss: 0.8436 - val_acc: 0.7211\n",
            "Epoch 939/1000\n",
            "297/297 [==============================] - 0s 616us/step - loss: 0.1051 - acc: 0.9832 - val_loss: 0.9598 - val_acc: 0.6735\n",
            "Epoch 940/1000\n",
            "297/297 [==============================] - 0s 544us/step - loss: 0.1028 - acc: 0.9798 - val_loss: 0.8493 - val_acc: 0.6803\n",
            "Epoch 941/1000\n",
            "297/297 [==============================] - 0s 560us/step - loss: 0.1032 - acc: 0.9798 - val_loss: 0.8476 - val_acc: 0.7007\n",
            "Epoch 942/1000\n",
            "297/297 [==============================] - 0s 535us/step - loss: 0.1129 - acc: 0.9731 - val_loss: 0.8686 - val_acc: 0.7211\n",
            "Epoch 943/1000\n",
            "297/297 [==============================] - 0s 538us/step - loss: 0.1038 - acc: 0.9832 - val_loss: 0.9167 - val_acc: 0.6531\n",
            "Epoch 944/1000\n",
            "297/297 [==============================] - 0s 578us/step - loss: 0.1101 - acc: 0.9731 - val_loss: 0.8361 - val_acc: 0.7007\n",
            "Epoch 945/1000\n",
            "297/297 [==============================] - 0s 549us/step - loss: 0.1192 - acc: 0.9663 - val_loss: 0.8856 - val_acc: 0.7143\n",
            "Epoch 946/1000\n",
            "297/297 [==============================] - 0s 544us/step - loss: 0.1043 - acc: 0.9798 - val_loss: 0.8759 - val_acc: 0.6803\n",
            "Epoch 947/1000\n",
            "297/297 [==============================] - 0s 554us/step - loss: 0.1209 - acc: 0.9697 - val_loss: 0.8328 - val_acc: 0.6667\n",
            "Epoch 948/1000\n",
            "297/297 [==============================] - 0s 555us/step - loss: 0.0984 - acc: 0.9899 - val_loss: 0.8341 - val_acc: 0.7075\n",
            "Epoch 949/1000\n",
            "297/297 [==============================] - 0s 557us/step - loss: 0.1116 - acc: 0.9832 - val_loss: 0.8581 - val_acc: 0.6939\n",
            "Epoch 950/1000\n",
            "297/297 [==============================] - 0s 533us/step - loss: 0.1202 - acc: 0.9731 - val_loss: 0.8825 - val_acc: 0.6871\n",
            "Epoch 951/1000\n",
            "297/297 [==============================] - 0s 552us/step - loss: 0.1212 - acc: 0.9697 - val_loss: 0.8328 - val_acc: 0.7075\n",
            "Epoch 952/1000\n",
            "297/297 [==============================] - 0s 539us/step - loss: 0.1140 - acc: 0.9697 - val_loss: 0.8751 - val_acc: 0.6803\n",
            "Epoch 953/1000\n",
            "297/297 [==============================] - 0s 559us/step - loss: 0.1057 - acc: 0.9764 - val_loss: 0.8736 - val_acc: 0.7143\n",
            "Epoch 954/1000\n",
            "297/297 [==============================] - 0s 563us/step - loss: 0.1103 - acc: 0.9731 - val_loss: 0.8408 - val_acc: 0.7075\n",
            "Epoch 955/1000\n",
            "297/297 [==============================] - 0s 585us/step - loss: 0.1314 - acc: 0.9630 - val_loss: 0.8186 - val_acc: 0.7007\n",
            "Epoch 956/1000\n",
            "297/297 [==============================] - 0s 600us/step - loss: 0.1205 - acc: 0.9731 - val_loss: 0.8527 - val_acc: 0.7143\n",
            "Epoch 957/1000\n",
            "297/297 [==============================] - 0s 537us/step - loss: 0.1102 - acc: 0.9764 - val_loss: 0.8489 - val_acc: 0.7075\n",
            "Epoch 958/1000\n",
            "297/297 [==============================] - 0s 602us/step - loss: 0.1137 - acc: 0.9798 - val_loss: 0.8620 - val_acc: 0.7007\n",
            "Epoch 959/1000\n",
            "297/297 [==============================] - 0s 551us/step - loss: 0.1042 - acc: 0.9764 - val_loss: 0.9639 - val_acc: 0.6803\n",
            "Epoch 960/1000\n",
            "297/297 [==============================] - 0s 539us/step - loss: 0.1046 - acc: 0.9832 - val_loss: 0.8994 - val_acc: 0.7007\n",
            "Epoch 961/1000\n",
            "297/297 [==============================] - 0s 564us/step - loss: 0.1124 - acc: 0.9731 - val_loss: 0.8690 - val_acc: 0.7347\n",
            "Epoch 962/1000\n",
            "297/297 [==============================] - 0s 553us/step - loss: 0.1054 - acc: 0.9865 - val_loss: 0.8933 - val_acc: 0.6939\n",
            "Epoch 963/1000\n",
            "297/297 [==============================] - 0s 572us/step - loss: 0.1014 - acc: 0.9663 - val_loss: 0.8912 - val_acc: 0.6803\n",
            "Epoch 964/1000\n",
            "297/297 [==============================] - 0s 541us/step - loss: 0.0982 - acc: 0.9899 - val_loss: 0.8260 - val_acc: 0.6939\n",
            "Epoch 965/1000\n",
            "297/297 [==============================] - 0s 544us/step - loss: 0.1195 - acc: 0.9731 - val_loss: 0.8619 - val_acc: 0.6939\n",
            "Epoch 966/1000\n",
            "297/297 [==============================] - 0s 547us/step - loss: 0.1026 - acc: 0.9865 - val_loss: 0.8791 - val_acc: 0.7211\n",
            "Epoch 967/1000\n",
            "297/297 [==============================] - 0s 558us/step - loss: 0.1061 - acc: 0.9697 - val_loss: 0.8357 - val_acc: 0.6871\n",
            "Epoch 968/1000\n",
            "297/297 [==============================] - 0s 591us/step - loss: 0.1206 - acc: 0.9663 - val_loss: 0.8427 - val_acc: 0.6735\n",
            "Epoch 969/1000\n",
            "297/297 [==============================] - 0s 540us/step - loss: 0.1094 - acc: 0.9764 - val_loss: 0.8303 - val_acc: 0.6871\n",
            "Epoch 970/1000\n",
            "297/297 [==============================] - 0s 563us/step - loss: 0.1165 - acc: 0.9764 - val_loss: 0.8525 - val_acc: 0.6803\n",
            "Epoch 971/1000\n",
            "297/297 [==============================] - 0s 544us/step - loss: 0.0970 - acc: 0.9798 - val_loss: 0.8574 - val_acc: 0.7007\n",
            "Epoch 972/1000\n",
            "297/297 [==============================] - 0s 552us/step - loss: 0.0992 - acc: 0.9798 - val_loss: 0.8623 - val_acc: 0.7007\n",
            "Epoch 973/1000\n",
            "297/297 [==============================] - 0s 545us/step - loss: 0.0938 - acc: 0.9832 - val_loss: 0.8681 - val_acc: 0.6871\n",
            "Epoch 974/1000\n",
            "297/297 [==============================] - 0s 540us/step - loss: 0.1090 - acc: 0.9764 - val_loss: 0.9024 - val_acc: 0.7007\n",
            "Epoch 975/1000\n",
            "297/297 [==============================] - 0s 572us/step - loss: 0.0841 - acc: 0.9899 - val_loss: 0.9201 - val_acc: 0.6735\n",
            "Epoch 976/1000\n",
            "297/297 [==============================] - 0s 543us/step - loss: 0.1224 - acc: 0.9596 - val_loss: 0.8828 - val_acc: 0.6871\n",
            "Epoch 977/1000\n",
            "297/297 [==============================] - 0s 549us/step - loss: 0.1052 - acc: 0.9697 - val_loss: 0.8673 - val_acc: 0.6871\n",
            "Epoch 978/1000\n",
            "297/297 [==============================] - 0s 537us/step - loss: 0.1094 - acc: 0.9697 - val_loss: 0.8627 - val_acc: 0.6871\n",
            "Epoch 979/1000\n",
            "297/297 [==============================] - 0s 555us/step - loss: 0.0963 - acc: 0.9764 - val_loss: 0.9054 - val_acc: 0.6939\n",
            "Epoch 980/1000\n",
            "297/297 [==============================] - 0s 562us/step - loss: 0.1059 - acc: 0.9663 - val_loss: 0.8686 - val_acc: 0.7007\n",
            "Epoch 981/1000\n",
            "297/297 [==============================] - 0s 530us/step - loss: 0.0925 - acc: 0.9933 - val_loss: 0.8471 - val_acc: 0.6939\n",
            "Epoch 982/1000\n",
            "297/297 [==============================] - 0s 555us/step - loss: 0.1113 - acc: 0.9764 - val_loss: 0.8643 - val_acc: 0.6803\n",
            "Epoch 983/1000\n",
            "297/297 [==============================] - 0s 537us/step - loss: 0.1166 - acc: 0.9731 - val_loss: 0.9633 - val_acc: 0.6803\n",
            "Epoch 984/1000\n",
            "297/297 [==============================] - 0s 553us/step - loss: 0.1024 - acc: 0.9832 - val_loss: 0.8721 - val_acc: 0.6939\n",
            "Epoch 985/1000\n",
            "297/297 [==============================] - 0s 556us/step - loss: 0.1235 - acc: 0.9663 - val_loss: 0.8371 - val_acc: 0.7007\n",
            "Epoch 986/1000\n",
            "297/297 [==============================] - 0s 573us/step - loss: 0.1094 - acc: 0.9832 - val_loss: 0.8695 - val_acc: 0.6735\n",
            "Epoch 987/1000\n",
            "297/297 [==============================] - 0s 586us/step - loss: 0.1004 - acc: 0.9798 - val_loss: 0.8712 - val_acc: 0.6939\n",
            "Epoch 988/1000\n",
            "297/297 [==============================] - 0s 550us/step - loss: 0.0936 - acc: 0.9832 - val_loss: 0.8824 - val_acc: 0.6871\n",
            "Epoch 989/1000\n",
            "297/297 [==============================] - 0s 587us/step - loss: 0.0943 - acc: 0.9832 - val_loss: 0.8623 - val_acc: 0.7075\n",
            "Epoch 990/1000\n",
            "297/297 [==============================] - 0s 598us/step - loss: 0.0901 - acc: 0.9798 - val_loss: 0.9176 - val_acc: 0.7007\n",
            "Epoch 991/1000\n",
            "297/297 [==============================] - 0s 574us/step - loss: 0.0987 - acc: 0.9697 - val_loss: 0.8520 - val_acc: 0.6939\n",
            "Epoch 992/1000\n",
            "297/297 [==============================] - 0s 609us/step - loss: 0.0960 - acc: 0.9764 - val_loss: 0.8600 - val_acc: 0.7007\n",
            "Epoch 993/1000\n",
            "297/297 [==============================] - 0s 577us/step - loss: 0.0993 - acc: 0.9764 - val_loss: 0.9620 - val_acc: 0.6667\n",
            "Epoch 994/1000\n",
            "297/297 [==============================] - 0s 589us/step - loss: 0.0979 - acc: 0.9798 - val_loss: 0.8875 - val_acc: 0.7075\n",
            "Epoch 995/1000\n",
            "297/297 [==============================] - 0s 565us/step - loss: 0.0968 - acc: 0.9832 - val_loss: 0.8605 - val_acc: 0.6803\n",
            "Epoch 996/1000\n",
            "297/297 [==============================] - 0s 626us/step - loss: 0.0978 - acc: 0.9798 - val_loss: 0.8553 - val_acc: 0.7075\n",
            "Epoch 997/1000\n",
            "297/297 [==============================] - 0s 604us/step - loss: 0.0956 - acc: 0.9798 - val_loss: 0.8738 - val_acc: 0.6735\n",
            "Epoch 998/1000\n",
            "297/297 [==============================] - 0s 605us/step - loss: 0.1009 - acc: 0.9731 - val_loss: 0.8485 - val_acc: 0.6939\n",
            "Epoch 999/1000\n",
            "297/297 [==============================] - 0s 616us/step - loss: 0.0896 - acc: 0.9832 - val_loss: 0.9435 - val_acc: 0.7007\n",
            "Epoch 1000/1000\n",
            "297/297 [==============================] - 0s 697us/step - loss: 0.1088 - acc: 0.9731 - val_loss: 0.9139 - val_acc: 0.6599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvx8sZb9RnoP",
        "colab_type": "code",
        "outputId": "f26f27d2-0c4c-4875-a026-b88ada0e260d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8FdX5+PHPk5ubDRICCTvI5gKI\nskpBsVXcEPeluG+tRVvbamv9ulRbrW3d+rPWumLFuuK+IsqiLFoQBEQFEjbZwpKELft6c35/nEly\nQ3KzkUlu5j7v1yuv3Dszd86ZO8kzZ545c0aMMSillPK+qLaugFJKqdahAV8ppSKEBnyllIoQGvCV\nUipCaMBXSqkIoQFfKaUihAZ8pQAR+a+I/LWRy24RkVMPdT1KtTYN+EopFSE04CulVITQgK/aDSeV\ncpuIfCciBSLyvIh0F5FPRCRPROaJSOeg5c8VkTUickBEFojIkKB5I0VkpfO5N4C4g8o6W0RWOZ9d\nLCLHNrPOvxCRjSKyT0Q+FJFeznQRkX+KSJaI5IrI9yIyzJk3WUTWOnXbISJ/aNYXptRBNOCr9uYi\n4DTgSOAc4BPgLqAr9u/5twAiciQwA7jFmTcL+EhEYkQkBngfeBnoArzlrBfnsyOB6cANQArwLPCh\niMQ2paIiMhF4AJgC9AS2Aq87s08HfuxsRydnmb3OvOeBG4wxicAw4POmlKtUKBrwVXvzb2NMpjFm\nB/AFsNQY840xphh4DxjpLHcJ8LExZq4xpgz4BxAPHA+MA/zAY8aYMmPM28DXQWVMBZ41xiw1xgSM\nMS8CJc7nmuIKYLoxZqUxpgS4ExgvIv2BMiARGAyIMSbNGLPL+VwZMFREkowx+40xK5tYrlJ10oCv\n2pvMoNdFdbzv6LzuhW1RA2CMqQC2A72deTtMzZEDtwa97gfc6qRzDojIAaCv87mmOLgO+dhWfG9j\nzOfAE8CTQJaITBORJGfRi4DJwFYRWSgi45tYrlJ10oCvvGonNnADNmeODdo7gF1Ab2dapcOCXm8H\n/maMSQ76STDGzDjEOnTApoh2ABhjHjfGjAaGYlM7tznTvzbGnAd0w6ae3mxiuUrVSQO+8qo3gbNE\n5BQR8QO3YtMyi4ElQDnwWxHxi8iFwNigzz4H3CgiP3IurnYQkbNEJLGJdZgBXCciI5z8/9+xKagt\nInKcs34/UAAUAxXONYYrRKSTk4rKBSoO4XtQqooGfOVJxph1wJXAv4E92Au85xhjSo0xpcCFwLXA\nPmy+/92gzy4HfoFNuewHNjrLNrUO84B7gHewZxWDgEud2UnYA8t+bNpnL/CIM+8qYIuI5AI3Yq8F\nKHXIRB+AopRSkUFb+EopFSE04CulVITQgK+UUhFCA75SSkWI6LauQLDU1FTTv3//tq6GUkq1GytW\nrNhjjOnamGXDKuD379+f5cuXt3U1lFKq3RCRrQ0vZWlKRymlIoQGfKWUihAa8JVSKkKEVQ6/LmVl\nZWRkZFBcXNzWVXFVXFwcffr0we/3t3VVlFIeFfYBPyMjg8TERPr370/NwQ29wxjD3r17ycjIYMCA\nAW1dHaWUR4V9Sqe4uJiUlBTPBnsAESElJcXzZzFKqbblagtfRLYAeUAAKDfGjGnmelqyWmEpErZR\nKdW2WiOlc7IxZo+bBWTmFpMQ4yMxTvPfSikVStindBojO6+E/JJyV9Z94MABnnrqqSZ/bvLkyRw4\ncMCFGimlVPO4HfANMEdEVojI1LoWEJGpIrJcRJZnZ2c3qxAB3BrWP1TALy+v/wAza9YskpOT3amU\nUko1g9spnQnGmB0i0g2YKyLpxphFwQsYY6YB0wDGjBnTvLAt9sjihjvuuINNmzYxYsQI/H4/cXFx\ndO7cmfT0dNavX8/555/P9u3bKS4u5uabb2bqVHtcqxwmIj8/nzPPPJMJEyawePFievfuzQcffEB8\nfLxLNVZKqbq5GvCNMZUPa84Skfewzw1dVP+nQrvvozWs3Zlba3phaYDoKCEmuuknLEN7JfHnc44O\nOf/BBx9k9erVrFq1igULFnDWWWexevXqqu6T06dPp0uXLhQVFXHcccdx0UUXkZKSUmMdGzZsYMaM\nGTz33HNMmTKFd955hyuvvLLJdVVKqUPhWkrHefBzYuVr4HRgtVvltdaDGseOHVujr/zjjz/O8OHD\nGTduHNu3b2fDhg21PjNgwABGjBgBwOjRo9myZUsr1VYppaq52cLvDrzndDeMBl4zxnx6KCsM1RJP\n25VLx9ho+nZJOJTVN0qHDh2qXi9YsIB58+axZMkSEhISOOmkk+rsSx8bG1v12ufzUVRU5Ho9lVLq\nYK4FfGPMD8Bwt9YfzM0e7ImJieTl5dU5Lycnh86dO5OQkEB6ejpfffWVizVRSqlDE/ZDKzSGuHjR\nNiUlhRNOOIFhw4YRHx9P9+7dq+ZNmjSJZ555hiFDhnDUUUcxbtw4l2qhlFKHToxb/RmbYcyYMebg\nB6CkpaUxZMiQej+3bncecf4o+qV0qHe5cNeYbVVKqWAisqKxoxh44sYrHZVAKaUa5omAD+7deKWU\nUl7hiYCvDXyllGqYNwK+SKv1w1dKqfbKEwEf7ENElFJKheaJgO9mt0yllPIKbwR8cC3iN3d4ZIDH\nHnuMwsLCFq6RUko1jycCPrjXwteAr5TyCo/caStgKlxZd/DwyKeddhrdunXjzTffpKSkhAsuuID7\n7ruPgoICpkyZQkZGBoFAgHvuuYfMzEx27tzJySefTGpqKvPnz3elfkop1VjtK+B/cgfs/r7W5J5l\nASow4G/G5vQ4Bs58MOTs4OGR58yZw9tvv82yZcswxnDuueeyaNEisrOz6dWrFx9//DFgx9jp1KkT\njz76KPPnzyc1NbXp9VJKqRbmmZROa5gzZw5z5sxh5MiRjBo1ivT0dDZs2MAxxxzD3Llzuf322/ni\niy/o1KlTW1dVKaVqaV8t/BAt8cy9BZSUVXBkj0RXizfGcOedd3LDDTfUmrdy5UpmzZrF3XffzSmn\nnMKf/vQnV+uilFJN5YkWvuDeRdvg4ZHPOOMMpk+fTn5+PgA7duwgKyuLnTt3kpCQwJVXXsltt93G\nypUra31WKaXaWvtq4YcignEp5AcPj3zmmWdy+eWXM378eAA6duzIK6+8wsaNG7ntttuIiorC7/fz\n9NNPAzB16lQmTZpEr1699KKtUqrNeWJ45O37CikoKWdwzyQ3q+c6HR5ZKdVUkTc8MnqnrVJKNcQT\nAR8dWkEppRrULgJ+Q2kn8UDED6fUmlLKm8I+4MfFxbF37956A6IdPK39BkxjDHv37iUuLq6tq6KU\n8rCw76XTp08fMjIyyM7ODrnMgaIyCkvKicqJb8Watay4uDj69OnT1tVQSnlY2Ad8v9/PgAED6l3m\ngVlpvLhkB+n3n9lKtVJKqfYn7FM6jREVJQQq2m9KRymlWoMnAn50lFCuAV8pperliYDvixKMgQoN\n+kopFZInAn50lAAQ0K6NSikVkicCflRlwNcWvlJKheSJgB+tAV8ppRrkiYDvi7Kbsb+wtI1ropRS\n4csTAX/8d3/kvKgv+fVr37R1VZRSKmy5HvBFxCci34jITLfKOHzP5wyL2kL67ly3ilBKqXavNVr4\nNwNpbhYQkGiiCeD3eeKERSmlXOFqhBSRPsBZwH/cLCeADz/lJEQF4PO/QWmBm8UppVS75HaT+DHg\n/4CKUAuIyFQRWS4iy+sbIK0+5U4L/wJZAIsehoUPNa+2SinlYa4FfBE5G8gyxqyobzljzDRjzBhj\nzJiuXbs2r6woP34J0CXWmVBW3Kz1KKWUl7nZwj8BOFdEtgCvAxNF5BU3CkrqmICfcnp3br/DIyul\nlNtcC/jGmDuNMX2MMf2BS4HPjTFXulGW+Px0iDZU6NAKSikVkje6tUT58ROgpDzkpQKllIp4rRLw\njTELjDFnu1aAL5pAeSmrd+RUluhaUUop1V55poUfTQCDHVMHTe0opVQt3gj4PttLR8O8UkqF5o2A\nH2X74SullArNGwHfF4Of8qAJ2tZXSqmDeSTg2146VTl8pZRStXgj4EdFE12jha+UUupgngn4Hf1t\nXQmllApv3gj4IqR0iGnrWiilVFjzRsBHEEH74SulVD08EvA56HKtBnyllDqYNwK+CKJBXiml6uWN\ngI8QFdzEz9nRZjVRSqlw5Y2ALzbgR/ucqL9hdtvWRymlwpA3Aj6AMcT7tW+mUkqF4pGAL4Ahzu+R\nzVFKKRd4I0LaPpnERHtjc5RSyg0eiZA2d++L8sjmKKWUCzwUIQ1RooOnKaVUKN4I+CJgDKIBXyml\nQvJGwHcu2kqUBnyllArFGwHfifO+oHgfqNA7b5VSKpg3Aj7YlE7QiDol5frIQ6WUCuaRgG9TOgkx\nvqopxWUVbVcdpZQKQ94I+M5F265JcVWTisu0ha+UUsG8EfCdVM7xh6dWTdGAr5RSNXkk4AMY/EE3\nXu08UNyGdVFKqfDjjYDvpHSCXfn80jaqjFJKhSdvBPzK3jl645VSSoXkkYAPdT3WsEL74iulVBVv\nBPw6UjoAOUVlbVAZpZQKT94I+E4//IPll5S3flWUUipMuRbwRSRORJaJyLciskZE7nOrrOrcfc0c\nfrmmdJRSqkq0i+suASYaY/JFxA98KSKfGGO+cqU0Y2pdtA1U6N22SilVybUWvrHynbd+58elJnfd\nKZ1TH13EXe99706RSinVzriawxcRn4isArKAucaYWp3jRWSqiCwXkeXZ2dnNLajOi7YAry3d1rx1\nKqWUx7ga8I0xAWPMCKAPMFZEhtWxzDRjzBhjzJiuXbs2syTtf6+UUg1plV46xpgDwHxgkouloIFf\nKaVCc7OXTlcRSXZexwOnAekuFeba1QGllPIKN3vp9AReFBEf9sDypjFmpjtF1X3RVimlVDXXAr4x\n5jtgpFvrr0FCj6Xj92maRymlwDN32mJ76dTRU6dDrJsnMUop1X54JOBXpnTqCPgxGvCVUgq8EvAr\n++HX0cKP1pSOUkoBXgn4VWoH/PKAXsxVSinwVMCvu4VfUq7j6SilFHgl4FcNrVA74O/JL6GkXB9o\nrpRSjQr4InKziCSJ9byIrBSR092uXOPVlaevDv7Xv7i89aqilFJhqrEt/J8ZY3KB04HOwFXAg67V\nqlnqTukAfLFhTyvXRSmlwk9jA35lE3oy8LIxZg3hNHBNHSkd0TtvlVKqhsYG/BUiMgcb8GeLSCIQ\nRldDnX74QS38GPTxhkopFayxAf/nwB3AccaYQuzDTK5zrVZNVTWkQnXAXxd3LVNHxgPQKd7fBpVS\nSqnw0tiAPx5YZ4w5ICJXAncDOe5VqxnquPHqrhOSuHp8v7qG2FFKqYjT2ID/NFAoIsOBW4FNwEuu\n1arJQoyWKVH4ooQDhWUs/WFvq9dKKaXCSWMDfrkxxgDnAU8YY54EEt2rVhOFesShROH32U28ZJo7\nz05XSqn2orEBP09E7sR2x/xYRKKwefzwYCrABGDLlzWnR/mIjqrO5+QUlbVyxZRSKnw0NuBfApRg\n++Pvxj6j9hHXatVUhU66Zu37NadLFNG+6k3UO26VUpGsUQHfCfKvAp1E5Gyg2BgTPjl8CbEZ4sMf\n1MIv04HUlFIRrLFDK0wBlgE/BaYAS0XkYjcr1jQhuuEc1MIvD4TRrQNKKdXKGvt0kD9i++BngX1A\nOTAPeNutijVJPf0ug4O8tvCVUpGssQE/qjLYO/YSTiNthkrpYCgqCw742sJXSkWuxgbtT0Vktohc\nKyLXAh8Ds9yrVlOFaOGbCorKqi/UnvmvL9iYld9KdVJKqfDS2Iu2twHTgGOdn2nGmNvdrFiThErp\nGENxWYB4ihFn6J+n5m9sxYoppVT4aPQTvo0x7wDvuFiX5gsxLDIYAiWFpMX9jGfKz+HB8ss4oH3x\nlVIRqt4WvojkiUhuHT95IpLbWpVsWIiAbyroE1MAwLm+/wFwoLC0tSqllFJhpd4WvjEmfIZPaA5j\nuGF8D/geikwsAAUlevOVUioyNTqlE9ZCpXRMBbHGpnAKiAOgrEJ76iilIlP4dK08JKFz+JQWAlCE\nbeGXlmvAV0pFJo8E/BCMgVLbDbPAOC187YuvlIpQ3gj4IVM6BkrtRdtiYgC921YpFbm8EfDrS+mY\nihpLlGlKRykVoTwS8EMwFVUBv1KppnSUUhHKtYAvIn1FZL6IrBWRNSJys1tl1ZvSqWrh27txywIV\nmJA3aimllHe52cIvB241xgwFxgE3ichQF8urg4GKmv3uKww8PHudfRMoB+2mqZSKEK4FfGPMLmPM\nSud1HpAG9HaptBCTa6d0AJ5esMm+uD8F3r3enSoppVSYaZUcvoj0B0YCS+uYN1VElovI8uzs7OYV\n0IiUzkmJOxgoO6tmrdy2375YHZ7DAymlVEtzPeCLSEfsoGu3GGNqjb9jjJlmjBljjBnTtWvXFi7d\n2IebAx0LM/g89g9Vc25989sWLksppcKbqwFfRPzYYP+qMeZd90qqL6VT97zNewrcq45SSoUhN3vp\nCPA8kGaMedStcoB6uuHXvmhbl6+37GvZ+iilVBhys4V/AnAVMFFEVjk/k10sr7Y6LtpuefCsWov9\n9JklrVUjpZRqM66NlmmM+ZKQzx5s8dJCT6+jl05ibDR5JfogFKVUZPHGnbb19tKpI6UjEBXyIKGU\nUt7kjYAfiqm7hQ8Qhd5wpZSKLB4J+E1L6QD4ggL+HdGvQU6GC/VSSqnw4ZGAH4KpqHPohG6JsUjQ\nQeLG6Jnw3o2tWTOllGp13gj4jbjTtsraD3l1Sh/OGtat5vSAPtxcKeVt3nimbVNSOm9eRY/Enlx5\n7mzY6HrFlFIqbHijhR+Kqai7l07eLuJ9rV8dpZRqS94I+E1J6Tji/VJrUaWU8jJvBPz6xtIJMbTC\nwS38orKGh2BQSqn2zCMBP5TQLfy46Jot/O37i1qjQkop1Wa8EfCbkdKJ8dX8TJxfk/pKKW/zRsAP\nJcQTrwBiD4rvfl8rDfujlFJtxCMBv+l32spB0/WirVLK67wR8ENF6w9/G3o8/NKaD0ApLteLtkop\nb/NGwA+lNB9Kaj1V0Xr6+BpvBxWtpqJCm/lKKe/ydsAH+O6NRi+aV1TiYkWUUqpteSTgt0zLPC8/\nn0CFIaAtfaWUB3kj4LfQFdfcggLGP/AZZzy2qEXWp5RS4cTjg6c1Tfr2PWTllZCVV0dqxxgozoH4\n5BYpSymlWpu28IM8+un3oWcumwYP9YN9P7RIWUop1dq8EfBbSCylxFLKkbK99sz0j+3vfZtbt1JK\nKdVCPBLwW6aFH0M5j/mfZE7s7Tz84XI2ZOZVzxSp+VsppdoZjwT8lnGB70vO9H0NwCuLN3HaPxex\navuBmgvpLblKqXbKGwG/hYLw1OiPq15HOw8533VAR9FUSnmDR3rptLxo7FALv3x1JQkxPtYcDprM\nUUq1Z95o4TckqunHtcFR26peF5YGdNgFpVS7FxkB35/Q5I+8FPMQk6O+qnpfXjm4Zk4dPXiUUqod\n8EbAryuHP+IKSDnCvm5MwE9IqTUpuJW/Yut+++Kjm5tTQ6WUanPeCPh1dcscfxN0SLWvYxoR8KX2\nE68qgr6e8oNSOusz8ygqKYOSvJofWvxveO3ShstTSqlW5pGAH2TAT+DeHOh+NMR3sdOkEZtZxzI3\n+j4CYFzUWn7sq74Lt3R3GjueOIs1/7oAHuhjx9avHHd/zt2w/pND3gyllGpp3uilU5nS6dgDznui\nenqCE/DLnK6Vo6+Fnd/Arm9rryOqdgs/Tsp4zv8P/NR8OErMM+M42QcUOhP+3sumkM5/6pA2Qyml\n3ORaC19EpotIloisdquMak7AP/ufkHxY9eTeo+3v5H72d1Q0XPhc3auoI6UDcJpvJSf56jhAHGzV\nq/DDwkbWVymlWp+bKZ3/ApNcXH+1cb+0v/uMqTl99LVw9Ycw6mr7vjg3dBfNqBb4Kl46t/p1Rd3P\n0lVKqbbiWsA3xiwC9rm1/hoGnmTz9h271ZwuAgN/Uj2kcXFOnakbu2wLfxUBO8Tyqu0HmLs2s2XX\nrZRqW+mzYNP8tq5Fk7X5RVsRmSoiy0VkeXZ2tjuF9Bxhfx9zcegWfkJqy5bpXDc4/8n/8YuXlttp\ngbLqln95CSx5EspLW7bcUDZ+Bh//oXXKUsrrXr8MXj6/rWvRZG0e8I0x04wxY4wxY7p27epOIUk9\n7RnAsVNqB/w+x8GUl+HE3zdqVaXRiY1aLu2t+1j4t7P4e7RzzaBgD9yfCrNute9XvgSz74Iv/l/N\nD+76DjLXNqqMJnnlQvj6uereRI2VPgtKCxteTqmKAKz7JLIGGMxeV/N9cS7c2wm+ebXu5V+YDK9O\nsa8/uAneuAoK9lZ3LHFZmwf8Vhcc8M98BK6fB0PPhaPObNTHY342s1HLDdn8X35S9iWXR89nS9zl\n8MggO2P5dDbtyIQtX9j3Cx+0vYb2bLT/KM+eCE+Pt/MqAvasIFjaR/BAX5j/QKPqUUtxTuOX3bnK\ntmRm39m8slRkWfoszLgU1r7fsus9+F6XlmBMzQNT4b7mnW3Pu69mIypvl/298MGay2WuteVt/R9s\nmG3P8L95BdI+hEcGwvOnNb3sZoi8gJ+QAuNugms+gh9NrTmvxzHVr0dc6VwX6G7fT7wHfp8OvUbA\nb1bCkHOaXYVBzx0Jaz+onvDsj+GJ0TDjsupp25fBX7rYs4JAOXx6F2xdAm9cCSW59g/qvRsbfgJX\n5hpY+2H1+2JnuOdAuW2NrJoRukVWuWxDZfywAFa/U/e8tR/A0mn1f16Fp4K9ULS/8ctXDjuSu7Pl\n6rDpc3uvy9YlNadnpdlGUnO9MBnu72r/B0oL4OEB8O71dS9bWgjfv23/TyoC8FD/6nnrPoaZt1S/\nr2xQHdhW3Vhb/Y5txD01vnq56WfULGP3961yZuRmt8wZwBLgKBHJEJGfu1VWk4jApL/DgB/XnnfF\n2/Dj/4PJ/4AzH7LTrnofTr0PfvwHmxoCSBkEP30Jzn6MzaknMaHkXywc/jBlF7/Eyr7XAnBnWe3N\nvbT07vrrFnzDVvAR/z8T4asn4YWDOj19OwPWvA/7t0BOhk0TBbfgKwLw9PHw5lXV04qcIP7EaHiw\nL7x/I7x1LZTkB633dXjzGlg+vf76VnrpPHj7Z3XPe/Nq+OQ2eO2ShtdTEbAHosYq2Ft/6y9vd+PX\n1RRZafDyBTZYgL0u09A/6/6tsGGeO/WptGeDbTnWZcGDNQ/8jfHIwJrBzRj75LfKFu20k2HWbfb1\n1iXwlXMfSn1pw13fwke32GU+/E3tQH6wTZ/b39sW298l+Xa/PjXO/g1X+v5t+Pdo+7dcnGu/63s7\n2fflpTXPlLcvs+urKLP/A485Db21H1Qvt3UxTDsJ9m6CT++Ad34O//uXbUAdfBBc+RK890uYPqnm\nPT73p9r7ftY5/9fZadXzdn5Tcx0ph4Nxv2efmDDKt40ZM8YsX768ravRJCXlAV5espVrju+P32eP\nnyZQzjOLtjBt9tcUE8NhksU2040i4hghGwkQRQVCLGWMGzuWwwrT6Vm0jsEx2XQbNBKZe0/zKxST\nCBdPh6J98N4Ntef3HQcn/BZev7zm9IEnww/z4aLn7R93sAE/gWs+hJwdkNijZk+nknx4oLd9/ecD\n9mygQ1eIS7LT7u1Uvey9QQejQBn4/DXLeXoC5GyDO5wxjCqDv6+OC+2Bcrg/xW7P2F9A37E178FY\nOs0eaC5+AYZdWPvzdVn7oT1A3bkdYoOu1WxbCtNPh9+thU697cFw7ftw/tNw7CX2TGzC7+HUP9de\nZ3mpvX5SmcK7txEptZ3f2I4G/3vM9h47oRHjN2WvhyePg6MvhAm3QOpR4I+rnl+5HyrLD5TDps/g\n8NNg6TP2zLWsyDaIBk2s+zPps2yK75Q/22tewfOD9/Op91XX+eAnxP29D5Tm2TPktI8gyg9/2lM9\nP1BmDxyDJtoz7tl/hCVPwKn3woTf2YPMzpXVy/9uDTwxFsoKqqdJVM3g2WUQ7Ntk99fKl6EgC/aG\nOjsQuGsHvHwhbP8qxDJN5O9Qs351Oe0vjdvPdRCRFcaYMQ0vqQHfVbtyivj1a98wsm8y//mycc/C\n9RHget8szrv4Ko5KLCMrt5Cen90MR58P0XFw3PWQudr+Ua95H757veGVig9MEy/WhjJ2Kpx2P/yt\ne+15l7wKb1xhA/4t39uW8HMnV8+/4FkYfqltDaV9CNd9AllrbeAvzoGZv7PLVQaYx46x2znlZfjk\ndigvsv/4ezbArIN6HCWk2IDsj7MtsMqW6ehrYfV7NoDfsBBePNceAI6cBHs3wOBzID/Tvv7kDtsK\nu+EL6Hms/fzif9vhMoLr/8FNNv867CIbmD64yX7Hd2yFhQ/BSXdVj9+0dxP8e1R1Pf+4G/zx1e/3\nb7Fpw7Iie+q/ab5NE5z+1+py782xrdL5f7PfcWzHmttesBcWPWwDd6Uex8DURfb+kvSPqw/wl86A\n79+EmA52G4ZfZs8Ug8UmwYm3wjznAHblO9DtaHuA+OAmO+3Xy+EJJ8Zc+a49qNVl7A0w+WHIz7Ld\npoMPDJXOfMQenHuPgUEnw6JH7PSLX4C3r7OvO3aHy9+EaT+pu5yWVHmAaCkJKTZ78MntkLGs5ryo\naKgoh5/NhsPGNWv1GvDD0I4DRXyensU979sbjx+++Fhe+Wor32U03OIb3TeJFdtzeeDCYxg/MIXO\nCTHEx/iI8Yntbrn1S+jUB7oOgW1L4PP7YfDZ0H8C9BpleyIVZMPKF23QCHcTfg971kN64y6QVznq\nLBh1lb1wWOmIM+xFslAGnmSvQYD9/rLTbCt59LUQHVsz13rSnTZd8NWTtdcT5YfuQ6tP6a+Zae/0\nfvEc2BH0Nz30PJsWiOkA5zweOoD1GVsdHK771Pbo2rnStnJ7HGvfT3nZ3u/x37PqXscZf4cdK2H1\n26G3vym6DKy+nnPY8dVploZUtuZbU7ehMOZntRsGwRJSoHBv88s4/DR7NrVsmj373b8FCvfAtR/b\nv4PZd9m/i6vft/+LxtiGzfdv2YZD5VlkRcUh3fipAT+MzVubSW5xGReO6oMxhleWbqs6CCy+YyLH\nP/h5o9Zz/KAUrj9xAJ+u3s0yi5OsAAATgklEQVTNpx7Jss178UVFce7wXg1/OHMtpB4JK16wp9xH\nTbaBoccxtnXV41jo2NX+fm5i9cXb9ig6DsqL27oW7ul/YnW6COzZVYFL97O0lTMesGdaeQ1cDL5p\nGTw51r4+61E47ueQNtOeBad9BJMegtHXwN96QHxnuH0LfHpn9bUHsNfs0j60Z1ilBfaAf87jdoj1\ngiyb0kzuZ68tHDmp5tmWMbabZrfBLf4V1EcDfjuTlVtMTHQUyQkx9L/DPlf3j5OH8LdZaQ18srZ7\nzh5K+q5c+nZJoHtSLJccd1jDH2qM4hybj45PhjXv2dbrx7+HTn3h9Pth9t2w6pXq5Sf83rZG18+2\nF8f2bHDSUO/aHO8P86svCscl21z4smfrLrvfBNi1CkqDLixfM9NeoHz1oro/c+IfYPvSmsEwWHS8\nTRE117lPwOd/hXyXLgw3x8S7Yfxv7BnLjBAXyRN72pTSwT2vjrse+p1QnULpObz6bOWyN6rXN3aq\nLeeFswADg8+CskKb2pp9N/QYBt+9YZdN6g25O2qW87M59qwnUALHTLFpjCHn2hTYP4+26bhLXrWN\nkANbbHqpQ6pNeX3wazj+N9VnRZe86lxzOMWmRnzR9mJwTgZ07hf6e9qzwa43sbv9m87fbb+zEVeE\nvhM/jGnAb8cqA/6mv0/mV6+uILeonOz8EjZm2WDXOzmeHU14sPq1x/fnhMNTOWVwN15bto2hvZIY\ndVhnV+pexZjaF+vqsn2ZzZd2cB4+k73enmbvWmW7nh5xBqz4rw0KPY61/9ClhfbUuftQW863r9sL\ncBJl89iVfr3Cpk0WP25TKBN+Z68pABx+KnQ9Eg5st624KJ+9ASY7Dc5/BpL7wvpPbe+LCb+3ufoZ\nl9kW3pFnwJif24uclTZ9bnvtRMfZFuAZD9h8fGxH2ytq7ftwy2qbF87Pht6jYN9m+Ph3tvvetR/b\ntMykh2DuPdChm22lD/gxbJxryxh1jU3JAfxyib1AXXmxfOoCewPf5H/Y8gE2L4Kv/1Pd/ffW9fZ7\nSj3Cdp9c8BCc+7jd/g1zYdyNdrlAud13Bwe+nAxbr+iYhvdrVrrtyRYVbb/zbkNswPb57U95iU11\nHJzGqOxhFldHnj9Y7k77dxId23BdIoAG/Hbs0bnriff7+OVJg6qmBSoM+cXlPLVgI1OO68vyLft4\n4X9bSN+dx+lDu5OaGEvH2GimLWqgv7xj0W0ns+SHPRgDU8b0JSrKBuf1mXk88flGHr74WOL87ayl\nU1Fhe0LEJtrA0VDQCDflpXUH04wVdpjvLgNs0PXF2nQbwMZ5NoXTc3j96zUVNXvsKE/RgB8hNu8p\noG/neKKd7qBXPb+ULzbsaeBTdYuNjqJLhxh25RTz5OWj+H5HDjefcgTxMdWB3xhDWcAQEx159+sp\nFa404Eeo/JJysvNK8Inw2rJtnH50d77csIdH564/pPX+5byjGdwjiYXrs3hyvu2uduNPBvHbUw4n\n3u9DGpO+UUq5QgO+qmH7vkJOfHg+w/t04sWfjWXk/XMxBsYO6MKyzTVHsG7qNYJzh/fitjOO4tWl\n2xjdrzM9O8UxrHc7S6co1Y5pwFf1MsawYF02Jx6RSkFJgKumL+XP5wxl+74izj62J3e/v5rXv97e\n7PVPOroHfzpnKKt35PCTo7oSG93Orgco1Y5owFeH7EBhKd9m5PDi4i18np5VNX1wj0T2F5aSmRti\nzJY6zPzNBF5espWsvGL+ddlIkuL8DX9IKdUoGvBVi8kpKuO2t77l1tOPoltiLJ072J4k5YEK7vlg\nDQAzlm1DpPGD/T144TGMPKwz6btzWbg+m4XrsiksDXDluMP4wxlHsW53Hqt35HL5j1roHgKlPEwD\nvmozxWUBBt/zKQB3TR7Mm8szqu4haIzhfZP5dru9s3fxHRN5a3kGF47qzXvf7ECAX518OL4ovUis\nVCUN+KpNfbNtP8u37Of6EwcgIry7MoPZa3Yze03LPNv34tF9GD8whYtG92mR9SnVnmnAV2Ert7iM\npxds4tmFm/jHT4fj90Xx+tfbuGPSEB78NA1B2LavkG37Gv9YxV+cOIAZy7bzp7OHctHoPsxLy+Tl\nJVs5tk8nbjn1SPw+0a6jyrM04CtP2JCZR2y0j9JAgFMfXXRI6/p/Px3O8L6dGNS1IyLC4o17iPZF\nkZzg54hudlpWbjFdOsRU3cimVHvQlIBfx5MllAoPR3S3w8caY7hs7GGcPrQ7732zg66Jsew8UETf\nLgnMS8vkh+wGHi4B3PrWtw0uA9A1MZbXp45jT14JPxqYwrLN++ic4KdbUhyd4rV3kWrftIWv2r2y\nQAVrd+YyvG8yAIvWZ9M9KY5H564jUAHz0uy1g66JsWTnNb47aad4PzlF1Y/GG9Y7iRF9k/nr+cfw\nWVomY/p1wWCIj/HpvQaqzWhKR6kg89dl4Y+KYvygFF5asoWsvBKS4/2k7cqlV3I8Ty04tKcbDe6R\nyJs3juejb3cytGcSXRNjSU6IoWOsnkAr92lKR6kgJx/Vrer1dScMqDGvpDxAfkk53RJjKS6r4Fcn\nD2L1jlzKAxWk787jLzPXcu3x/fnv4i0h15++O49j751Ta/rgHomk786jQ4yPgtIAPTvFcf2JA9mY\nlc/95x2t1wpUq9MWvlL1MMZU9fApD1Tw38Vb+OvHdlz9C0f1Zu3OXNJ35zV5vdFRwrkjepEU56df\nin3+7bfbD3DZ2MMY3DOJzNxi3lmRQVK8n8vHHkZSvF/vP1B10pSOUi7avq+Q/JJyhvRMAuxB4YNV\nOxnUtSN9u8Tz8pKtfLpmN2t25la17lvCeSN6cdrQ7kwe1hMD7M4tpntibI0zhc/SMumWGMem7HzO\nH9m7RcpV4U0DvlJhoKCknA5OHv/pBZv4dPUurjthALe8sYrR/Trz64mH8+7KHXz0bQPPaq1HYlw0\nfTsncOGo3lwwsjej/zqvat47vzyev3y0hqR4P5cedxj9UhKIEmFIz0S9L8FDNOArFcYCFQaBqieN\n3T9zLR1jozn96O6kdoxl6eZ99EiKY+7a3fRP7cDGrHxe+N+WFq3DY5eMoFtiLB9+u5PM3GKevWoM\nBSXlFJYFSPD7qsZMUuFPA75SHvPlhj0M653E0ws3MTC1A3F+HxMHd6OwNMCP/v5Zi5d3RLeOxMf4\nGNu/CzO/28Xu3GLADn2dW1zGkd0TuXh0H5Zv2cdlPzqM1TtySIrzV907EayiwiACIkJxWYBN2fkc\n3UufmdBSNOArFUHW7syluDzAyL7JVamaz9MzGTsghQS/j8/Ss+iVHMcNL6/g1ycfzmlDu3P7O98z\nLy2TO84czPz0LJYe9CCc5vrD6Uey5Ie9XDiyD0N7JZGdV8KfPljN0F5JDOvdiee/2MzeglLev+kE\nBnbtQFKcn8oYtGzzPsoChglHpLZIXSKFBnylVKNVVBgKSsvZureQgpJy3l25g36pCRwoLOPrLfsY\nkNqBzgkxPP/l5qrPdE+K5bAuCXy9Zf8hlT0wtQM/7Kl5p3RSXDT/N2kwmbnFLNu8j9SOsdx86hEM\n6tqRb7btp1dyPPF+H88s3ES0T7jtjMEA5BWX8Z8vNvPLkwYR54+cG+E04CulWlxxWYAX/reFn47p\nQ2rHWMCOjOqLEr7LyOG+j9ZQFjBc8aPD6JUczyOz19VaR5cOMewrKG3ReqV2jCU2Oqrq0ZzjBnZh\nX0Ep6zPz+eCmE5yntxmy80oZ2LUDt08aTEl5gOioKESgtLyCnKIyeiXHA7YXVlZeCaP7dW7RerpF\nA75Sqk2UllcQE227iZYHKti8p4AjuieyITOPw7t1pMLA/sJSVu/IIVBh+GLDHmZ+t5N+KR2YfExP\nzjm2J0t+2Muby7eTHB9DhTEcPyil6mE7bpo4uBvrdudVHTiuHt+PeWsz2ZlTzEWj+nBk947MWZvJ\nTScPImN/EYN7JDGmX2e+zbDPbzhQVMZHq3Zy11lDSO0Yy6bsfIwx9O2SwKasArolxZIU52deWiYT\nB3erOgsJvtejOTTgK6U8Zc6a3eQUlTFxcDdWbN3P2AFdSE6I4dmFmzhuQBf6dk7g4+920jM5nm+3\nH6g1XMYNPx7ICYencvX0ZW20BTUN75vMWcf0oGOsn0fnrqdXchxv3Ti+WWMyacBXSkW0igpDaaCC\ndbvzOKZ3p6ousPPWZjKmf2ey80p4/evtRPuEPXmlvLMyg6vH98MYOH9kLz5YtZPjB6VSXBZgb0Ep\n989c22CZY/p1przCsCunqEnPfAbon5LAgttObta2hk3AF5FJwL8AH/AfY8yD9S2vAV8pFY627i2g\nd3I8JeUViMCO/UU1hu8uC5iqVBbYA07AGKKjhJXbDjBj2TZ+ddIgDhSVMaJPMos37eVAUSnDenVi\n5bb9jO7XmX4pHZpVt7AI+CLiA9YDpwEZwNfAZcaYkIdKDfhKKdU0TQn4bg7XNxbYaIz5wRhTCrwO\nnOdieUopperhZsDvDWwPep/hTKtBRKaKyHIRWZ6dne1idZRSKrK1+YDcxphpxpgxxpgxXbt2bevq\nKKWUZ7kZ8HcAfYPe93GmKaWUagNuBvyvgSNEZICIxACXAh+6WJ5SSql6uPaIQ2NMuYj8GpiN7ZY5\n3Rjj/u1ySiml6uTqM22NMbOAWW6WoZRSqnHa/KKtUkqp1hFWQyuISDawtZkfTwX2tGB12gPd5sig\n2+x9h7K9/YwxjeriGFYB/1CIyPLG3m3mFbrNkUG32ftaa3s1paOUUhFCA75SSkUILwX8aW1dgTag\n2xwZdJu9r1W21zM5fKWUUvXzUgtfKaVUPTTgK6VUhGj3AV9EJonIOhHZKCJ3tHV9WoqI9BWR+SKy\nVkTWiMjNzvQuIjJXRDY4vzs700VEHne+h+9EZFTbbkHziYhPRL4RkZnO+wEistTZtjecsZkQkVjn\n/UZnfv+2rHdziUiyiLwtIukikiYi472+n0Xkd87f9WoRmSEicV7bzyIyXUSyRGR10LQm71cRucZZ\nfoOIXHModWrXAd95qtaTwJnAUOAyERnatrVqMeXArcaYocA44CZn2+4APjPGHAF85rwH+x0c4fxM\nBZ5u/Sq3mJuBtKD3DwH/NMYcDuwHfu5M/zmw35n+T2e59uhfwKfGmMHAcOy2e3Y/i0hv4LfAGGPM\nMOxYW5fivf38X2DSQdOatF9FpAvwZ+BH2IdK/bnyINEsxph2+wOMB2YHvb8TuLOt6+XStn6AfVzk\nOqCnM60nsM55/Sz2EZKVy1ct155+sMNofwZMBGYCgr0DMfrgfY4dmG+88zraWU7aehuauL2dgM0H\n19vL+5nqhyN1cfbbTOAML+5noD+wurn7FbgMeDZoeo3lmvrTrlv4NPKpWu2dcwo7ElgKdDfG7HJm\n7Qa6O6+98l08BvwfUOG8TwEOGGPKnffB21W1zc78HGf59mQAkA284KSx/iMiHfDwfjbG7AD+AWwD\ndmH32wq8vZ8rNXW/tuj+bu8B3/NEpCPwDnCLMSY3eJ6xh3zP9KsVkbOBLGPMirauSyuKBkYBTxtj\nRgIFVJ/mA57cz52xz7ceAPQCOlA79eF5bbFf23vA9/RTtUTEjw32rxpj3nUmZ4pIT2d+TyDLme6F\n7+IE4FwR2YJ96P1EbH47WUQqh/IO3q6qbXbmdwL2tmaFW0AGkGGMWeq8fxt7APDyfj4V2GyMyTbG\nlAHvYve9l/dzpabu1xbd3+094Hv2qVoiIsDzQJox5tGgWR8ClVfqr8Hm9iunX+1c7R8H5ASdOrYL\nxpg7jTF9jDH9sfvyc2PMFcB84GJnsYO3ufK7uNhZvl21hI0xu4HtInKUM+kUYC0e3s/YVM44EUlw\n/s4rt9mz+zlIU/frbOB0EensnBmd7kxrnra+qNECF0UmA+uBTcAf27o+LbhdE7Cne98Bq5yfydjc\n5WfABmAe0MVZXrA9ljYB32N7QLT5dhzC9p8EzHReDwSWARuBt4BYZ3qc836jM39gW9e7mds6Alju\n7Ov3gc5e38/AfUA6sBp4GYj12n4GZmCvUZRhz+R+3pz9CvzM2faNwHWHUicdWkEppSJEe0/pKKWU\naiQN+EopFSE04CulVITQgK+UUhFCA75SSkUIDfhKtQAROalydE+lwpUGfKWUihAa8FVEEZErRWSZ\niKwSkWedsffzReSfzvjsn4lIV2fZESLylTM++XtBY5cfLiLzRORbEVkpIoOc1XcMGtf+VecuUqXC\nhgZ8FTFEZAhwCXCCMWYEEACuwA7etdwYczSwEDv+OMBLwO3GmGOxdz9WTn8VeNIYMxw4Hns3JdgR\nTW/BPpthIHZ8GKXCRnTDiyjlGacAo4GvncZ3PHbwqgrgDWeZV4B3RaQTkGyMWehMfxF4S0QSgd7G\nmPcAjDHFAM76lhljMpz3q7BjoX/p/mYp1Tga8FUkEeBFY8ydNSaK3HPQcs0db6Qk6HUA/f9SYUZT\nOiqSfAZcLCLdoOr5ov2w/weVozReDnxpjMkB9ovIic70q4CFxpg8IENEznfWESsiCa26FUo1k7ZA\nVMQwxqwVkbuBOSIShR3F8CbsQ0fGOvOysHl+sMPXPuME9B+A65zpVwHPishfnHX8tBU3Q6lm09Ey\nVcQTkXxjTMe2rodSbtOUjlJKRQht4SulVITQFr5SSkUIDfhKKRUhNOArpVSE0ICvlFIRQgO+UkpF\niP8PvHvYIX4n9nAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U_V5T41SlaK",
        "colab_type": "code",
        "outputId": "3b1d0e9d-acae-4a6e-d62c-bcc894554c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(cnnhistory.history['acc'])\n",
        "plt.plot(cnnhistory.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSogVUU5Ss_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict_classes(x_testcnn)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgxbt83CSxlb",
        "colab_type": "code",
        "outputId": "d894f4f9-65c2-4af3-b36b-c832649dfe18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "predictions\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 2, 2, 2, 2, 3, 2, 3, 0, 3, 3, 3, 0, 0, 3, 2, 2, 3, 2, 3, 2,\n",
              "       2, 0, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 0, 0, 2, 2, 2, 3, 3, 2, 2,\n",
              "       2, 2, 0, 3, 2, 0, 2, 2, 3, 0, 0, 3, 3, 0, 3, 3, 3, 3, 2, 2, 2, 0,\n",
              "       2, 0, 2, 3, 0, 2, 2, 0, 2, 3, 3, 2, 3, 0, 2, 3, 3, 3, 2, 3, 2, 2,\n",
              "       0, 3, 2, 2, 2, 3, 0, 2, 3, 2, 0, 3, 3, 2, 2, 3, 3, 0, 0, 0, 2, 2,\n",
              "       3, 3, 3, 3, 2, 0, 2, 0, 3, 3, 3, 2, 3, 0, 2, 0, 3, 2, 3, 3, 0, 0,\n",
              "       0, 0, 2, 3, 2, 2, 2, 0, 0, 0, 3, 3, 3, 3, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUjZwQJkSz_s",
        "colab_type": "code",
        "outputId": "9acaed2e-ccd5-458b-8af2-f492e0d2b544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "y_test\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 2, 0, 2, 0, 3, 3, 2, 3, 3, 2, 3, 0, 0, 0, 3, 2, 2, 3, 3, 0, 2,\n",
              "       3, 0, 3, 2, 0, 2, 3, 2, 2, 3, 0, 2, 2, 0, 0, 2, 2, 3, 3, 3, 2, 2,\n",
              "       2, 0, 0, 3, 2, 3, 2, 3, 0, 0, 0, 2, 3, 0, 0, 0, 3, 3, 2, 3, 2, 3,\n",
              "       2, 0, 2, 0, 2, 2, 2, 0, 2, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 3,\n",
              "       0, 0, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 0, 3, 0, 3, 2,\n",
              "       3, 3, 3, 0, 2, 2, 2, 0, 0, 2, 3, 2, 2, 0, 0, 3, 3, 3, 2, 2, 0, 0,\n",
              "       0, 0, 2, 3, 2, 2, 2, 0, 0, 0, 3, 0, 3, 3, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoWIhoZES4Ce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_Ytest = y_test.astype(int)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_R1Ei_5S6S_",
        "colab_type": "code",
        "outputId": "e2697136-95d7-4628-e1ec-db73ad5d557c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "new_Ytest\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 2, 0, 2, 0, 3, 3, 2, 3, 3, 2, 3, 0, 0, 0, 3, 2, 2, 3, 3, 0, 2,\n",
              "       3, 0, 3, 2, 0, 2, 3, 2, 2, 3, 0, 2, 2, 0, 0, 2, 2, 3, 3, 3, 2, 2,\n",
              "       2, 0, 0, 3, 2, 3, 2, 3, 0, 0, 0, 2, 3, 0, 0, 0, 3, 3, 2, 3, 2, 3,\n",
              "       2, 0, 2, 0, 2, 2, 2, 0, 2, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 3,\n",
              "       0, 0, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 0, 3, 0, 3, 2,\n",
              "       3, 3, 3, 0, 2, 2, 2, 0, 0, 2, 3, 2, 2, 0, 0, 3, 3, 3, 2, 2, 0, 0,\n",
              "       0, 0, 2, 3, 2, 2, 2, 0, 0, 0, 3, 0, 3, 3, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIwoisf0S8cz",
        "colab_type": "code",
        "outputId": "c1cefed0-c852-4caa-be87-059c2dd4f3bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(new_Ytest, predictions)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.59      0.64        41\n",
            "           2       0.66      0.80      0.72        49\n",
            "           3       0.63      0.60      0.61        57\n",
            "\n",
            "    accuracy                           0.66       147\n",
            "   macro avg       0.67      0.66      0.66       147\n",
            "weighted avg       0.66      0.66      0.66       147\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHhx1Tr8TALZ",
        "colab_type": "code",
        "outputId": "ce0cb6d1-9a39-48b1-c4ec-1ff79ea74938",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "matrix = confusion_matrix(new_Ytest, predictions)\n",
        "print (matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[24  5 12]\n",
            " [ 2 39  8]\n",
            " [ 8 15 34]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q20eYX5gTH0k",
        "colab_type": "code",
        "outputId": "a0605b23-ee7b-46b9-f74d-7bb778d9e4f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = '/content/drive/My Drive/AI/ravdessDataset/Ravdess_model'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved trained model at /content/drive/My Drive/AI/ravdessDataset/Ravdess_model/Emotion_Voice_Detection_Model.h5 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKssm1wDTLGH",
        "colab_type": "code",
        "outputId": "5ab2eee9-a348-4e3b-abd2-a08d400f0b87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "loaded_model = keras.models.load_model('/content/drive/My Drive/AI/ravdessDataset/Ravdess_model/Emotion_Voice_Detection_Model.h5')\n",
        "loaded_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 5128      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 8)                 0         \n",
            "=================================================================\n",
            "Total params: 87,944\n",
            "Trainable params: 87,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN73BU9lTPak",
        "colab_type": "code",
        "outputId": "8add8fc4-cfcd-4727-f860-d71f1f184ca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "loss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "147/147 [==============================] - 0s 884us/step\n",
            "Restored model, accuracy: 65.99%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9mYukXVTp3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mbx_iIqW6Ry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rforest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"log2\", \n",
        "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
        "                                 n_estimators= 22000, random_state= 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXlrQ4WJW-TS",
        "colab_type": "code",
        "outputId": "30b872bc-5cba-4364-db6a-9296cd2370cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "rforest.fit(X_train, y_train)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "                       max_depth=10, max_features='log2', max_leaf_nodes=100,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=3, min_samples_split=20,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=22000,\n",
              "                       n_jobs=None, oob_score=False, random_state=5, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSogTUwGXCQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = rforest.predict(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJZ7u480XFZG",
        "colab_type": "code",
        "outputId": "93855880-d9cb-4029-df2d-8d09dc8fa765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "print(classification_report(y_test,predictions))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.09      0.16        34\n",
            "           2       0.58      0.82      0.68        45\n",
            "           3       0.59      0.69      0.64        68\n",
            "\n",
            "    accuracy                           0.59       147\n",
            "   macro avg       0.72      0.53      0.49       147\n",
            "weighted avg       0.68      0.59      0.54       147\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUoXxxdXXID_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}